Analysis of familiar word recognition
===========================================================================

```{r include = FALSE}
d_m <- readr::read_csv("./data/aim1-model-ready.csv.gz")
knitr::read_chunk("./helpers.R")
if (interactive()) source("./helpers.R")
```

```{r helpers, include = FALSE, cache = FALSE}
```

```{r, include = FALSE}
library(rstanarm)
library(bayesplot)
theme_set(theme_grey())
library(stringr)
library(ggstance)
knitr::opts_chunk$set(cache = TRUE)
```

```{r, include = FALSE}
parse_text <- function(x) parse(text = x)

b <- readr::read_rds("./data/stan_aim1_cubic_model.rds.gz")
b$stan_function <- "stan_glmer"

nsamples <- nrow(as.data.frame(b))

convert_study_to_age <- function(xs) {
  factor(
    xs, 
    levels = c("TimePoint1", "TimePoint2", "TimePoint3"), 
    labels = c("Age 3", "Age 4", "Age 5"))
}
```


General Todos:

* Determine and use consistent terminology for the studies and growth curve
  features.
* Make sure axis labels and legend labels are consistent.

## Growth curve analysis

_TJM: I'll have to clean this section up once the order of the main results is
pinned down. That way this can flow neatly in to the findings._

Looks to the familiar image were analyzed using Bayesian mixed
effects logistic regression. I used *logistic* regression because
the outcome measurement is a probability (the log-odds of looking to the
target image versus a distractor). I used *mixed-effects* models
to estimate a separate growth curve for each child (to
measure individual differences in word recognition) but also treat each
child's individual growth curve as a draw from a distribution of related
curves.

I used *Bayesian* techniques to study a generative model of the
data. Instead of reporting and describing a single, best-fitting model
of some data, Bayesian methods consider an entire distribution of
plausible models that are consistent with the data and any prior
information we have about the models. By using this approach, one can
explicitly quantify uncertainty about statistical effects and draw
inferences using estimates of uncertainty (instead of using statistical
significance—which is not a straightforward matter for mixed-effects
models).[^2]

[^2]: It is tempting to further justify this approach by comparing
    Bayesian versus classical/frequentist statistics, but my goals in
    using this method are simple: To estimate statistical effects and
    quantify uncertainty about those effects. This pragmatic brand of
    Bayesian statistics is illustrated in texts by @GelmanHill and 
    @RethinkingBook.

The eyetracking growth curves were fit using an orthogonal cubic
polynomial function of time [a now-conventional approach; see
@Mirman2014]. Put differently, I modeled the probability of looking to
the target during an eyetracking task as:

$$
\text{log-odds}(\textit{looking}) = 
  \beta_0 + 
  \beta_1 * \textit{Time}^1 + 
  \beta_2 * \textit{Time}^2 + 
  \beta_3 * \textit{Time}^3
$$

That the time terms are *orthogonal* means that $\\textit{Time}^1$,
$\textit{Time}^2$ and $\textit{Time}^3$ are transformed so that they
are uncorrelated. Under this formulation, the parameters $\beta_0$ and
$\beta_1$ have a direct interpretation in terms of lexical processing
performance. The intercept, $\beta_0$, measures the area under the
growth curve—or the probability of fixating on the target word averaged
over the whole window. We can think of $\beta_0$ as a measure of *word
recognition reliability*. The linear time parameter, $\beta_1$,
estimates the steepness of the growth curve—or how the probability of
fixating changes from frame to frame. We can think of $\beta_1$ as a
measure of *processing efficiency*, because growth curves with stronger
linear features exhibit steeper frame-by-frame increases in looking
probability.[^3]


[^3]: The polynomial other terms are less important—or rather, they have
    do not map as neatly onto behavioral descriptions as the accuracy
    and efficiency parameters. The primary purpose of quadratic and
    cubic terms is to ensure that the estimated growth curve adequately
    fits the data. In this kind of data, there is a steady baseline at
    chance probability before the child hears the word, followed a
    window of increasing probability of fixating on the target as the
    child recognizes the word, followed by a period of plateauing and
    then diminishing looks to target. The cubic polynomial allows the
    growth curve to be fit with two inflection points: the point when
    the looks to target start to increase from baseline and the point
    when the looks to target stops increasing.

To study how word recognition changes over time, I modeled how the growth curves
change over developmental time. This amounted to studying how the growth curve
parameters changes year over year. We included dummy-coded indicators for Age 3,
Age 4, and Age 5 and allowed these indicators interact with the growth curve
parameters. These year-by-growth-curve terms captured how the shape of the
growth curves changed each year. We also included random effects to represent
child-by-year effects.


### Growth curve features as measures of word recognition performance

As noted above, two of the model's growth curve features have direct
interpretations in terms of lexical processing performance: The model's
intercept parameter corresponds to the average proportion/probability of looking
to the named image over the trial window and the linear time parameter
corresponds to slope of the growth curve or lexical processing efficiency. We
also were interested in _peak_ proportion of looks to the target. We derived
this value by computing the growth curves from the model and taking the median
of the five highest points on the curve. Figure \@ref(fig:curve-features) shows
three simulated growth curves and how each of these growth curve features relate
to word recognition performance.

(ref:curve-features) Illustration of the three growth curve features and how
they describe lexical processing performance. The three curves used are
simulations of new participants at Age&nbsp;4.

```{r curve-features, fig.cap = "(ref:curve-features)", echo = FALSE, out.width = "80%", fig.height = 5.5, fig.width = 5.5}
dummy_data <- d_m %>% 
  filter(Study == "TimePoint2") %>% 
  distinct(Study, Time, ot1, ot2, ot3) %>% 
  mutate(
    ResearchID = "NEW", 
    Primary = 0, 
    Others = 0)

set.seed(02062018)

lpred <- dummy_data %>% 
  tristan::augment_posterior_linpred(model = b, newdata = ., nsamples = 3) 

lpred_lm <- lpred %>% 
  tidyr::nest(-.draw) %>% 
  mutate(
    model = purrr::map(data, ~ lm(.posterior_value ~ ot1 + ot2 + ot3, .x)),
    coef = lapply(model, coef),
    auc = purrr::map_dbl(coef, purrr::pluck, 1),
    slope = purrr::map_dbl(coef, purrr::pluck, 2)) %>% 
  select(-model, -coef) %>% 
  tidyr::unnest(data) %>% 
  # Sort using the AUC values
  mutate(.draw = forcats::fct_reorder(as.factor(.draw), auc))

p1 <- ggplot(lpred_lm) + 
  aes(x = ot1, y = .posterior_value, group = .draw) + 
  geom_line(linetype = "dashed", color = "#111111") + 
  geom_hline(
    aes(yintercept = auc), 
    color = "#0074D9", size = .75) + 
  labs(
    x = NULL,
    y = "Logodds looking to target",
    title = "Intercept term represents average probability") + 
  facet_wrap(".draw") + 
  theme_grey(base_size = 9) + 
  theme(
    strip.text = element_blank(), 
    axis.text.x = element_blank(),
    axis.title.y = element_text(colour = "white"),
    axis.ticks.x = element_blank())

p2 <- ggplot(lpred_lm) + 
  aes(x = ot1, y = .posterior_value, group = .draw) + 
  geom_line(linetype = "dashed", color = "#111111") + 
  geom_abline(
    aes(intercept = auc, slope = slope),
    color = "#0074D9", size = .75) + 
  facet_wrap(".draw") + 
  labs(
    x = NULL,
    y = "Logodds looking to target",
    title = "Linear time term conveys processing efficiency") +
  theme_grey(base_size = 9) + 
  theme(
    strip.text = element_blank(), 
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank())

p3 <- ggplot(lpred_lm) + 
  aes(x = ot1, y = .posterior_value, group = .draw) + 
  geom_line(linetype = "dashed", color = "#111111") + 
  geom_point(
    data = lpred_lm %>% group_by(.draw) %>% top_n(5, .posterior_value), 
    size = 2, color = "#111111") + 
  geom_hline(
    aes(yintercept = .posterior_value), 
    data = lpred_lm %>% 
      group_by(.draw) %>% 
      top_n(5, .posterior_value) %>% 
      summarise(.posterior_value = median(.posterior_value)), 
    size = .75, color = "#0074D9") + 
  facet_wrap(".draw") + 
  labs(
    x = "Time after target onset [ms]",
    y = "Logodds looking to target",
    title = "Peak probability is derived from median of top five points") + 
  theme_grey(base_size = 9) + 
  theme(
    strip.text = element_blank(), 
    axis.text.x = element_blank(),
    axis.title.y = element_text(colour = "white"),
    axis.ticks.x = element_blank())

cowplot::plot_grid(p1, p2, p3, ncol = 1)
```









### Year over year changes in word recognition

_TJM: This is a bit rough still._

As a mixed-effects model, the model estimated a population-average growth curve
(the "fixed" effects) and how individual children deviated from average (the
"random" effects). Figure \@ref(fig:average-growth-curves) shows 200 posterior samples of the average
growth curves for each study. Clearly, on average, the growth curves become
steeper and achieve higher looking probabilities with each year of the study.

(ref:average-growth-curves) The model estimated an average word
recognition growth for each study, and the colored lines represent 200
posterior samples of these growth curves. The thick dark lines represent
the observed average growth curve in each study.

```{r average-growth-curves, fig.cap = "(ref:average-growth-curves)", echo = FALSE, out.width = "50%", fig.height = 3, fig.width = 4}
dummy_data <- d_m %>% 
  distinct(Study, Time, ot1, ot2, ot3) %>% 
  mutate(
    ResearchID = "NEW", 
    Primary = 0, 
    Others = 0)

set.seed(11102017)
lpred <- dummy_data %>% 
  tristan::augment_posterior_linpred(model = b, newdata = ., re.form = NA)

peaks <- lpred %>% 
  group_by(Study, .draw) %>% 
  top_n(5, .posterior_value) %>% 
  mutate(med = median(.posterior_value)) %>% 
  select(.draw, Study, peek_accuracy = med) %>% 
  distinct() %>% 
  ungroup() %>% 
  mutate(peek_accuracy = peek_accuracy) %>% 
  tidyr::spread(Study, peek_accuracy) %>% 
  select(
    `Peak~~(Age~~3)` = TimePoint1,
    `Peak~~(Age~~4)` = TimePoint2,
    `Peak~~(Age~~5)` = TimePoint3)

p <- lpred %>% 
  tjmisc::sample_n_of(200, .draw) %>% 
  mutate(Study = convert_study_to_age(Study)) %>% 
  ggplot() + 
    aes(x = Time, y = plogis(.posterior_value), color = Study) +
    geom_hline(yintercept = .25, color = "white", size = 2) +
    geom_line(
      aes(group = interaction(Study, ResearchID, .draw)), 
      alpha = .1) +
    stat_summary(
      aes(y = Prop, group = Study), 
      data = d_m %>% 
        mutate(Study = convert_study_to_age(Study)), 
      fun.y = "mean", geom = "line", 
      color = "#111111", size = 1) + 
    theme(
      legend.position = c(.01, .99), 
      legend.justification = c(0, 1),
      legend.margin = margin(3, 6, 6, 6)) +
    guides(
      color = guide_legend(title = NULL, override.aes = list(alpha = 1))) +
    labs(
      title = "Observed means and 200 posterior fits",
      x = "Time after target onset [ms]",
      y = "Proportion looks to target")
print(p)
```

We now describe how the curvature of the average growth curves change
each year. Figure \@ref(fig:effects2) depicts uncertainty intervals with
the model's average effects of each timepoint on the growth curve
features. The intercept and linear time effects increased each year,
confirming that children become more reliable and faster at recognizing
words as they grow older. The peak accuracy also increased each year.
For each effect, the change from age 3 to age 4 is approximately the
same as the change from age 4 to age 5, as visible in
Figure \@ref(fig:pairwise-effects).

(ref:effects2) Uncertainty intervals for the effects of study years on
growth curve features. The intercept and peak features were converted from
log-odds to proportions to ease interpretation.

```{r effects2, fig.cap = "(ref:effects2)", echo = FALSE, out.width = "80%", fig.height=4, fig.width=5}
# Column names will have mathematical formatting too
draws <- as.data.frame(b) %>% 
  as_tibble() %>% 
  transmute(
    `Intercept~~(Age~~3)` = `(Intercept)`,
    `Intercept~~(Age~~4)` = `(Intercept)` + StudyTimePoint2,
    `Intercept~~(Age~~5)` = `(Intercept)` + StudyTimePoint3,
    `Time~~(Age~~3)` = ot1,
    `Time~~(Age~~4)` = ot1 + `ot1:StudyTimePoint2`,
    `Time~~(Age~~5)` = ot1 + `ot1:StudyTimePoint3`,
    `Time^2~~(Age~~3)` = ot2,
    `Time^2~~(Age~~4)` = ot2 + `ot2:StudyTimePoint2`,
    `Time^2~~(Age~~5)` = ot2 + `ot2:StudyTimePoint3`,
    `Time^3~~(Age~~3)` = ot3,
    `Time^3~~(Age~~4)` = ot3 + `ot3:StudyTimePoint2`,
    `Time^3~~(Age~~5)` = ot3 + `ot3:StudyTimePoint3`) %>% 
  bind_cols(peaks)

logodds <- draws %>% 
  select(starts_with("Time")) %>% 
  mcmc_intervals_data(prob = 0.5, prob_outer = 0.9) %>% 
  mutate(Scale = "Log-odds") %>% 
  mutate(parameter = factor(parameter, levels = rev(names(draws))))

props <- draws %>% 
  select(starts_with("Intercept"), starts_with("Peak")) %>% 
  mutate_all(plogis) %>% 
  mcmc_intervals_data(prob = 0.5, prob_outer = 0.9) %>% 
  mutate(Scale = "Proportion") %>% 
  mutate(parameter = factor(parameter, levels = rev(names(draws))))

intervals2 <- bind_rows(props, logodds) %>% 
  group_by(Scale) %>% 
  mutate(min_x = min(ll), max_x = max(hh)) %>% 
  ungroup()

ggplot(intervals2) + 
  aes(y = parameter) +
  geom_vline(xintercept = 0, size = 2, color = "white") +
  geom_point(aes(x = m), size = 3, shape = 3) + 
  # Draw mock gridlines in case the point's + shape is wider than the interval.
  # All that's visible is then a | with a gap in the middle.
  geom_segment(
    aes(y = parameter, yend = parameter, x = min_x, xend = max_x), 
    size = .25, color = "white") +
  ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  scale_y_discrete(labels = parse_text) + 
  labs(
    x = NULL, 
    y = NULL, 
    title = "Average effects by study",
    caption = "Posterior median with 90% and 50% intervals") + 
  facet_wrap("Scale", scales = "free", ncol = 1, labeller = label_both)
```

(ref:pairwise-effects) Uncertainty intervals for the differences between study
timepoints. Again, the intercept and peak features were converted to
proportions.

```{r pairwise-effects, fig.cap = "(ref:pairwise-effects)", fig.show = 'hold', echo = FALSE, out.width = "50%", fig.height = c(4, 3), fig.width = c(4, 4)}
clean_names <- . %>% 
  stringr::str_replace_all("[()]", "") %>% 
  stringr::str_replace("Age~~", "Age ") %>% 
  stringr::str_replace("~~", "_") %>% 
  stringr::str_replace("TP1", "Age 3") %>% 
  stringr::str_replace("TP2", "Age 4") %>% 
  stringr::str_replace("TP3", "Age 5")

pairwise <- draws %>% 
  mutate_at(vars(starts_with("Intercept"), starts_with("Peak")), plogis) %>% 
  tibble::rowid_to_column(".draw") %>% 
  set_names(clean_names) %>% 
  tidyr::gather(parameter, value, -.draw) %>% 
  tidyr::separate(parameter, c("parameter", "year"), sep = "_") %>% 
  compare_pairs(year, value) %>% 
  tidyr::spread(pair, value) %>% 
  split(.$parameter) %>% 
  lapply(
    . %>% select(-.draw, -parameter) %>% 
      mcmc_intervals_data(prob = 0.5, prob_outer = 0.9) %>% 
      rename(pair = parameter)) %>% 
  bind_rows(.id = "parameter") %>% 
  mutate(
    pair = pair %>% 
      stringr::str_replace_all("Age ", "Age~~") %>%
      factor(c("Age~~4-Age~~3", "Age~~5-Age~~4", "Age~~5-Age~~3")))

ggplot(pairwise %>% filter(!(parameter %in% c("Peak", "Intercept")))) + 
  aes(y = forcats::fct_rev(pair)) +
  geom_vline(xintercept = 0, size = 2, color = "white") +
  ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  geom_point(aes(x = m), size = 3, shape = 3) + 
  facet_wrap(
    "parameter", ncol = 1, strip.position = "left", 
    labeller = label_parsed) + 
  scale_y_discrete(labels = parse_text) + 
  labs(
    x = NULL, y = NULL, 
    title = "Differences in average effects",
    caption = "Posterior median with 90% and 50% intervals") + 
  theme(
    strip.placement = "outside", 
    strip.background = element_rect(fill = NA),
    axis.text.y = element_text(size = rel(1.2)),
    strip.text.y = element_text(
      size = rel(1.4), 
      margin = margin(0, 10, 0, 5, "pt")))

ggplot(pairwise %>% filter(parameter %in% c("Peak", "Intercept"))) + 
  aes(y = forcats::fct_rev(pair)) +
  geom_vline(xintercept = 0, size = 2, color = "white") +
  ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  geom_point(aes(x = m), size = 3, shape = 3) + 
  facet_wrap(
    "parameter", ncol = 1, strip.position = "left", 
    labeller = label_parsed) + 
  labs(
    x = NULL, y = NULL, 
    title = " ",
    caption = "Posterior median with 90% and 50% intervals") + 
  theme(
    strip.placement = "outside", 
    strip.background = element_rect(fill = NA),
    axis.text.y = element_text(size = rel(1.2)),
    strip.text.y = element_text(
      size = rel(1.4), 
      margin = margin(0, 10, 0, 5, "pt")))
```

```{r, include = FALSE}
pluck <- purrr::pluck

get_pts <- function(x, n = 2) {
  x %>% 
    lapply(. %>% pluck("m") %>% printy::fmt_fix_digits(n))
} 

get_uis <- function(x, n = 2) {
  fmt_ui <- . %>% 
    glue::glue_data(
      "{printy::fmt_fix_digits(ll,n)}--{printy::fmt_fix_digits(hh,n)}") 
  
  x %>% 
    lapply(fmt_ui) 
} 

get_cor_pts <- function(x, n = 2) {
  fmt_cor <- . %>%
    printy::fmt_fix_digits(n) %>% 
    printy::fmt_leading_zero() %>% 
    printy::fmt_minus_sign()
  
  x %>% 
    lapply(. %>% pluck("m") %>% fmt_cor())
}
  
get_cor_uis <- function(x, n = 2) {
  fmt_cor <- . %>%
    printy::fmt_fix_digits(n) %>% 
    printy::fmt_leading_zero() %>% 
    printy::fmt_minus_sign()
  
  x %>% 
    lapply(. %>% glue::glue_data("{fmt_cor(ll)}--{fmt_cor(hh)}")) 
}

props <- props %>% 
  mutate(
    parameter = parameter %>% 
      stringr::str_replace("~~.", "_") %>% 
      stringr::str_replace("\\)", "")) %>% 
  tidyr::separate(parameter, into = c("parameter", "study"))

prop_diffs <- pairwise %>% 
  filter(parameter %in% c("Intercept", "Peak"))

prop_list1 <- props %>% 
  filter(parameter == "Intercept") %>% 
  split(.$study) %>% 
  set_names(str_extract, "TP.")

prop_list2 <- props %>% 
  filter(parameter == "Peak") %>% 
  split(.$study) %>% 
  set_names(str_extract, "TP.")

int_pts <- get_pts(prop_list1)
int_uis <- get_uis(prop_list1)
peak_pts <- get_pts(prop_list2)
peak_uis <- get_uis(prop_list2)

diff_list1 <- prop_diffs %>% 
  filter(parameter == "Intercept") %>% 
  split(.$pair) %>% 
  set_names(str_replace, "-", "_")

diff_list2 <- prop_diffs %>% 
  filter(parameter == "Peak") %>% 
  split(.$pair) %>% 
  set_names(str_replace, "-", "_")

int_dpts <- get_pts(diff_list1)
int_duis <- get_uis(diff_list1)
peak_dpts <- get_pts(diff_list2)
peak_duis <- get_uis(diff_list2)
```

The average looking probability (intercept feature) was `r int_pts$TP1` [90% UI: `r int_uis$TP1`] for
timepoint 1, `r int_pts$TP2` [`r int_uis$TP2`] for timepoint 2, and
`r int_pts$TP3` [`r int_uis$TP3`] for timepoint 3. The averages increased by
`r int_dpts$TP2_TP1` [`r int_duis$TP2_TP1`] from timepoint 1 to timepoint 2
and by `r int_dpts$TP3_TP2` [`r int_duis$TP3_TP2`] from timepoint 2 to
timepoint 3. The peak looking probability was `r peak_pts$TP1` [`r peak_uis$TP1`]
for timepoint 1, `r peak_pts$TP2` [`r peak_uis$TP2`] for timepoint 2, and
`r peak_pts$TP3` [`r peak_uis$TP3`] for timepoint 3. The peak values increased
by `r peak_dpts$TP2_TP1` [`r peak_duis$TP2_TP1`] from timepoint 1 to
timepoint 2 and by `r peak_dpts$TP3_TP2` [`r peak_duis$TP3_TP2`] from
timepoint 2 to timepoint 3. These results numerically confirm the hypothesis
that children would improve in their word recognition reliability, both in terms
of average looking and in terms of peak accuracy, each year.

**Summary**. The average growth curve features increased year over year, so that
children looked to the target more quickly and more reliably.

  - This result is as expected.
  - It's good that the task scaled with development so that there was room to
    grow each year.
  - The growth curve changes each year involved peak accuracy and steepness of
    the curve. They reach higher heights, and they hit year 1 peak earlier each
    year.



### Exploring plausible ranges of performance over time

_TJM: I like this analysis, but I'm realizing I should describe how variable the
real participants are first._

```{r, include = FALSE}
dummy_data <- d_m %>% 
  distinct(Study, Time, ot1, ot2, ot3) %>% 
  mutate(ResearchID = "NEW",
         Primary = 0, 
         Others = 0)

set.seed(11102017)
lpred <- dummy_data %>% 
  tristan::augment_posterior_linpred(b, newdata = ., nsamples = 1000)

new_peaks <- lpred %>% 
  distinct(.draw, .posterior_value, Study) %>% 
  group_by(Study, .draw) %>% 
  top_n(5, .posterior_value) %>% 
  summarise(peak = median(.posterior_value)) %>% 
  ungroup() %>% 
  mutate(peak = plogis(peak)) %>% 
  tidyr::spread(Study, peak) %>% 
  select(-.draw) %>% 
  mcmc_intervals_data(prob = 0.5, prob_outer = 0.9)

new_peaks_pts <- new_peaks %>% 
  split(.$parameter) %>% 
  get_pts()

new_peaks_uis <- new_peaks %>% 
  split(.$parameter) %>% 
  get_uis()

new_peaks_diff <- new_peaks %>% 
  split(.$parameter) %>% 
  lapply(function(l) round(l$hh - l$ll, 2))
```


Bayesian models are generative; they describe how the data could have been
generated. Our model assumed that each child's growth curve was drawn from a
population of related growth curves, and it tried to infer the parameters over
that distribution. These two features---a generative model and learning about
the population of growth curves---allow the model to simulate new samples from
that distribution of growth curves. That is, we can predict a set of growth
curves for a hypothetical, unobserved child drawn from the same distribution as
the `r n_distinct(d_m$ResearchID)` observed children. This procedure lets us
explore the plausible degrees of variability in performance at each age.

Figure \@ref(fig:new-participants) shows the posterior predictions for 1,000
simulated participants, which demonstrates how the model expects new
participants to improve longitudinally but also exhibit stable individual
differences over time. Figure \@ref(fig:new-participants-intervals) shows
uncertainty intervals for these simulations. The model learned to predict less
accurate and more variable performance at age 3 with improving accuracy and
narrowing variability at age 4 and age 5.

(ref:new-participants) Posterior predictions for hypothetical *unobserved*
participants. Each line represents the predicted performance for a new
participant. The three dark lines highlight predictions from one single
simulated participant. The simulated participant shows both longitudinal
improvement in word recognition and similar relative performance compared to
other simulations each year, indicating that the model would predict new
children to improve year over year and show stable individual differences over
time.


```{r new-participants, echo = FALSE, fig.cap = "(ref:new-participants)", fig.width = 6, fig.height = 3, out.width = "80%"}
ggplot(lpred) + 
  aes(x = Time, y = plogis(.posterior_value), color = Study) +
  geom_hline(yintercept = .25, size = 2, color = "white") +
  geom_line(aes(group = interaction(Study, .draw)), 
            alpha = .1, show.legend = FALSE) +
  geom_line(aes(group = interaction(Study, .draw)), 
            data = sample_n_of(lpred, 1, .draw), color = "grey20",
            show.legend = FALSE) +
  facet_wrap("Study") + 
  guides(color = guide_legend("none")) +
  labs(
    title = "Posterior predictions for 1,000 new participants",
    x = "Time after target onset [ms]",
    y = "Proportion looks to target")
```



(ref:new-participants-intervals) Uncertainty intervals for the simulated
participants. Variability is widest at age 3 and narrowest at age 5,
consistent with the prediction that children become less variable as they 
grow older.

```{r new-participants-intervals, echo = FALSE, fig.cap = "(ref:new-participants-intervals)", fig.width = 6, fig.height = 3, out.width = "80%"}
ggplot(lpred) + 
  aes(x = Time, y = plogis(.posterior_value), color = Study) +
  geom_hline(yintercept = .25, size = 2, color = "white") +
  stat_summary(fun.data = median_hilow, fun.args = list(conf.int = .9),
               size = 1, geom = "linerange") + 
  stat_summary(fun.data = median_hilow, fun.args = list(conf.int = .5), 
               size = 1.5, geom = "linerange") + 
  facet_wrap("Study") + 
  guides(color = guide_legend("none")) +
  labs(
    title = "Posterior predictions for 1,000 new participants",
    x = "Time after target onset [ms]",
    y = "Proportion looks to target",
    caption = "90% and 50% intervals")
```

We hypothesized that children would become less variable as they
grew older and converged on a mature level of performance. We can address
this question by inspecting the ranges of predictions for the simulated
participants. The claim that children become less variable would imply that the
range of predictions should be narrower age 5 than for age 4 than age 3. Figure
\@ref(fig:new-ranges) depicts the range of the predictions, both in terms of the
90 percentile range (i.e., the range of the middle 90% of the data) and in terms
of the 50 percentile (interquartile) range. The ranges of performance decrease
from age 3 to age 4 to age 5, consistent with the hypothesized reduction in
variability.

(ref:new-ranges) Ranges of predictions for simulated
participants over the course of a trial. The ranges are most similar during the
first half of the trial when participants are at chance performance, and the
ranges are most different at the end of the trial as children reliably fixate on
the target image. The ranges of performance decreases with each year of the
study as children show less variability.

```{r new-ranges, echo = FALSE, fig.cap = "(ref:new-ranges)", fig.width = 6, fig.height = 3, out.width = "80%"}
by_draw <- lpred %>%
  group_by(Study, Time) %>%
  summarise(
    `95th` = quantile(plogis(.posterior_value), .95),
    `05th` = quantile(plogis(.posterior_value), .05),
    `75th` = quantile(plogis(.posterior_value), .75),
    `25th` = quantile(plogis(.posterior_value), .25),
    `95^th~vs.~05^th` = `95th` - `05th`,
    `75^th~vs.~25^th` = `75th` - `25th`) %>% 
  select(-matches("^..th$")) %>% 
  ungroup() %>% 
  tidyr::gather("range", "extent", -Study, -Time) %>% 
  mutate(range = factor(range, levels = unique(rev(sort(range))))) 
  
ggplot(by_draw) +
  aes(x = Time, y = extent, color = Study) +
  geom_point() + 
  facet_wrap("range", labeller = label_parsed) +
  labs(
    title = "Ranges of predictions for 1000 new participants",
    x = "Time after target onset [ms]",
    y = "Difference of percentiles") 
```

The developmental pattern of increasing reliability and decreasing variability was also observed for the growth curve peaks. For the synthetic participants, 
the model predicted that individual peak probabilities will increase each
year, peak<sub>TP1</sub>\ = `r new_peaks_pts$TimePoint1` 
[90% UI: `r new_peaks_uis$TimePoint1`], 
peak<sub>TP2</sub>\ = `r new_peaks_pts$TimePoint2` [`r new_peaks_uis$TimePoint2`], 
peak<sub>TP3</sub>\ = `r new_peaks_pts$TimePoint3` [`r new_peaks_uis$TimePoint3`]. Moreover, the range of plausible values for the individual peaks narrowed each
for the simulated data. For instance, the difference between the 95^th^
and 5^th^ percentiles was `r new_peaks_diff$TimePoint1` for timepoint 1,
`r new_peaks_diff$TimePoint2` for timepoint 2, and `r new_peaks_diff$TimePoint3`
for timepoint 3.  

**Summary**. We used the model's random effects estimates to simulate growth
curves from 1,000 hypothetical, unobserved participants. The simulated dataset
showed increasing looking probability and decreasing variability with each year
of the study. These simulations confirm the hypothesis that variability would be
diminish as children converge on a mature level of performance on this task.


  - Word recognition performance is a skill where variation is greatest at
    younger ages.
  - What mechanisms might come to bear on this? Does variability narrow
    developmentally for vocabulary?
  - Children different in their word-learning trajectories, so the early
    differences in word recognition could be from younger children who are
    relatively early/late in word-learning. The SDs of the EVT-2 scores narrows a
    small amount each year, even when we only consider the children who
    participated at all three years.
  - (It will be easier to fold this in to the mechanism discussion once we have
    firmer results for the looks-to-foils analysis.)
  - If differences in word recognition matter (and they do) and the differences
    are greatest at younger ages, then they are most informative at younger
    ages.
  - Maybe a few words on why individual differences are worth studying?



### Are individual differences stable over time?

_TJM: This section is in really good shape._

```{r compute-kendalls, include = FALSE}
fits <- readr::read_csv("./data/aim1-gca-features.csv.gz")

# Compute Kendall's coefficient of correspondence
tidy_kendall <- . %>%
  unclass() %>%
  as.data.frame(stringsAsFactors = FALSE)

# Add random ratings
new_coef <- fits %>%
  filter(coef == "intercept") %>%
  mutate(
    .posterior_value = runif(length(.posterior_value)),
    coef = "random values")

# Keep only data from participants who visited all three years
reduced_data <- fits %>%
  bind_rows(new_coef) %>%
  tidyr::spread(Study, .posterior_value) %>%
  tidyr::drop_na(TimePoint1:TimePoint3) 

n_rated <- n_distinct(reduced_data$ResearchID)

ws <- reduced_data %>%
  select(-ResearchID) %>%
  tidyr::nest(TimePoint1:TimePoint3) %>%
  mutate(ws = purrr::map(data, irr::kendall) %>% purrr::map(tidy_kendall)) %>%
  select(-data) %>%
  tidyr::unnest(ws)

posterior_w <- ws %>%
  select(.draw, coef, value) %>%
  tidyr::spread(coef, value) %>%
  rename(`random numbers` = `random values`) %>%
  select(-.draw)
```

We predicted that children would show stable individual differences such that
children who are faster and more reliable at recognizing words at age 3 remain
relatively faster and more reliable at age 5. To evaluate this hypothesis,
we used Kendall's _W_ (the coefficient of correspondence or concordance). This
nonparametric statistic measures the degree of agreement among _J_ judges who
are rating _I_ items. For our purposes, the items are the `r n_rated`
children who provided reliable eyetracking for all three years of the study.
(That is, we excluded children who only had reliable eyetracking data for one or
two years.) The judges are the sets of growth curve parameters from each year of 
study. For example, the intercept term provides three sets of ratings: The 
participants' intercept terms from year 1 are one set of ratings and the 
terms from years 2 and 3 provide two more sets of ratings. These three ratings
are the "judges" used to compute the intercept's _W_. Thus, we compute five 
groups of _W_ coefficients, one for each set of growth curve features: 
Intercept, Time^1^, Time^2^, Time^3^, and Peak looking probability.


```{r table-example-of-feature-ranks, echo = FALSE, eval = FALSE}
# (_Maybe: Table X illustrates some sample ratings of these participants._)

zero_pad_int <- function(xs) {
  formatter <- paste0("%0", max(nchar(xs)), "d")
  sprintf(formatter, xs)
}

val_rank <- function(xs) {
  a <- printy::fmt_minus_sign(printy::fmt_fix_digits(xs, 2))
  b <- zero_pad_int(rank(-xs))
  glue::glue("{a} ({b})")
}

reduced_data %>% 
  filter(coef == "intercept") %>% 
  group_by(ResearchID, coef) %>% 
  summarise_at(vars(starts_with("TimePoint")), median) %>% 
  ungroup() %>% 
  arrange(desc(TimePoint3)) %>% 
  mutate(
    TimePoint1 = val_rank(TimePoint1), 
    TimePoint2 = val_rank(TimePoint2), 
    TimePoint3 = val_rank(TimePoint3)) %>% 
  rename(
    `Participant ID` = ResearchID,
    `Growth curve feature` = coef,
    `Year 1` = `TimePoint1`,
    `Year 2` = `TimePoint2`,
    `Year 3` = `TimePoint3`) %>% 
  head(10) %>% 
  knitr::kable(align = c("l", "l", "r", "r", "r"))
```

Because we used a Bayesian model, we have a distribution of ratings and thus a
distribution of concordance statistics. Each sample of the posterior
distribution fits a growth curve for each child in each study, so each posterior
sample provides a set of ratings for concordance coefficients. The distribution
of *W*'s lets us quantify our uncertainty because we can compute *W*'s for each
of the `r nsamples` samples from the posterior distribution.

One final matter is how do we assess whether a concordance statistic is 
meaningful. To tackle this question, we also included a "null rater", a fake 
parameter that assigned each child in each year a random number. We can use the 
distribution of _W_'s generated by randomly rating children as a benchmark for 
assessing whether the other concordance statistics differ meaningfully from 
chance.

(ref:kendall-stats) Uncertainty intervals for the Kendall's coefficient of
concordance. Random ratings provide a baseline of null _W_ statistics. The
intercept and linear time features are decisively non-null, indicating a
significant degree of correspondence in children's relative word recognition
reliability and efficiency over three years of study.

```{r kendall-stats, fig.cap = "(ref:kendall-stats)", echo = FALSE, out.width = "80%", fig.height = 3, fig.width = 6}
subtitle <- glue::glue(
  "Kendall's W. Raters: 3 time points. Items: {n_rated} children.")

w_intervals <- posterior_w %>% 
  rename(
    Intercept = `intercept`, `Time` = ot1, `Time^2` = ot2, `Time^3` = ot3,
    Peak = peak_logit, `Random~ratings` = `random numbers`) %>% 
  mcmc_intervals_data(prob_outer = .9, prob = .5) %>% 
  mutate(parameter = forcats::fct_rev(parameter)) 

ggplot(w_intervals) + 
  aes(y = parameter) +
  geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  geom_point(aes(x = m), size = 3, shape = 3) + 
  scale_y_discrete(labels = parse_text) + 
  labs(
    x = NULL, 
    y = NULL, 
    title = "Concordance coefficients for growth curve features",
    caption = "Posterior median with 90% and 50% intervals",
    subtitle = subtitle)

# TODO format as correlations
w_pts <- w_intervals %>% 
  split(.$parameter) %>% 
  get_pts()

w_uis <- w_intervals %>% 
  split(.$parameter) %>% 
  get_uis()

rm(ws, reduced_data, posterior_w)
```

We used the `kendall()` function in the `irr` package (vers.
`r packageVersion("irr")`, CITATION) to compute concordance statistics.
Figure \@ref(fig:kendall-stats) depicts uncertainty intervals for the Kendall
_W_'s for these growth curve features. The 90% uncertainty interval of _W_
statistics from random ratings [`r w_uis[["Random~ratings"]]`] subsumes the
intervals for the Time^2^ effect [`r w_uis[["Time^2"]]`] and the Time^3^ effect
[`r w_uis[["Time^3"]]`], indicating that these values do not differentiate
children in a longitudinally stable way. That is, the Time^2^ and Time^3^
features differentiate children across studies as well as random numbers.
Earlier, we stated that only the intercept, linear time, and peak features have
psychologically meaningful interpretations and that the higher-order features of
these models serve to capture the shape of the growth curve data. These
statistics support that assertion.

Concordance is strongest for the peak feature, _W_\ = `r w_pts[["Peak"]]` 
[`r w_uis[["Peak"]]`] and the intercept term, _W_\ = `r w_pts[["Intercept"]]`
[`r w_uis[["Intercept"]]`], followed by the linear time term, _W_\ =
`r w_pts[["Time"]]` [`r w_uis[["Time"]]`]. Because these values are removed 
from the statistics for random ratings, we conclude that there is a credible
degree of correspondence across studies when we rank children using their peak
looking probability, average look probability (the intercept) or their growth
curve slope (linear time).

**Summary**. Growth curve features reflect individual differences in word
recognition reliability and efficiency. By using Kendall's *W* to measure the
degree of concordance among growth curve features over developmental time, we
tested whether individual differences in lexical processing persisted over
development. We found that the peak looking probability, average looking
probability and linear time features were stable over time.

  - Although the range of variability decreases, individual differences do not
    wash out.
  - Lexical processing is a stable ability over the preschool years.
  - Extrapolating outwards, the differences probably diminish to the point that
    they are not meaningful. But traces of those early differences can reappear
    years later on some test scores.

### Predicting future vocabulary size

_TJM: This section is in good shape._

```{r vocab correlations, include = FALSE}
if (!exists("fits")) {
  fits <- readr::read_csv("./data/aim1-gca-features.csv.gz")
}

scores <- readr::read_csv("./data-raw/test_scores.csv") %>% 
  select(
    Study, ResearchID, Age, EVT_GSV, EVT_Standard, PPVT_GSV, PPVT_Standard)

# scores %>%
#   semi_join(
#     scores %>% count(ResearchID) %>% filter(n == 3), 
#     by = "ResearchID") %>% 
#   group_by(Study) %>%
#   summarise_at(vars(matches("Age"), matches("VT")), .funs = sd, na.rm = TRUE)

wide_scores <- scores %>% 
  tidyr::gather("Test", "Value", -ResearchID, -Study) %>% 
  tidyr::unite("Col", Study, Test) %>% 
  tidyr::spread(Col, Value)

cor_complete <- function(...) cor(..., use = "pairwise.complete")

vocab_cors <- fits %>% 
  left_join(wide_scores) %>% 
  group_by(Study, coef, .draw) %>% 
  summarise(
    r_TimePoint3_EVT_Standard = .posterior_value %>% 
      cor_complete(TimePoint3_EVT_Standard),
    r_TimePoint3_EVT_GSV = .posterior_value %>% 
      cor_complete(TimePoint3_EVT_GSV),
    r_TimePoint2_PPVT_GSV = .posterior_value %>% 
      cor_complete(TimePoint2_PPVT_GSV),
    r_TimePoint2_PPVT_Standard = .posterior_value %>% 
      cor_complete(TimePoint2_PPVT_Standard)) %>% 
  ungroup()
```

```{r correlation intervals, include = FALSE}
# Compute the intervals of the correlations
c_intervals <- vocab_cors %>%
  tidyr::nest(-Study, -coef) %>% 
  mutate(
    data = data %>% 
      purrr::map(select, -.draw) %>% 
      purrr::map(mcmc_intervals_data, prob = .5, prob_outer = .9)) %>% 
  tidyr::unnest()

# Compute pairwise differences in vocabulary correlations
vocab_cor_diffs <- vocab_cors %>% 
  tidyr::gather("variable", "correlation", -Study, -coef, -.draw) %>% 
  tidyr::nest(Study, .draw, correlation) %>% 
  mutate(data = purrr::map(data, . %>% compare_pairs(Study, correlation))) %>% 
  tidyr::unnest() %>% 
  # Negative the pairwise differences and flip the labels. 
  # E.g., if TP2-TP1 = .2, then make TP1-TP2 = -.2.
  mutate(
    value = -value,
    pair = pair %>% 
           str_replace("TimePoint(\\d)-TimePoint(\\d)", "TP\\2 - TP\\1")) %>% 
  tidyr::spread(pair, value) %>% 
  tidyr::nest(-coef, -variable) %>% 
  mutate(
    data = data %>% 
      purrr::map(select, -.draw) %>% 
      purrr::map(mcmc_intervals_data, prob = .5, prob_outer = .9)) %>% 
  tidyr::unnest() %>% 
  mutate(parameter = as.character(parameter))
  
main_diffs <- c_intervals %>% 
  mutate(
    parameter = as.character(parameter),
    variable = parameter) %>% 
  bind_rows(vocab_cor_diffs %>% mutate(Study = parameter)) %>% 
  filter(coef %in% c("intercept", "ot1", "peak_logit"))

main_diffs$Study <- main_diffs$Study %>% 
  factor(
    c("TimePoint1", "TimePoint2", "TimePoint3", 
      "TP1 - TP2", "TP2 - TP3", "TP1 - TP3"))
```

```{r correlation-texts, include = FALSE}
# EVT Standard at TimePoint3
evt_tp3_diffs <- main_diffs %>% 
  filter(variable == "r_TimePoint3_EVT_Standard")

peak_evt_tp3_data <- evt_tp3_diffs %>% 
  filter(coef == "peak_logit") %>% 
  split(.$Study) 
peak_evt_tp3_pts <- get_cor_pts(peak_evt_tp3_data) 
peak_evt_tp3_uis <- get_cor_uis(peak_evt_tp3_data)

ot1_evt_tp3_data <- evt_tp3_diffs %>% 
  filter(coef == "ot1") %>% 
  split(.$Study)
ot1_evt_tp3_pts <- get_cor_pts(ot1_evt_tp3_data) 
ot1_evt_tp3_uis <- get_cor_uis(ot1_evt_tp3_data)

int_evt_tp3_data <- evt_tp3_diffs %>% 
  filter(coef == "intercept") %>% 
  split(.$Study) 
int_evt_tp3_pts <- get_cor_pts(int_evt_tp3_data) 
int_evt_tp3_uis <- get_cor_uis(int_evt_tp3_data)


# PPVT Standard at TimePoint2
ppvt_tp2_diffs <- main_diffs %>% 
  filter(variable == "r_TimePoint2_PPVT_Standard")

peak_ppvt_tp2_data <- ppvt_tp2_diffs %>% 
  filter(coef == "peak_logit") %>% 
  split(.$Study) 
peak_ppvt_tp2_pts <- get_cor_pts(peak_ppvt_tp2_data)
peak_ppvt_tp2_uis <- get_cor_uis(peak_ppvt_tp2_data)

int_ppvt_tp2_data <- ppvt_tp2_diffs %>% 
  filter(coef == "intercept") %>% 
  split(.$Study) 
int_ppvt_tp2_pts <- get_cor_pts(int_ppvt_tp2_data)
int_ppvt_tp2_uis <- get_cor_uis(int_ppvt_tp2_data)

ot1_ppvt_tp2_data <- ppvt_tp2_diffs %>% 
  filter(coef == "ot1") %>% 
  split(.$Study) 
ot1_ppvt_tp2_pts <- get_cor_pts(ot1_ppvt_tp2_data)
ot1_ppvt_tp2_uis <- get_cor_uis(ot1_ppvt_tp2_data)
```

We hypothesized that individual differences in word recognition at age 3 will be
more discriminating and predictive future language outcomes than differences
at age 4 or age 5. To test this hypothesis, we calculated the correlations
of growth curve features with year 3 expressive vocabulary size and year 2
receptive vocabulary. (The receptive test was not administered during year 3
for logistical reasons). As with the concordance analysis, we computed each
of the correlations for each sample of the posterior distribution to obtain a
distribution of correlations.

Figure \@ref(fig:evt2-gca-cors) shows the correlations of the peak looking
probability, average looking probability and linear time features with
expressive vocabulary size at year 3, and Figure \@ref(fig:ppvt4-gca-cors) shows
analogous correlations for the receptive vocabulary at year 2. For all cases,
the strongest correlations were found between the growth curve features 
at year 1.
Growth curve peaks from year 1 correlated with year 3 vocabulary with _r_ =
`r peak_evt_tp3_pts$TimePoint1`, 90% UI [`r peak_evt_tp3_uis$TimePoint1`], but
the concurrent peaks from year 3 showed a correlation
of just _r_ = `r peak_evt_tp3_pts$TimePoint3`, [`r peak_evt_tp3_uis$TimePoint3`],
a difference between year 1 and year 3 of _r_<sub>TP1&minus;TP3</sub>\ =
`r peak_evt_tp3_pts[["TP1 - TP3"]]`, [`r peak_evt_tp3_uis[["TP1 - TP3"]]`].
A similar pattern held for lexical processing efficiency values. Linear time
features from year 1 correlated with year 3 vocabulary with *r* =
`r ot1_evt_tp3_pts$TimePoint1`, 90% UI [`r ot1_evt_tp3_uis$TimePoint1`],
whereas the concurrent lexical processing values from year 3 only showed a
correlation of *r* = `r ot1_evt_tp3_pts$TimePoint3`,
[`r ot1_evt_tp3_uis$TimePoint3`], a difference of *r*<sub>TP1−TP3</sub>\ =
`r ot1_evt_tp3_pts[["TP1 - TP3"]]`, [`r ot1_evt_tp3_uis[["TP1 - TP3"]]`].
For the average looking probabilities, the correlation for year 1, _r_ =
`r int_evt_tp3_pts$TimePoint1`, [`r ot1_evt_tp3_uis$TimePoint1`],
was probably only slightly greater than the correlation for year 2,
_r_<sub>TP1&minus;TP2</sub>\ = `r  int_evt_tp3_pts[["TP1 - TP2"]]`,
[`r int_evt_tp3_uis[["TP1 - TP2"]]`] but considerably greater than
the concurrent correlation at year 3, _r_<sub>TP1&minus;TP3</sub> =
`r  int_evt_tp3_pts[["TP1 - TP3"]]`, [`r int_evt_tp3_uis[["TP1 - TP3"]]`].

(ref:evt2-gca-cors) Uncertainty intervals for the correlations of growth curve
features at each time point with expressive vocabulary (EVT-2 standard scores) at
year 3. The bottom rows provide intervals for the pairwise differences in
correlations between time points.

```{r evt2-gca-cors, fig.cap = "(ref:evt2-gca-cors)", out.width = "80%", fig.height = 3, fig.width = 6, echo = FALSE}
plot_names <- c(
  intercept = "Avg. Probability", 
  ot1 = "Linear Time",
  peak_logit = "Peak Probability")

main_diffs <- main_diffs %>% 
  mutate(plot_coef = plot_names[coef] %>% 
           factor(c("Peak Probability", "Avg. Probability", "Linear Time")))

ggplot(main_diffs %>% filter(variable == "r_TimePoint3_EVT_Standard")) + 
  aes(y = forcats::fct_rev(Study)) +
  geom_vline(xintercept = 0, size = 2, color = "white") +
  geom_point(aes(x = m), size = 3, shape = 3) + 
  geom_segment(
    aes(y = Study, yend = Study, x = min_x, xend = max_x), 
    data = . %>% 
      group_by(plot_coef) %>% 
      mutate(min_x = min(ll), max_x = max(hh)) %>% 
      ungroup(),
    size = .25, color = "white") +
  ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  facet_wrap("plot_coef") +
  labs(
    x = NULL, 
    y = NULL, 
    caption = "90% and 50% intervals") + 
  ggtitle(
    "Correlations of curve features and expressive vocabulary at timepoint 3") + 
  theme(title = element_text(size = rel(.9)))
```

Peak looking probabilities from year 1 were strongly correlated with year 2 
receptive vocabulary, _r_\ = `r peak_ppvt_tp2_pts$TimePoint1`, 
[`r peak_ppvt_tp2_uis$TimePoint1`], and this correlation was much greater
than the correlation observed for the year 2 growth curve peaks,
*r*<sub>TP1−TP2</sub> = `r  peak_ppvt_tp2_pts[["TP1 - TP2"]]`,
[`r peak_ppvt_tp2_pts[["TP1 - TP2"]]`].
The correlation of year 1 average looking probabilities, _r_
\ = `r int_ppvt_tp2_pts$TimePoint1`, [`r int_ppvt_tp2_uis$TimePoint1`],
was greater than the year 2 correlation, _r_<sub>TP1&minus;TP2</sub> =
`r  int_ppvt_tp2_pts[["TP1 - TP2"]]`, [`r int_ppvt_tp2_pts[["TP1 - TP2"]]`], and
the correlation for year 1 linear time features, 
_r_\ = `r ot1_ppvt_tp2_pts$TimePoint1`,
[`r ot1_ppvt_tp2_uis$TimePoint1`], was likewise greater than the year 2
correlation, _r_<sub>TP1&minus;TP2</sub> = `r ot1_ppvt_tp2_pts[["TP1 - TP2"]]`,
[`r ot1_ppvt_tp2_uis[["TP1 - TP2"]]`].

(ref:ppvt4-gca-cors) Uncertainty intervals for the correlations of growth curve
features at each time point with expressive vocabulary (PPVT-4 standard scores)
at year 2. The bottom row shows pairwise differences between the correlations at
year 1 and year 2.

```{r ppvt4-gca-cors, fig.cap = "(ref:ppvt4-gca-cors)", out.width = "80%", fig.height = 2, fig.width = 6, echo = FALSE}
main_diffs %>% 
  filter(variable == "r_TimePoint2_PPVT_Standard") %>% 
  filter(Study %in% c("TimePoint1", "TimePoint2", "TP1 - TP2")) %>% 
  ggplot() + 
    aes(y = forcats::fct_rev(Study)) +
    geom_vline(xintercept = 0, size = 2, color = "white") +
    geom_point(aes(x = m), size = 3, shape = 3) + 
    geom_segment(
      aes(y = Study, yend = Study, x = min_x, xend = max_x), 
      data = . %>% 
        group_by(plot_coef) %>% 
        mutate(min_x = min(ll), max_x = max(hh)) %>% 
        ungroup(),
      size = .25, color = "white") +
    ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
    ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
    facet_wrap("plot_coef") +
    labs(x = NULL, y = NULL, caption = "90% and 50% intervals") + 
    ggtitle(
      "Correlations of curve features and receptive vocabulary at timepoint 2") + 
    theme(title = element_text(size = rel(.9)))
```

**Summary**. Although individual differences in word recognition are stable over
time, early differences are more significant than later ones. The strongest
predictors of future vocabulary size were the growth curve features from age 3.
That is, word recognition performance from age 3 was more strongly correlated
with age 5 expressive vocabulary than word recognition performance at age 5. A
similar pattern of results held for predicting receptive vocabulary at age 4.

- This finding is surprising because vocabulary scores from the same week as the
  eyetracking data are less correlated than scores from two year earlier.
- This establishes that the differences are greatest and most predictive at
  younger ages.

### Relationships with other child-level predictors

_TJM: This is where I would analyze the other test scores as we have discussed._











































## Bayesian model results

Here is the code used to fit the model with Stan. It took about 24 hours to run
the model. The regression terms have the prior Normal(0, 1)

```{r, eval = FALSE}
library(rstanarm)
options(mc.cores = parallel::detectCores())

m <- stan_glmer(
  cbind(Primary, Others) ~
    (ot1 + ot2 + ot3) * Study +
    (ot1 + ot2 + ot3 | ResearchID/Study),
  family = binomial,
  prior = normal(0, 1, autoscale = FALSE),
  prior_intercept = normal(0, 2),
  prior_covariance = decov(2, 1, 1),
  data = d_m)
readr::write_rds(m, "./data/stan_aim1_cubic_model.rds.gz")
```



The output below contains the model quick view, a summary of the fixed effect
terms, and a summary of the priors used.

```{r}
b

summary(b, pars = names(fixef(b)))

prior_summary(b)
```






Let's try to understand our model by making some plots.





### Plot the intervals for the random effect parameters

These are the parameters governing the random effect distributions. First, we
plot the standard deviations. Recall that in our hierarchical model we suppose 
that each growth curve is drawn from a population of related curves. The 
model's fixed effects estimate the means of the distribution. These terms
estimate the variability around that mean. We did not have any a priori 
hypotheses about the values of these scales, so do not discuss them any further.

```{r posterior-sds, echo = FALSE, out.width = "80%", fig.height = 2.5, fig.width = 5}
sdcors <- tristan::draw_var_corr(b)
sdcors_wide <- sdcors %>% 
  select(.draw, .parameter, sdcor) %>% 
  tidyr::spread(.parameter, sdcor) %>% 
  select(-.draw)

# Create the mathematical labels for parameters
group_info <- sdcors %>% 
  select(.parameter:var2) %>% 
  distinct()
group_info$group <- group_info$grp %>% 
  stringr::str_replace("Study:ResearchID", "Child-Study") %>% 
  stringr::str_replace("ResearchID", "Child") 

group_info$r <- ifelse(
  is.na(group_info$var2), "", 
  paste0(",", group_info$var2))

group_info$sym <- ifelse(is.na(group_info$var2), "sigma", "rho")

group_info$var1 <- ifelse(
  group_info$var1 == "(Intercept)", "Intercept", 
  group_info$var1)

group_info$math <- sprintf(
  "%s[list(%s%s)]", group_info$sym, group_info$var1, group_info$r)

group_info$class <- ifelse(is.na(group_info$var2), "scale", "correlation")
group_info <- group_info %>% 
  select(group, class, var1, var2, parameter = .parameter, math) %>% 
  mutate(parameter = as.factor(parameter))

intervals <- as.data.frame(sdcors_wide) %>% 
  mcmc_intervals_data() %>% 
  left_join(group_info, by = "parameter") %>% 
  mutate(math = forcats::fct_rev(math))

ggplot(intervals %>% filter(class == "scale")) + 
  aes(y = math) +
  # Draw medians with + then draw white horizontal lines over the horizontal 
  # parts of the + symbols
  geom_point(aes(x = m), size = 3, shape = 3) +
  geom_hline(aes(yintercept = as.numeric(parameter)), color = "white") +
  geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  scale_y_discrete(labels = parse_text) +
  facet_wrap("group", ncol = 1, strip.position = "left") + 
  theme(
    strip.placement = "outside", 
    strip.background = element_rect(fill = NA),
    axis.text.y = element_text(size = rel(1.2))) +
  labs(
    x = NULL, y = NULL, 
    title = "Random effect scales", 
    caption = "Posterior median with 90% and 50% intervals")
```

Then the correlations.

```{r posterior-cors, echo = FALSE, out.width = "80%", fig.height = 3, fig.width = 5}
ggplot(intervals %>% filter(class == "correlation")) + 
  aes(y = math) +
  geom_vline(xintercept = 0, size = 2, color = "white") +
  # Draw medians with + then draw white horizontal lines over the horizontal 
  # parts of the + symbols
  geom_point(aes(x = m), size = 3, shape = 3) +
  geom_hline(aes(yintercept = as.numeric(parameter)), color = "white") +
  geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  scale_y_discrete(labels = parse_text) +
  facet_wrap("group", ncol = 1, strip.position = "left") + 
  theme(
    strip.placement = "outside", 
    strip.background = element_rect(fill = NA),
    axis.text.y = element_text(size = rel(1.2))) +
  labs(
    x = NULL, y = NULL, 
    title = "Random effect correlations", 
    caption = "Posterior median with 90% and 50% intervals")
```









### Posterior predictive checks

Bayesian models are generative; they describe how the data could have been
generated. One way to evaluate the model is to have it simulate new
observations. If the simulated data closely resembles the observed data, then we
have some confidence that our model has learned an approximation of how the data
could have been generated. Figure \@ref(fig:post-pred) depicts the density of
the observed data from each year of the study versus 200 posterior simulations.
Because the simulations closely track the density of the observed data, we can
infer that the model has learned how to generate data from each year of the
study.

(ref:post-pred) Posterior predictive density for the observed data from each
year of the study. The _x_-axis represents the outcome measure---the proportion
of looks to the target image---and the _y_-axis is the density of those values
at year. At age 3, there is a large density of looks around chance performance
(.25) with a rightward skew (above-chance looks are common). At age 4 and age 5,
a bimodal distribution emerges, reflecting how looks start at chance and
reliably increase to above-chance performance. Each light line is a simulation
of the observed data from the model, and the thick lines are the observed data.
Because the thick line is surrounded by light lines, we visually infer that the
the model faithfully approximates the observed data.

```{r post-pred, fig.cap = "(ref:post-pred)", echo = FALSE, out.width = "80%", fig.height=3, fig.width=6}
sims <- rstanarm::posterior_predict(b, draws = 200, seed = "09272017")
n_trials <- data_frame(y_id = seq_along(b$y[, 1]), n_trials = rowSums(b$y))

bayesplot:::ppc_data(b$y[, 1], sims, group = d_m$Study) %>% 
  left_join(n_trials, by = "y_id") %>% 
  mutate(prop = value / n_trials) %>% 
  ggplot() +
    aes(x = prop) +
    stat_density(
      aes_(group = ~ rep_id),
      data = function(x) dplyr::filter(x, !.data$is_y),
      geom = "line", position = "identity", size = .25, alpha = .1, 
      color = "#0074D9") +
    stat_density(
      data = function(x) dplyr::filter(x, .data$is_y),
      geom = "line", position = "identity", size = 1) + 
    labs(
      x = "Proportion of looks", 
      y = "Density",
      title = "Observed data and 200 posterior simulations") + 
    coord_cartesian(xlim = c(0, 1), expand = FALSE) + 
    facet_wrap("group")
```


We can ask the model make even more specific posterior predictions. Below we
plot the posterior predictions for random participants. This is the model
simulating new data for these participants.

```{r posterior-lines, out.width = "100%", fig.height=4, fig.width=5}
set.seed(09272017)

ppred <- d_m %>% 
  sample_n_of(8, ResearchID) %>% 
  tristan::augment_posterior_predict(b, newdata = ., nsamples = 100) %>% 
  mutate(trials = Primary + Others)

ggplot(ppred) + 
  aes(x = Time, y = Prop, color = Study, group = Study) + 
  geom_line(aes(y = .posterior_value / trials, 
                group = interaction(.draw, Study)), 
            alpha = .20) + 
  geom_line(size = 1, color = "grey50") + 
  facet_wrap("ResearchID") + 
  theme(
    legend.position = c(.95, 0), 
    legend.justification = c(1, 0),
    legend.margin = margin(0)) +
  guides(color = guide_legend(title = NULL, override.aes = list(alpha = 1))) +
  labs(
    title = "Observed means and 100 simulations of new data",
    x = "Time after target onset [ms]",
    y = "Proportion looks to target") 
```

Or we can plot the linear predictions. These are posterior predictions of the
log-odds of looking to target before adding binomial noise.

```{r posterior-mean-lines, out.width = "100%", fig.height=4, fig.width=5}
lpred <- d_m %>% 
  sample_n_of(8, ResearchID) %>% 
  tristan::augment_posterior_linpred(b, newdata = ., nsamples = 100)

ggplot(lpred) + 
  aes(x = Time, y = .posterior_value, color = Study) +
  geom_line(aes(group = interaction(Study, ResearchID, .draw)), 
            alpha = .1) +
  facet_wrap("ResearchID") + 
  geom_point(aes(y = qlogis(Prop)), shape = 1) + 
  theme(
    legend.position = c(.95, 0), 
    legend.justification = c(1, 0),
    legend.margin = margin(0)) +
  guides(color = guide_legend(title = NULL, override.aes = list(alpha = 1))) +
  labs(
    title = "Observed data and 100 posterior predictions",
    x = "Time after target onset [ms]",
    y = "Posterior log-odds")
```








### Formal model specification

<div class = "infobox">

**Box 1: A brief comment about priors.**

Bayesian models require prior information (“priors”). Priors are also
commonly referred to as "prior beliefs", and Bayesian techniques are
criticized or dismissed for smuggling subjectivity into the scientific
enterprise. I find this unfortunate on two grounds. First, *belief*
overstates things. As George Box said, "all models are wrong, but some
are useful" [cite], so no part of a statistical model should be called
a "belief" when the whole thing is a convenient fiction. That's why I
prefer the term *prior information*. [Hat tip Gelman?] Second, other
parts of the statistical model are also subjective: likelihood
functions, what kind of ANOVA, what to covary, whether to transform
measurements, whether a *p* = .07 is a "marginal" effect or no effect at
all, and so on. This subjectivity seems reasonable, provided that we
scientists are open about modeling decisions.

For these models, I will use weakly to moderately informative priors.
For example, suppose *x* and *y* are scaled to mean 0 and standard
deviation 1. A weakly informative prior for the effect of *x* on *y*
might be Normal(0, 5)—a normal distribution with mean 0 and standard
deviation 5. If we fit a regression model and observed an effect size
of 12 SD units, our first assumption would be that something went wrong
with our software. The weakly informative prior captures this level of
prior information. A moderately informative prior would be Normal(0, 1).
This prior information captures our disciplinary experience that effect
sizes greater than ±1 relatively uncommon in child language research. A
strongly informative prior for this effect might be something like
Normal(.4, .1) which says that our model should be very skeptical of
negative effects and of effects larger than .8. For this project, I will
default to the first two levels of prior information.

</div>


_TJM: Ignore this section for now. It might go to an appendix. Basically, I
think I need to briefly summarize and justify the priors used in the analysis._

We used moderately informative priors for the main regression effects.

* b ~ Normal(mean = 0, sd = 1)

When we computed the orthogonal polynomial features for Time, they were rescaled
so that the linear feature ranged from −.5 to .5. Under this scaling a unit
change in Time^1^ was equal to change from the start to the end of the analysis
window. The polynomial features for the Time had the following ranges:

```{r}
d_m %>% 
  distinct(ot1, ot2, ot3) %>% 
  tidyr::gather("Feature", "Value") %>% 
  group_by(Feature) %>% 
  summarise(Min = min(Value), Max = max(Value), Range = Max - Min) %>% 
  mutate_if(is.numeric, round, 2) %>% 
  mutate(Feature = stringr::str_replace(Feature, "ot(\\d)", "Time^\\1^")) %>% 
  knitr::kable()
```

Under the Normal(0, 1) prior, before seeing any data, we expect 95% of plausible
effects to fall in the range ±1.96, which is an adequate range for these
growth curve models. For example, consider just the effect of Time^1^. If a
listener starts at chance performance, 25% or `r qlogis(.25) %>% round(2)`
logits, and increases to, say, 65% or `r qlogis(.65) %>% round(2)`, the effect
of a unit change in Time^1^ would be a change of
`r (qlogis(.65) - qlogis(.25)) %>% round(2)` logits. This magnitude of effect is
accommodated by our Normal(0, 1) prior. 

_Here I would have to also describe the random effects structure.


For the hierarchical part of the model, I used RStanARM's `decov()` prior which
simultaneously sets a prior of the variances and correlations of the model's
random effect terms. For these terms, I used the default prior for the variance
terms and used a weakly informative LKJ(2) prior on the random effect
correlations. Under LKJ(1) supports all correlations in the range ±1, but under
LKJ(2) extreme correlations are less plausible. In the figure below, we see that
the LKJ(2) prior nudges some of the probability mass away from ±1 towards the
center. The motivation for this kind of prior was *regularization*: We give the
model a small amount of information to nudge it away from extreme, degenerate
values.

```{r, include = FALSE, fig.height = 3, fig.width = 5}
lkj1 <- rstanarm::stan_glmer(
  Sepal.Length ~ 1 + (Sepal.Width + Petal.Length | Species), 
  prior = normal(0, 1),
  prior_covariance = decov(1,1,1,1),
  data = iris, 
  prior_PD = TRUE)

lkj2 <- rstanarm::stan_glmer(
  Sepal.Length ~ 1 + (Sepal.Width + Petal.Length | Species), 
  prior = normal(0, 1),
  prior_covariance = decov(2,1,1,1),
  data = iris, 
  prior_PD = TRUE)

big_dumb_model1 <- tristan::draw_var_corr(lkj1, nsamples = 2000)
big_dumb_model2 <- tristan::draw_var_corr(lkj2, nsamples = 2000)
nrow(big_dumb_model2)

correlations <- bind_rows(
  big_dumb_model1 %>% mutate(Model = "LKJ(1)"), 
  big_dumb_model2 %>% mutate(Model = "LKJ(2)"))
```

```{r, echo = FALSE,  out.width = "80%", fig.height = 3, fig.width = 5}
ggplot(correlations %>% filter(!is.na(var2))) + 
  aes(x = sdcor) + 
  geom_histogram(binwidth = .1, boundary = 0) + 
  facet_wrap("Model", labeller = label_both) +
  labs(
    x = NULL, y = "Num. prior samples", 
    caption = "12,000 prior samples drawn per model")

ggplot(correlations %>% filter(!is.na(var2))) + 
  aes(x = sdcor) + 
  geom_density() + 
  facet_wrap("Model", labeller = label_both) +
  labs(
    x = NULL, y = "Num. prior samples", 
    caption = "12,000 prior samples drawn per model")
```
