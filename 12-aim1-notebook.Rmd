Analysis of familiar word recognition
===========================================================================

```{r include = FALSE}
d_m <- readr::read_csv("./data/aim1-model-ready.csv.gz")
knitr::read_chunk("./helpers.R")
if (interactive()) source("./helpers.R")
```

```{r helpers, include = FALSE}
```

```{r, include = FALSE}
library(rstanarm)
library(bayesplot)
theme_set(theme_grey())
library(stringr)
library(ggstance)
parse_text <- function(x) parse(text = x)

b <- readr::read_rds("./data/stan_aim1_cubic_model.rds.gz")
b$stan_function <- "stan_glmer"
```


General Todos:

* Determine and use consistent terminology for the studies and growth curve features.

## Growth curve analysis

Looks to the familiar image were analyzed using **Bayesian mixed
effects logistic regression**. We used *logistic* regression because
the outcome measurement is a probability (the log-odds of looking to the
target image versus a distractor). We used *mixed-effects* models
to estimate a separate growth curve for each child (to
measure individual differences in word recognition) but also treat each
child's individual growth curve as a draw from a distribution of related
curves.

We used *Bayesian* techniques to study a generative model of the
data. Instead of reporting and describing a single, best-fitting model
of some data, Bayesian methods consider an entire distribution of
plausible models that are consistent with the data and any prior
information we have about the models. By using this approach, one can
explicitly quantify uncertainty about statistical effects and draw
inferences using estimates of uncertainty (instead of using statistical
significance—which is not a straightforward matter for mixed-effects
models).[^2]

[^2]: It is tempting to further justify this approach by comparing
    Bayesian versus classical/frequentist statistics, but my goals in
    using this method are simple: To estimate statistical effects and
    quantify uncertainty about those effects. This pragmatic brand of
    Bayesian statistics is illustrated in texts by @GelmanHill and 
    @RethinkingBook.

The eyetracking growth curves were fit using an orthogonal cubic polynomial
function of time [a now-conventional approach; see @Mirman2014]. Put
differently, we modeled the probability of looking to the target during an
eyetracking task as:

$$
\text{log-odds}(\mathit{looking}) = \beta_0 + \beta_1 * \textit{Time}^1 +  \beta_2 * \textit{Time}^2 +   \beta_3 * \textit{Time}^3
$$

That the time terms are *orthogonal* means that $\textit{Time}^1$, $\textit{Time}^2$ and
$\textit{Time}^3$ are transformed so that they are uncorrelated. Under this
formulation, the parameters $\beta_0$ and $\beta_1$ have a clear
interpretation in terms of lexical processing performance. The
intercept, $\beta_0$, measures the area under the growth curve—or the
probability of fixating on the target word averaged over the whole
window. We can think of $\beta_0$ as a measure of 
*word recognition reliability*. The linear time parameter, $\beta_1$,
estimates the steepness of the growth curve—or how the probability of
fixating changes from frame to frame. We can think of $\beta_1$ as a
measure of *processing efficiency*, because growth curves with stronger
linear features exhibit steeper frame-by-frame increases in looking
probability.[^3]

[^3]: The polynomial other terms are less important—or rather, they have
    do not map as neatly onto behavioral descriptions as the accuracy
    and efficiency parameters. The primary purpose of quadratic and
    cubic terms is to ensure that the estimated growth curve adequately
    fits the data. In this kind of data, there is a steady baseline at
    chance probability before the child hears the word, followed a
    window of increasing probability of fixating on the target as the
    child recognizes the word, followed by a period of plateauing and
    then diminishing looks to target. The cubic polynomial allows the
    growth curve to be fit with two inflection points: the point when
    the looks to target start to increase from baseline and the point
    when the looks to target stops increasing.

We studied how word recognition changes
over time by modeling how growth curves change over developmental time.
This amounted to studying how the growth curve parameters changes year over
year. We included dummy-coded indicators for Year 1, Year 2, and 
Year 3 and having these indicators interact with the growth curve parameters. 
We also included random effects to represent child-by-study effects.

### Formal model specification

We used moderately informative priors for the main regression effects.

* b ~ Normal(mean = 0, sd = 1)

When we computed the orthogonal polynomial features for Time, they were rescaled
so that the linear feature ranged from −.5 to .5. Under this scaling a unit
change in Time^1^ was equal to change from the start to the end of the analysis
window. The polynomial features for the Time had the following ranges:

```{r}
d_m %>% 
  distinct(ot1, ot2, ot3) %>% 
  tidyr::gather("Feature", "Value") %>% 
  group_by(Feature) %>% 
  summarise(Min = min(Value), Max = max(Value), Range = Max - Min) %>% 
  mutate_if(is.numeric, round, 2) %>% 
  mutate(Feature = stringr::str_replace(Feature, "ot(\\d)", "Time^\\1^")) %>% 
  knitr::kable()
```

Under the Normal(0, 1) prior, before seeing any data, we expect 95% of plausible
effects to fall in the range ±1.96, which is an adequate range for these
growth curve models. For example, consider just the effect of Time^1^. If a
listener starts at chance performance, 25% or `r qlogis(.25) %>% round(2)`
logits, and increases to, say, 65% or `r qlogis(.65) %>% round(2)`, the effect
of a unit change in Time^1^ would be a change of
`r (qlogis(.65) - qlogis(.25)) %>% round(2)` logits. This magnitude of effect is
accommodated by our Normal(0, 1) prior. 


### Growth curve features as measures of individual performance

As noted above, two of the model's growth curve features have direct
interpretations in terms of  lexical processing performance: The model's
intercept parameter corresponds to the average proportion of looks to the named
image of the trial window and the linear time parameter corresponds to slope of
the growth curve or lexical processing efficiency. We also may be interested in
_peak_ proportion of looks to the target. We derived this value from the growth
curve by taking the median of the five points on a growth curve. Figure _TODO_
shows three simulated growth curves and how each of these growth curve features
relate to word recognition performance.

```{r curve-features, fig.show = 'hold', echo = FALSE, out.width = "80%", fig.height = 6, fig.width = 6}
dummy_data <- d_m %>% 
  filter(Study == "TimePoint2") %>% 
  distinct(Study, Time, ot1, ot2, ot3) %>% 
  mutate(
    ResearchID = "NEW", 
    Primary = 0, 
    Others = 0)

set.seed(11102017)

lpred <- dummy_data %>% 
  tristan::augment_posterior_linpred(model = b, newdata = ., nsamples = 3)

lpred_lm <- lpred %>% 
  tidyr::nest(-.draw) %>% 
  mutate(
    model = purrr::map(data, ~ lm(.posterior_value ~ ot1 + ot2 + ot3, .x)),
    coef = lapply(model, coef),
    auc = purrr::map_dbl(coef, purrr::pluck, 1),
    slope = purrr::map_dbl(coef, purrr::pluck, 2)) %>% 
  select(-model, -coef) %>% 
  tidyr::unnest(data)


p1 <- ggplot(lpred_lm) + 
  aes(x = ot1, y = .posterior_value, group = .draw) + 
  geom_line(linetype = "dashed") + 
  geom_hline(
    aes(yintercept = auc), 
    color = "red", size = .75) + 
  labs(
    x = "Time",
    y = "Logodds looking to familiar",
    title = "Intercept term represents average probability") + 
  facet_wrap(".draw") + 
  theme_grey(base_size = 9) + 
  theme(
    strip.text = element_blank(), 
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank())


p2 <- ggplot(lpred_lm) + 
  aes(x = ot1, y = .posterior_value, group = .draw) + 
  geom_line(linetype = "dashed") + 
  geom_abline(
    aes(intercept = auc, slope = slope),
    color = "red", size = .75) + 
  facet_wrap(".draw") + 
  labs(
    x = "Time",
    y = "Logodds looking to familiar",
    title = "Linear time term conveys processing efficiency") +
  theme_grey(base_size = 9) + 
  theme(
    strip.text = element_blank(), 
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank())


p3 <- ggplot(lpred_lm) + 
  aes(x = ot1, y = .posterior_value, group = .draw) + 
  geom_line(linetype = "dashed") + 
  geom_point(
    data = lpred_lm %>% group_by(.draw) %>% top_n(5, .posterior_value), 
    size = 2) + 
  geom_hline(
    aes(yintercept = .posterior_value), 
    data = lpred_lm %>% 
      group_by(.draw) %>% 
      top_n(5, .posterior_value) %>% 
      summarise(.posterior_value = median(.posterior_value)), 
    size = .75, color = "red") + 
  facet_wrap(".draw") + 
  labs(
    x = "Time",
    y = "Logodds looking to familiar",
    title = "Median of top five points provides peak probability") + 
  theme_grey(base_size = 9) + 
  theme(
    strip.text = element_blank(), 
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank())

cowplot::plot_grid(p1, p2, p3, ncol = 1)
```

_TODO: CAPTION_




### Year over year changes in word recognition




```{r average-growth-curves, fig.cap = "", echo = FALSE, out.width = "50%", fig.height=3, fig.width=4}
dummy_data <- d_m %>% 
  distinct(Study, Time, ot1, ot2, ot3) %>% 
  mutate(
    ResearchID = "NEW", 
    Primary = 0, 
    Others = 0)

set.seed(11102017)
lpred <- dummy_data %>% 
  tristan::augment_posterior_linpred(model = b, newdata = ., re.form = NA)

peaks <- lpred %>% 
  group_by(Study, .draw) %>% 
  top_n(5, .posterior_value) %>% 
  mutate(med = median(.posterior_value)) %>% 
  select(.draw, Study, peek_accuracy = med) %>% 
  distinct() %>% 
  ungroup() %>% 
  mutate(peek_accuracy = peek_accuracy) %>% 
  tidyr::spread(Study, peek_accuracy) %>% 
  select(
    `Peak~~(TP1)` = TimePoint1,
    `Peak~~(TP2)` = TimePoint2,
    `Peak~~(TP3)` = TimePoint3)

lpred %>% 
  tjmisc::sample_n_of(200, .draw) %>% 
  ggplot() + 
    aes(x = Time, y = plogis(.posterior_value), color = Study) +
    geom_hline(yintercept = .25, color = "white", size = 2) +
    geom_line(
      aes(group = interaction(Study, ResearchID, .draw)), 
      alpha = .1) +
    stat_summary(
      aes(y = Prop, group = Study), 
      data = d_m, fun.y = "mean", geom = "line", 
      color = "grey50", size = 1) + 
    theme(
      legend.position = c(.95, 0), 
      legend.justification = c(1, 0),
      legend.margin = margin(0)) +
    guides(
      color = guide_legend(title = NULL, override.aes = list(alpha = 1))) +
    labs(
      title = "Observed means and 200 posterior predictions of mean",
      x = "Time after target onset",
      y = "Proportion of looks")
```

We first consider how the shape of the average growth curves change each year.
Figure \@ref(fig:effects2) depicts uncertainty intervals with the model's
average effects at each timepoint on the growth curve features. The intercept
and time effects increased each year, confirming that children get more reliable
and faster at recognizing words as they grow older. The peak accuracy also
increases each year. For each effect, the change from year\ 1 to year\ 2 is
approximately the same as the change from year\ 2 to year\ 3, as visible in
figure \@ref(fig:pairwise-effects).


(ref:effects2) Uncertainty intervals for the effects of study timepoints on
growth curve features.

```{r effects2, fig.cap = "(ref:effects2)", echo = FALSE, out.width = "80%", fig.height=4, fig.width=5}
# Column names will have mathematical formatting too
draws <- as.data.frame(b) %>% 
  as_tibble() %>% 
  transmute(
    `Intercept~~(TP1)` = `(Intercept)`,
    `Intercept~~(TP2)` = `(Intercept)` + StudyTimePoint2,
    `Intercept~~(TP3)` = `(Intercept)` + StudyTimePoint3,
    `Time~~(TP1)` = ot1,
    `Time~~(TP2)` = ot1 + `ot1:StudyTimePoint2`,
    `Time~~(TP3)` = ot1 + `ot1:StudyTimePoint3`,
    `Time^2~~(TP1)` = ot2,
    `Time^2~~(TP2)` = ot2 + `ot2:StudyTimePoint2`,
    `Time^2~~(TP3)` = ot2 + `ot2:StudyTimePoint3`,
    `Time^3~~(TP1)` = ot3,
    `Time^3~~(TP2)` = ot3 + `ot3:StudyTimePoint2`,
    `Time^3~~(TP3)` = ot3 + `ot3:StudyTimePoint3`) %>% 
  bind_cols(peaks)

logodds <- draws %>% 
  select(starts_with("Time")) %>% 
  mcmc_intervals_data(prob = 0.5, prob_outer = 0.9) %>% 
  mutate(Scale = "Log-odds") %>% 
  mutate(parameter = factor(parameter, levels = rev(names(draws))))

props <- draws %>% 
  select(starts_with("Intercept"), starts_with("Peak")) %>% 
  mutate_all(plogis) %>% 
  mcmc_intervals_data(prob = 0.5, prob_outer = 0.9) %>% 
  mutate(Scale = "Proportion") %>% 
  mutate(parameter = factor(parameter, levels = rev(names(draws))))

intervals2 <- bind_rows(props, logodds) %>% 
  group_by(Scale) %>% 
  mutate(min_x = min(ll), max_x = max(hh)) %>% 
  ungroup()

ggplot(intervals2) + 
  aes(y = parameter) +
  geom_vline(xintercept = 0, size = 2, color = "white") +
  geom_point(aes(x = m), size = 3, shape = 3) + 
  # Draw mock gridlines in case the point's + shape is wider than the interval.
  # All that's visible is then a | with a gap in the middle.
  geom_segment(
    aes(y = parameter, yend = parameter, x = min_x, xend = max_x), 
    size = .25, color = "white") +
  ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  scale_y_discrete(labels = parse_text) + 
  labs(
    x = NULL, 
    y = NULL, 
    title = "Average effects by study",
    caption = "Posterior median with 90% and 50% intervals") + 
  facet_wrap("Scale", scales = "free", ncol = 1, labeller = label_both)
```



(ref:pairwise-effects) Uncertainty intervals for the differences between study
timepoints.

```{r pairwise-effects, fig.cap = "(ref:pairwise-effects)", fig.show = 'hold', echo = FALSE, out.width = "50%", fig.height = c(4, 3), fig.width = c(4, 4)}
clean_names <- . %>% 
  stringr::str_replace_all("[()]", "") %>% 
  stringr::str_replace("~~", "_")

pairwise <- draws %>% 
  mutate_at(vars(starts_with("Intercept"), starts_with("Peak")), plogis) %>% 
  tibble::rowid_to_column(".draw") %>% 
  set_names(clean_names) %>% 
  tidyr::gather(parameter, value, -.draw) %>% 
  tidyr::separate(parameter, c("parameter", "year"), sep = "_") %>% 
  compare_pairs(year, value) %>% 
  tidyr::spread(pair, value) %>% 
  split(.$parameter) %>% 
  lapply(
    . %>% select(-.draw, -parameter) %>% 
      mcmc_intervals_data(prob = 0.5, prob_outer = 0.9) %>% 
      rename(pair = parameter)) %>% 
  bind_rows(.id = "parameter")

ggplot(pairwise %>% filter(!(parameter %in% c("Peak", "Intercept")))) + 
  aes(y = forcats::fct_rev(pair)) +
  geom_vline(xintercept = 0, size = 2, color = "white") +
  ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  geom_point(aes(x = m), size = 3, shape = 3) + 
  facet_wrap(
    "parameter", ncol = 1, strip.position = "left", 
    labeller = label_parsed) + 
  labs(
    x = NULL, y = NULL, 
    title = "Differences in average effects",
    caption = "Posterior median with 90% and 50% intervals") + 
  theme(
    strip.placement = "outside", 
    strip.background = element_rect(fill = NA),
    axis.text.y = element_text(size = rel(1.2)))

ggplot(pairwise %>% filter(parameter %in% c("Peak", "Intercept"))) + 
  aes(y = forcats::fct_rev(pair)) +
  geom_vline(xintercept = 0, size = 2, color = "white") +
  ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  geom_point(aes(x = m), size = 3, shape = 3) + 
  facet_wrap(
    "parameter", ncol = 1, strip.position = "left", 
    labeller = label_parsed) + 
  labs(
    x = NULL, y = NULL, 
    title = " ",
    caption = "Posterior median with 90% and 50% intervals") + 
  theme(
    strip.placement = "outside", 
    strip.background = element_rect(fill = NA),
    axis.text.y = element_text(size = rel(1.2)))
```

```{r, include = FALSE}
pluck <- purrr::pluck
  
lapply(. %>% )

get_pts <- . %>% 
  lapply(. %>% pluck("m") %>% round(3))

get_uis <- . %>% 
  lapply(. %>% glue::glue_data("{round(ll,3)}--{round(hh,3)}")) 

get_cor_pts <- function(x) {
  format_cor <- . %>%
    round(3) %>% 
    printy::fmt_leading_zero() %>% 
    printy::fmt_minus_sign()
  
  x %>% 
    lapply(. %>% pluck("m") %>% format_cor())
}
  
get_cor_uis <- function(x) {
  format_cor <- . %>%
    round(3) %>% 
    printy::fmt_leading_zero() %>% 
    printy::fmt_minus_sign()
  
  x %>% 
    lapply(. %>% glue::glue_data("{format_cor(ll)}--{format_cor(hh)}")) 
}

props <- props %>% 
  mutate(
    parameter = parameter %>% 
      stringr::str_replace("~~.", "_") %>% 
      stringr::str_replace("\\)", "")) %>% 
  tidyr::separate(parameter, into = c("parameter", "study"))

prop_diffs <- pairwise %>% 
  filter(parameter %in% c("Intercept", "Peak"))

prop_list1 <- props %>% 
  filter(parameter == "Intercept") %>% 
  split(.$study) %>% 
  set_names(str_extract, "TP.")

prop_list2 <- props %>% 
  filter(parameter == "Peak") %>% 
  split(.$study) %>% 
  set_names(str_extract, "TP.")

int_pts <- get_pts(prop_list1)
int_uis <- get_uis(prop_list1)
peak_pts <- get_pts(prop_list2)
peak_uis <- get_uis(prop_list2)

diff_list1 <- prop_diffs %>% 
  filter(parameter == "Intercept") %>% 
  split(.$pair) %>% 
  set_names(str_replace, "-", "_")

diff_list2 <- prop_diffs %>% 
  filter(parameter == "Peak") %>% 
  split(.$pair) %>% 
  set_names(str_replace, "-", "_")

int_dpts <- get_pts(diff_list1)
int_duis <- get_uis(diff_list1)
peak_dpts <- get_pts(diff_list2)
peak_duis <- get_uis(diff_list2)
```

The average reliability was `r int_pts$TP1` [90% UI: `r int_uis$TP1`] for
timepoint 1, `r int_pts$TP2` [`r int_uis$TP2`] for timepoint 2, and
`r int_pts$TP3` [`r int_uis$TP3`] for timepoint 3. The averages increased by
`r int_dpts$TP2_TP1` [`r int_duis$TP2_TP1`] from timepoint 1 to timepoint 2
and by `r int_dpts$TP3_TP2` [`r int_duis$TP3_TP2`] from timepoint 2 to
timepoint 3. The peak accuracy was `r peak_pts$TP1` [90% UI: `r peak_uis$TP1`]
for timepoint 1, `r peak_pts$TP2` [`r peak_uis$TP2`] for timepoint 2, and
`r peak_pts$TP3` [`r peak_uis$TP3`] for timepoint 3. The peak values increased
by `r peak_dpts$TP2_TP1` [`r peak_duis$TP2_TP1`] from timepoint 1 to
timepoint 2 and by `r peak_dpts$TP3_TP2` [`r peak_duis$TP3_TP2`] from
timepoint 2 to timepoint 3. These results numerically confirm the hypothesis
that children would improve in their word recognition reliability, both in terms
of average looking and in terms of peak accuracy, each year.


### Are individual differences stable over time?

_TJM: This section is in really good shape._

```{r compute-kendalls, include = FALSE}
fits <- readr::read_csv("./data/fits.csv.gz") %>%
  semi_join(d_m)

peaks <- readr::read_csv("./data/peaks.csv.gz") %>%
  rename(.posterior_value = Peak_Logit) %>% 
  mutate(coef = "peak_logit") %>% 
  semi_join(d_m)

fits <- bind_rows(fits, peaks)
rm(peaks)

# Compute Kendall's coefficient of correspondence
tidy_kendall <- . %>%
  unclass() %>%
  as.data.frame(stringsAsFactors = FALSE)

# Add random ratings
new_coef <- fits %>%
  filter(coef == "intercept") %>%
  mutate(
    .posterior_value = runif(length(.posterior_value)),
    coef = "random values")

# Keep only data from participants who visited all three years
reduced_data <- fits %>%
  bind_rows(new_coef) %>%
  tidyr::spread(Study, .posterior_value) %>%
  tidyr::drop_na(TimePoint1:TimePoint3) 

n_rated <- n_distinct(reduced_data$ResearchID)

ws <- reduced_data %>%
  select(-ResearchID) %>%
  tidyr::nest(TimePoint1:TimePoint3) %>%
  mutate(ws = purrr::map(data, irr::kendall) %>% purrr::map(tidy_kendall)) %>%
  select(-data) %>%
  tidyr::unnest(ws)

posterior_w <- ws %>%
  select(.draw, coef, value) %>%
  tidyr::spread(coef, value) %>%
  rename(`random numbers` = `random values`) %>%
  select(-.draw)
```

We predicted that children would show stable individual differences such that
children who are faster and more reliable at recognizing words at age 3 remain
relatively faster and more reliable at age 5. To evaluate this hypothesis,
we used Kendall's _W_ (the coefficient of correspondence or concordance). This
nonparametric statistic measures the degree of agreement among _J_ judges who
are rating _I_ items. For our purposes, the items are the `r n_rated`
children who provided reliable eyetracking for all three years of the study.
(That is, we excluded children who only had reliable eyetracking data for one or
two years.) The judges are the sets of growth curve parameters from each year of 
study. For example, the intercept term provides three sets of ratings: The 
participants' intercept terms from year 1 are one set of ratings and the 
terms from years 2 and 3 provide two more sets of ratings. These three ratings
are the "judges" used to compute the intercept's _W_. Thus, we compute five 
groups of _W_ coefficients, one for each set of growth curve features: 
Intercept, Time^1^, Time^2^, Time^3^, and Peak looking probability.


```{r table-example-of-feature-ranks, echo = FALSE, eval = FALSE}
# (_Maybe: Table X illustrates some sample ratings of these participants._)

zero_pad_int <- function(xs) {
  formatter <- paste0("%0", max(nchar(xs)), "d")
  sprintf(formatter, xs)
}

val_rank <- function(xs) {
  a <- printy::fmt_minus_sign(printy::fmt_fix_digits(xs, 2))
  b <- zero_pad_int(rank(-xs))
  glue::glue("{a} ({b})")
}

reduced_data %>% 
  filter(coef == "intercept") %>% 
  group_by(ResearchID, coef) %>% 
  summarise_at(vars(starts_with("TimePoint")), median) %>% 
  ungroup() %>% 
  arrange(desc(TimePoint3)) %>% 
  mutate(
    TimePoint1 = val_rank(TimePoint1), 
    TimePoint2 = val_rank(TimePoint2), 
    TimePoint3 = val_rank(TimePoint3)) %>% 
  rename(
    `Participant ID` = ResearchID,
    `Growth curve feature` = coef,
    `Year 1` = `TimePoint1`,
    `Year 2` = `TimePoint2`,
    `Year 3` = `TimePoint3`) %>% 
  head(10) %>% 
  knitr::kable(align = c("l", "l", "r", "r", "r"))

nsamples <- nrow(as.data.frame(b))
```

Because we used a Bayesian model, we have a distribution of ratings and thus a
distribution of concordance statistics. Each sample of the posterior
distribution fits a growth curve for each child in each study, so each posterior
sample provides a set of ratings for concordance coefficients. The distribution
of *W*'s lets us quantify our uncertainty because we can compute *W*'s for each
of the `r nsamples` samples from the posterior distribution.

One final matter is how do we assess whether a concordance statistic is 
meaningful. To tackle this question, we also included a "null rater", a fake 
parameter that assigned each child in each year a random number. We can use the 
distribution of _W_'s generated by randomly rating children as a benchmark for 
assessing whether the other concordance statistics differ meaningfully from 
chance.



(ref:kendall-stats) Uncertainty intervals for the Kendall's coefficient of
concordance. Random ratings provide a baseline of null _W_ statistics. The
intercept and linear time features are decisively non-null, indicating a
significant degree of correspondence in children's relative word recognition
reliability and efficiency over three years of study.

```{r kendall-stats, fig.cap = "(ref:kendall-stats)", echo = FALSE, out.width = "80%", fig.height = 3, fig.width = 6}
subtitle <- glue::glue(
  "Kendall's W. Raters: 3 time points. Items: {n_rated} children.")

w_intervals <- posterior_w %>% 
  rename(
    Intercept = `intercept`, `Time` = ot1, `Time^2` = ot2, `Time^3` = ot3,
    Peak = peak_logit, `Random~ratings` = `random numbers`) %>% 
  mcmc_intervals_data(prob_outer = .9, prob = .5) %>% 
  mutate(parameter = forcats::fct_rev(parameter)) 

ggplot(w_intervals) + 
  aes(y = parameter) +
  geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  geom_point(aes(x = m), size = 3, shape = 3) + 
  scale_y_discrete(labels = parse_text) + 
  labs(
    x = NULL, 
    y = NULL, 
    title = "Concordance coefficients for growth curve features",
    caption = "Posterior median with 90% and 50% intervals",
    subtitle = subtitle)

# TODO format as correlations
w_pts <- w_intervals %>% 
  split(.$parameter) %>% 
  get_pts()

w_uis <- w_intervals %>% 
  split(.$parameter) %>% 
  get_uis()
```

We used the `kendall()` function in the `irr` package (vers.
`r packageVersion("irr")`, CITATION) to compute concordance statistics.
Figure \@ref(fig:kendall-stats) depicts uncertainty intervals for the Kendall
_W_'s for these growth curve features. The 90% uncertainty interval of _W_
statistics from random ratings [`r w_uis[["Random~ratings"]]`] subsumes the
intervals for the Time^2^ effect [`r w_uis[["Time^2"]]`] and the Time^3^ effect
[`r w_uis[["Time^3"]]`], indicating that these values do not differentiate
children in a longitudinally stable way. That is, the Time^2^ and Time^3^
features differentiate children across studies as well as random numbers.
Earlier, we stated that only the intercept, linear time, and peak features have
psychologically meaningful interpretations and that the higher-order features of
these models serve to capture the shape of the growth curve data. These
statistics support that assertion.

Concordance is strongest for the peak feature, _W_\ = `r w_pts[["Peak"]]` 
[`r w_uis[["Peak"]]`] and the intercept term, _W_\ = `r w_pts[["Intercept"]]`
[`r w_uis[["Intercept"]]`], followed by the linear time term, _W_\ =
`r w_pts[["Time"]]` [`r w_uis[["Time"]]`]. Because these values are removed 
from the statistics for random ratings, we conclude that there is a credible
degree of correspondence across studies when we rank children using their peak
looking probability, average look probability (the intercept) or their growth
curve slope (linear time).

**Summary**. Growth curve features reflect individual differences in word
recognition reliability and efficiency. By using Kendall's *W* to measure the
degree of concordance among growth curve features over developmental time, we
tested whether individual differences in lexical processing persisted over
development. We found that the peak, intercept and linear time features were
stable over time.

```{r}
rm(ws, reduced_data, posterior_w)
```


### Predicting the future

```{r vocab correlations, include = FALSE}
if (!exists("fits")) {
  fits <- readr::read_csv("./data/fits.csv.gz") %>%
    semi_join(d_m)

  peaks <- readr::read_csv("./data/peaks.csv.gz") %>%
    rename(.posterior_value = Peak_Logit) %>% 
    mutate(coef = "peak_logit") %>% 
    semi_join(d_m)

  fits <- bind_rows(fits, peaks)
  rm(peaks)
}

scores <- readr::read_csv("./data-raw/test_scores.csv") %>% 
  select(
    Study, ResearchID, Age, EVT_GSV, EVT_Standard, PPVT_GSV, PPVT_Standard)

wide_scores <- scores %>% 
  tidyr::gather("Test", "Value", -ResearchID, -Study) %>% 
  tidyr::unite("Col", Study, Test) %>% 
  tidyr::spread(Col, Value)

cor_complete <- function(...) cor(..., use = "pairwise.complete")

vocab_cors <- fits %>% 
  left_join(wide_scores) %>% 
  group_by(Study, coef, .draw) %>% 
  summarise(
    r_TimePoint3_EVT_Standard = .posterior_value %>% 
      cor_complete(TimePoint3_EVT_Standard),
    r_TimePoint3_EVT_GSV = .posterior_value %>% 
      cor_complete(TimePoint3_EVT_GSV),
    r_TimePoint2_PPVT_GSV = .posterior_value %>% 
      cor_complete(TimePoint2_PPVT_GSV),
    r_TimePoint2_PPVT_Standard = .posterior_value %>% 
      cor_complete(TimePoint2_PPVT_Standard)) %>% 
  ungroup()
```

```{r correlation intervals, include = FALSE}
# Compute the intervals of the correlations
c_intervals <- vocab_cors %>%
  tidyr::nest(-Study, -coef) %>% 
  mutate(
    data = data %>% 
      purrr::map(select, -.draw) %>% 
      purrr::map(mcmc_intervals_data, prob = .5, prob_outer = .9)) %>% 
  tidyr::unnest()

# Compute pairwise differences in vocabulary correlations
vocab_cor_diffs <- vocab_cors %>% 
  tidyr::gather("variable", "correlation", -Study, -coef, -.draw) %>% 
  tidyr::nest(Study, .draw, correlation) %>% 
  mutate(data = purrr::map(data, . %>% compare_pairs(Study, correlation))) %>% 
  tidyr::unnest() %>% 
  # Negative the pairwise differences and flip the labels. 
  # E.g., if TP2-TP1 = .2, then make TP1-TP2 = -.2.
  mutate(
    value = -value,
    pair = pair %>% 
           str_replace("TimePoint(\\d)-TimePoint(\\d)", "TP\\2 - TP\\1")) %>% 
  tidyr::spread(pair, value) %>% 
  tidyr::nest(-coef, -variable) %>% 
  mutate(
    data = data %>% 
      purrr::map(select, -.draw) %>% 
      purrr::map(mcmc_intervals_data, prob = .5, prob_outer = .9)) %>% 
  tidyr::unnest() %>% 
  mutate(parameter = as.character(parameter))
  
main_diffs <- c_intervals %>% 
  mutate(
    parameter = as.character(parameter),
    variable = parameter) %>% 
  bind_rows(vocab_cor_diffs %>% mutate(Study = parameter)) %>% 
  filter(coef %in% c("intercept", "ot1", "peak_logit"))

main_diffs$Study <- main_diffs$Study %>% 
  factor(
    c("TimePoint1", "TimePoint2", "TimePoint3", 
      "TP1 - TP2", "TP2 - TP3", "TP1 - TP3"))
```


```{r correlation-texts, include = FALSE}
# EVT Standard at TimePoint3
evt_tp3_diffs <- main_diffs %>% 
  filter(variable == "r_TimePoint3_EVT_Standard")

peak_evt_tp3_data <- evt_tp3_diffs %>% 
  filter(coef == "peak_logit") %>% 
  split(.$Study) 
peak_evt_tp3_pts <- get_cor_pts(peak_evt_tp3_data) 
peak_evt_tp3_uis <- get_cor_uis(peak_evt_tp3_data)

ot1_evt_tp3_data <- evt_tp3_diffs %>% 
  filter(coef == "ot1") %>% 
  split(.$Study)
ot1_evt_tp3_pts <- get_cor_pts(ot1_evt_tp3_data) 
ot1_evt_tp3_uis <- get_cor_uis(ot1_evt_tp3_data)

int_evt_tp3_data <- evt_tp3_diffs %>% 
  filter(coef == "intercept") %>% 
  split(.$Study) 
int_evt_tp3_pts <- get_cor_pts(int_evt_tp3_data) 
int_evt_tp3_uis <- get_cor_uis(int_evt_tp3_data)


# PPVT Standard at TimePoint2
ppvt_tp2_diffs <- main_diffs %>% 
  filter(variable == "r_TimePoint2_PPVT_Standard")

peak_ppvt_tp2_data <- ppvt_tp2_diffs %>% 
  filter(coef == "peak_logit") %>% 
  split(.$Study) 
peak_ppvt_tp2_pts <- get_cor_pts(peak_ppvt_tp2_data)
peak_ppvt_tp2_uis <- get_cor_uis(peak_ppvt_tp2_data)

int_ppvt_tp2_data <- ppvt_tp2_diffs %>% 
  filter(coef == "intercept") %>% 
  split(.$Study) 
int_ppvt_tp2_pts <- get_cor_pts(int_ppvt_tp2_data)
int_ppvt_tp2_uis <- get_cor_uis(int_ppvt_tp2_data)

ot1_ppvt_tp2_data <- ppvt_tp2_diffs %>% 
  filter(coef == "ot1") %>% 
  split(.$Study) 
ot1_ppvt_tp2_pts <- get_cor_pts(ot1_ppvt_tp2_data)
ot1_ppvt_tp2_uis <- get_cor_uis(ot1_ppvt_tp2_data)
```

We hypothesized that individual differences in word recognition at age 3 will be
more discriminating and predictive future language outcomes than differences
at age 4 or age 5. To test this hypothesis, we calculated the correlations
of growth curve features with year 3 expressive vocabulary size and year 2
receptive vocabulary. (The receptive test was not administered during year 3
for logistical reasons). As with the concordance analysis, we computed each
the correlations for each sample of the posterior distribution to obtain a
distribution of correlations.




(ref:evt2-gca-cors) Uncertainty intervals for the correlations of growth curve
features at each time point with expressive vocabulary (EVT2 standard scores) at
year 3. The bottom rows provide intervals for the pairwise differences in
correlations between time points.

```{r evt2-gca-cors, fig.cap = "(ref:evt2-gca-cors)", out.width = "80%", fig.height = 3, fig.width = 6, echo = FALSE}
plot_names <- c(
  intercept = "Avg. Probability", 
  ot1 = "Linear Time",
  peak_logit = "Peak Probability")

main_diffs <- main_diffs %>% 
  mutate(plot_coef = plot_names[coef] %>% 
           factor(c("Peak Probability", "Avg. Probability", "Linear Time")))

ggplot(main_diffs %>% filter(variable == "r_TimePoint3_EVT_Standard")) + 
  aes(y = forcats::fct_rev(Study)) +
  geom_vline(xintercept = 0, size = 2, color = "white") +
  geom_point(aes(x = m), size = 3, shape = 3) + 
  geom_segment(
    aes(y = Study, yend = Study, x = min_x, xend = max_x), 
    data = . %>% 
      group_by(plot_coef) %>% 
      mutate(min_x = min(ll), max_x = max(hh)) %>% 
      ungroup(),
    size = .25, color = "white") +
  ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  facet_wrap("plot_coef") +
  labs(
    x = NULL, 
    y = NULL, 
    caption = "90% and 50% intervals") + 
  ggtitle(
    "Correlations of curve features and expressive vocabulary at timepoint 3") + 
  theme(title = element_text(size = rel(.9)))
```

(ref:ppvt4-gca-cors) Uncertainty intervals for the correlations of growth curve
features at each time point with expressive vocabulary (PPVT4 standard scores)
at year 2. The bottom row shows pairwise differences between the correlations at
year 1 and year 2.

```{r ppvt4-gca-cors, fig.cap = "(ref:ppvt4-gca-cors)", out.width = "80%", fig.height = 2, fig.width = 6, echo = FALSE}
main_diffs %>% 
  filter(variable == "r_TimePoint2_PPVT_Standard") %>% 
  filter(Study %in% c("TimePoint1", "TimePoint2", "TP1 - TP2")) %>% 
  ggplot() + 
    aes(y = forcats::fct_rev(Study)) +
    geom_vline(xintercept = 0, size = 2, color = "white") +
    geom_point(aes(x = m), size = 3, shape = 3) + 
    geom_segment(
      aes(y = Study, yend = Study, x = min_x, xend = max_x), 
      data = . %>% 
        group_by(plot_coef) %>% 
        mutate(min_x = min(ll), max_x = max(hh)) %>% 
        ungroup(),
      size = .25, color = "white") +
    ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
    ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
    facet_wrap("plot_coef") +
    labs(x = NULL, y = NULL, caption = "90% and 50% intervals") + 
    ggtitle(
      "Correlations of curve features and receptive vocabulary at timepoint 2") + 
    theme(title = element_text(size = rel(.9)))
```


Figure \@ref(fig:evt2-gca-cors) shows the correlations of the peak looking
probability, average looking probability and linear time features with
expressive vocabulary size at year 3, and Figure \@ref(fig:ppvt4-gca-cors) shows
analagous correlations for the receptive vocabulary at year 2. For all cases,
the strongest correlations were found between the growth curve features 
at year 1.
Growth curve peaks from year 1 correlated with year 3 vocabulary with _r_ =
`r peak_evt_tp3_pts$TimePoint1`, 90% UI [`r peak_evt_tp3_uis$TimePoint1`], but
the concurrent peaks from year 3 showed a correlation
of just _r_ = `r peak_evt_tp3_pts$TimePoint3`, [`r peak_evt_tp3_uis$TimePoint3`],
a difference between year 1 and year 3 of _r_<sub>TP1&minus;TP3</sub>\ =
`r peak_evt_tp3_pts[["TP1 - TP3"]]`, [`r peak_evt_tp3_uis[["TP1 - TP3"]]`].
A similar pattern held for lexical processing efficiency values. Linear time
features from year 1 correlated with year 3 vocabulary with *r* =
`r ot1_evt_tp3_pts$TimePoint1`, 90% UI [`r ot1_evt_tp3_uis$TimePoint1`],
whereas the concurrent lexical processing values from year 3 only showed a
correlation of *r* = `r ot1_evt_tp3_pts$TimePoint3`,
[`r ot1_evt_tp3_uis$TimePoint3`], a difference of *r*<sub>TP1−TP3</sub>\ =
`r ot1_evt_tp3_pts[["TP1 - TP3"]]`, [`r ot1_evt_tp3_uis[["TP1 - TP3"]]`].
For the average looking probabilities, the correlation for year 1, _r_ =
`r int_evt_tp3_pts$TimePoint1`, [`r ot1_evt_tp3_uis$TimePoint1`],
was probably only slightly greater than the correlation for year 2,
_r_<sub>TP1&minus;TP2</sub>\ = `r  int_evt_tp3_pts[["TP1 - TP2"]]`,
[`r int_evt_tp3_uis[["TP1 - TP2"]]`] but considerably greater than
the concurrent correlation at year 3, _r_<sub>TP1&minus;TP3</sub> =
`r  int_evt_tp3_pts[["TP1 - TP3"]]`, [`r int_evt_tp3_uis[["TP1 - TP3"]]`].

Peak looking probabilities from year 1 were strongly correlated with year 2 
receptive vocabulary, _r_\ = `r peak_ppvt_tp2_pts$TimePoint1`, 
[`r peak_ppvt_tp2_uis$TimePoint1`], and this correlation was much greater
than the correlation observed for the year 2 growth curve peaks,
*r*<sub>TP1−TP2</sub> = `r  peak_ppvt_tp2_pts[["TP1 - TP2"]]`,
[`r peak_ppvt_tp2_pts[["TP1 - TP2"]]`].
The correlation of year 1 average looking probabilities, _r_
\ = `r int_ppvt_tp2_pts$TimePoint1`, [`r int_ppvt_tp2_uis$TimePoint1`],
was greater than the year 2 correlation, _r_<sub>TP1&minus;TP2</sub> =
`r  int_ppvt_tp2_pts[["TP1 - TP2"]]`, [`r int_ppvt_tp2_pts[["TP1 - TP2"]]`], and
the correlation for year 1 linear time features, 
_r_\ = `r ot1_ppvt_tp2_pts$TimePoint1`,
[`r ot1_ppvt_tp2_uis$TimePoint1`], was likewise greater than the year 2
correlation, _r_<sub>TP1&minus;TP2</sub> = `r  ot1_ppvt_tp2_pts[["TP1 - TP2"]]`,
[`r ot1_ppvt_tp2_uis[["TP1 - TP2"]]`].

**Summary**.











































## Bayesian model results

Here is the code used to fit the model with Stan. It took about 24 hours to run
the model. The regression terms have the prior Normal(0, 1)

```{r, eval = FALSE}
library(rstanarm)
options(mc.cores = parallel::detectCores())

m <- stan_glmer(
  cbind(Primary, Others) ~
    (ot1 + ot2 + ot3) * Study +
    (ot1 + ot2 + ot3 | ResearchID/Study),
  family = binomial,
  prior = normal(0, 1, autoscale = FALSE),
  prior_intercept = normal(0, 2),
  prior_covariance = decov(2, 1, 1),
  data = d_m)
readr::write_rds(m, "./data/stan_aim1_cubic_model.rds.gz")
```



The output below contains the model quick view, a summary of the fixed effect
terms, and a summary of the priors used.

```{r}
b

summary(b, pars = names(fixef(b)))

prior_summary(b)
```




For the hierarchical part of the model, I used RstanARM's `decov()` prior which
simultaneously sets a prior of the variances and correlations of the model's
random effect terms. For these terms, I used the default prior for the variance
terms and used a weakly informative LKJ(2) prior on the random effect
correlations. Under LKJ(1) supports all correlations in the range ±1, but under
LKJ(2) extreme correlations are less plausible. In the figure below, we see that
the LKJ(2) prior nudges some of the probability mass away from ±1 towards the
center. The motivation for this kind of prior was *regularization*: We give the
model a small amount of information to nudge it away from extreme, degenerate
values.

```{r, include = FALSE, fig.height = 3, fig.width = 5}
lkj1 <- rstanarm::stan_glmer(
  Sepal.Length ~ 1 + (Sepal.Width + Petal.Length | Species), 
  prior = normal(0, 1),
  prior_covariance = decov(1,1,1,1),
  data = iris, 
  prior_PD = TRUE)

lkj2 <- rstanarm::stan_glmer(
  Sepal.Length ~ 1 + (Sepal.Width + Petal.Length | Species), 
  prior = normal(0, 1),
  prior_covariance = decov(2,1,1,1),
  data = iris, 
  prior_PD = TRUE)

big_dumb_model1 <- tristan::draw_var_corr(lkj1, nsamples = 2000)
big_dumb_model2 <- tristan::draw_var_corr(lkj2, nsamples = 2000)
nrow(big_dumb_model2)

correlations <- bind_rows(
  big_dumb_model1 %>% mutate(Model = "LKJ(1)"), 
  big_dumb_model2 %>% mutate(Model = "LKJ(2)"))
```

```{r, echo = FALSE,  out.width = "80%", fig.height = 3, fig.width = 5}
ggplot(correlations %>% filter(!is.na(var2))) + 
  aes(x = sdcor) + 
  geom_histogram(binwidth = .1, boundary = 0) + 
  facet_wrap("Model", labeller = label_both) +
  labs(
    x = NULL, y = "Num. prior samples", 
    caption = "12,000 prior samples drawn per model")

ggplot(correlations %>% filter(!is.na(var2))) + 
  aes(x = sdcor) + 
  geom_density() + 
  facet_wrap("Model", labeller = label_both) +
  labs(
    x = NULL, y = "Num. prior samples", 
    caption = "12,000 prior samples drawn per model")
```


Let's try to understand our model by making some plots.





### Plot the intervals for the random effect parameters

These are the parameters governing the random effect distributions. First, we
plot the standard deviations. Recall that in our hierarchical model we suppose 
that each growth curve is drawn from a population of related curves. The 
model's fixed effects estimate the means of the distribution. These terms
estimate the variability around that mean. We did not have any a priori 
hypotheses about the values of these scales, so do not discuss them any further.

```{r posterior-sds, echo = FALSE, out.width = "80%", fig.height = 2.5, fig.width = 5}
sdcors <- tristan::draw_var_corr(b)
sdcors_wide <- sdcors %>% 
  select(.draw, .parameter, sdcor) %>% 
  tidyr::spread(.parameter, sdcor) %>% 
  select(-.draw)

# Create the mathematical labels for parameters
group_info <- sdcors %>% 
  select(.parameter:var2) %>% 
  distinct()
group_info$group <- group_info$grp %>% 
  stringr::str_replace("Study:ResearchID", "Child-Study") %>% 
  stringr::str_replace("ResearchID", "Child") 

group_info$r <- ifelse(
  is.na(group_info$var2), "", 
  paste0(",", group_info$var2))

group_info$sym <- ifelse(is.na(group_info$var2), "sigma", "rho")

group_info$var1 <- ifelse(
  group_info$var1 == "(Intercept)", "Intercept", 
  group_info$var1)

group_info$math <- sprintf(
  "%s[list(%s%s)]", group_info$sym, group_info$var1, group_info$r)

group_info$class <- ifelse(is.na(group_info$var2), "scale", "correlation")
group_info <- group_info %>% 
  select(group, class, var1, var2, parameter = .parameter, math) %>% 
  mutate(parameter = as.factor(parameter))

intervals <- as.data.frame(sdcors_wide) %>% 
  mcmc_intervals_data() %>% 
  left_join(group_info, by = "parameter") %>% 
  mutate(math = forcats::fct_rev(math))

ggplot(intervals %>% filter(class == "scale")) + 
  aes(y = math) +
  # Draw medians with + then draw white horizontal lines over the horizontal 
  # parts of the + symbols
  geom_point(aes(x = m), size = 3, shape = 3) +
  geom_hline(aes(yintercept = as.numeric(parameter)), color = "white") +
  geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  scale_y_discrete(labels = parse_text) +
  facet_wrap("group", ncol = 1, strip.position = "left") + 
  theme(
    strip.placement = "outside", 
    strip.background = element_rect(fill = NA),
    axis.text.y = element_text(size = rel(1.2))) +
  labs(
    x = NULL, y = NULL, 
    title = "Random effect scales", 
    caption = "Posterior median with 90% and 50% intervals")
```

Then the correlations.

```{r posterior-cors, echo = FALSE, out.width = "80%", fig.height = 3, fig.width = 5}
ggplot(intervals %>% filter(class == "correlation")) + 
  aes(y = math) +
  geom_vline(xintercept = 0, size = 2, color = "white") +
  # Draw medians with + then draw white horizontal lines over the horizontal 
  # parts of the + symbols
  geom_point(aes(x = m), size = 3, shape = 3) +
  geom_hline(aes(yintercept = as.numeric(parameter)), color = "white") +
  geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  scale_y_discrete(labels = parse_text) +
  facet_wrap("group", ncol = 1, strip.position = "left") + 
  theme(
    strip.placement = "outside", 
    strip.background = element_rect(fill = NA),
    axis.text.y = element_text(size = rel(1.2))) +
  labs(
    x = NULL, y = NULL, 
    title = "Random effect correlations", 
    caption = "Posterior median with 90% and 50% intervals")
```









### Posterior predictive checks

Bayesian models are generative; they describe how the data could have been
generated. One way to evaluate the model is to have it simulate new
observations. If the simulated data closely resembles the observed data, then we
have some confidence that our model has learned an approximation of how the data
could have been generated. Figure \@ref(fig:post-pred) depicts the density of
the observed data from each year of the study versus 200 posterior simulations.
Because the simulations closely track the density of the observed data, we can
infer that the model has learned how to generate data from each year of the
study.

(ref:post-pred) Posterior predictive density for the observed data from each
year of the study. The _x_-axis represents the outcome measure---the proportion
of looks to the target image---and the _y_-axis is the density of those values
at year. At age 3, there is a large density of looks around chance performance
(.25) with a rightward skew (above-chance looks are common). At age 4 and age 5,
a bimodal distribution emerges, reflecting how looks start at chance and
reliably increase to above-chance performance. Each light line is a simulation
of the observed data from the model, and the thick lines are the observed data.
Because the thick line is surrounded by light lines, we visually infer that the
the model faithfully approximates the observed data.

```{r post-pred, fig.cap = "(ref:post-pred)", echo = FALSE, out.width = "80%", fig.height=3, fig.width=6}
sims <- rstanarm::posterior_predict(b, draws = 200, seed = "09272017")
n_trials <- data_frame(y_id = seq_along(b$y[, 1]), n_trials = rowSums(b$y))

bayesplot:::ppc_data(b$y[, 1], sims, group = d_m$Study) %>% 
  left_join(n_trials, by = "y_id") %>% 
  mutate(prop = value / n_trials) %>% 
  ggplot() +
    aes(x = prop) +
    stat_density(
      aes_(group = ~ rep_id),
      data = function(x) dplyr::filter(x, !.data$is_y),
      geom = "line", position = "identity", size = .25, alpha = .1, 
      color = "#0074D9") +
    stat_density(
      data = function(x) dplyr::filter(x, .data$is_y),
      geom = "line", position = "identity", size = 1) + 
    labs(x = "Proportion of looks", 
         title = "Observed data and 200 posterior simulations") + 
    coord_cartesian(xlim = c(0, 1), expand = FALSE) + 
    facet_wrap("group")
```


We can ask the model make even more specific posterior predictions. Below we
plot the posterior predictions for random participants. This is the model
simulating new data for these participants.

```{r posterior-lines, out.width = "100%", fig.height=4, fig.width=5}
set.seed(09272017)

ppred <- d_m %>% 
  sample_n_of(8, ResearchID) %>% 
  tristan::augment_posterior_predict(b, newdata = ., nsamples = 100) %>% 
  mutate(trials = Primary + Others)

ggplot(ppred) + 
  aes(x = Time, y = Prop, color = Study, group = Study) + 
  geom_line(aes(y = .posterior_value / trials, 
                group = interaction(.draw, Study)), 
            alpha = .20) + 
  geom_line(size = 1, color = "grey50") + 
  facet_wrap("ResearchID") + 
  theme(
    legend.position = c(.95, 0), 
    legend.justification = c(1, 0),
    legend.margin = margin(0)) +
  guides(color = guide_legend(title = NULL, override.aes = list(alpha = 1))) +
  labs(
    title = "Observed means and 100 simulations of new data",
    x = "Time after target onset",
    y = "Proportion looks to target") 
```

Or we can plot the linear predictions. These are posterior predictions of the
log-odds of looking to target before adding binomial noise.

```{r posterior-mean-lines, out.width = "100%", fig.height=4, fig.width=5}
lpred <- d_m %>% 
  sample_n_of(8, ResearchID) %>% 
  tristan::augment_posterior_linpred(b, newdata = ., nsamples = 100)

ggplot(lpred) + 
  aes(x = Time, y = .posterior_value, color = Study) +
  geom_line(aes(group = interaction(Study, ResearchID, .draw)), 
            alpha = .1) +
  facet_wrap("ResearchID") + 
  geom_point(aes(y = qlogis(Prop)), shape = 1) + 
  theme(
    legend.position = c(.95, 0), 
    legend.justification = c(1, 0),
    legend.margin = margin(0)) +
  guides(color = guide_legend(title = NULL, override.aes = list(alpha = 1))) +
  labs(
    title = "Observed data and 100 posterior predictions",
    x = "Time after target onset",
    y = "Posterior log-odds")
```


### Simulating data from new participants

This mixed effects model assumes that the each child's growth curve is drawn
from a distribution of related growth curves, and it tries to infer the
parameters of that distribution of growth curves (like the scale of individual
differences in the intercept term or the correlation among growth curve
features). A natural next step is to ask the model to simulate new samples from
that distribution of growth curves: That is, predict new data for a
hypothetical, unobserved child drawn from the same distribution as the `N
CHILDREN` observed children.This procedure lets us explore the range of
variability in performance at each age.

Figure \@ref(fig:new-participants) shows the posterior predictions for 1,000
simulated participants, which demonstrates how the model expects new
participants to improve longitudinally but also exhibit stable individual
differences over time. Figure \@ref(fig:new-participants-intervals) shows
uncertainty intervals for these simulations. The model has learned to predict
less accurate and more variable performance at age 3 with improving accuracy and
narrowing variability at age 4 and age 5.


(ref:new-participants) Posterior predictions for new _unobserved_ participants.
Each line represents the predicted performance for a new participant. The three
dark lines highlight predictions from one single simulated participant. The
simulated participant shows both longitudinal improvement in word recognition
and similar relative performance compared to other simulations each year,
indicating that the model would predict new children to improve year over year
and show stable individual differences over time.

```{r new-participants, echo = FALSE, fig.cap = "(ref:new-participants)", fig.width = 6, fig.height = 3, out.width = "80%"}
dummy_data <- d_m %>% 
  distinct(Study, Time, ot1, ot2, ot3) %>% 
  mutate(ResearchID = "NEW",
         Primary = 0, 
         Others = 0)

set.seed(11102017)
lpred <- dummy_data %>% 
  tristan::augment_posterior_linpred(b, newdata = ., nsamples = 1000)

ggplot(lpred) + 
  aes(x = Time, y = plogis(.posterior_value), color = Study) +
  geom_hline(yintercept = .25, size = 2, color = "white") +
  geom_line(aes(group = interaction(Study, .draw)), 
            alpha = .1, show.legend = FALSE) +
  geom_line(aes(group = interaction(Study, .draw)), 
            data = sample_n_of(lpred, 1, .draw), color = "grey20",
            show.legend = FALSE) +
  facet_wrap("Study") + 
  guides(color = guide_legend("none")) +
  labs(
    title = "Posterior predictions for 1,000 new participants",
    x = "Time after target onset",
    y = "Proportion looks to target")
```

(ref:new-participants-intervals) Uncertainty intervals for the simulated
participants. Variability is widest at age 3 and narrowest at age 5,
consistent with the prediction that children become less variable as they 
grow older.

```{r new-participants-intervals, echo = FALSE, fig.cap = "(ref:new-participants-intervals)", fig.width = 6, fig.height = 3, out.width = "80%"}
ggplot(lpred) + 
  aes(x = Time, y = plogis(.posterior_value), color = Study) +
  geom_hline(yintercept = .25, size = 2, color = "white") +
  stat_summary(fun.data = median_hilow, fun.args = list(conf.int = .9),
               size = 1, geom = "linerange") + 
  stat_summary(fun.data = median_hilow, fun.args = list(conf.int = .5), 
               size = 1.5, geom = "linerange") + 
  facet_wrap("Study") + 
  guides(color = guide_legend("none")) +
  labs(
    title = "Posterior predictions for 1,000 new participants",
    x = "Time after target onset",
    y = "Proportion looks to target",
    caption = "90% and 50% intervals")
```


One of the predictions was that children would become less variable as they
grew older and converge on a mature level of performance. We can tackle
this question by inspecting the ranges of predictions for the simulated
participants. The claim that children become less variable would imply that the
range of predictions should be narrower age 5 than for age 4 than age 3. Figure
\@ref(fig:new-ranges) depicts the range of the predictions, both in terms of the
90 percentile range (i.e., the range of the middle 90% of the data) and in terms
of the 50 percentile (interquartile) range. The ranges of performance decrease
from age 3 to age 4 to age 5, consistent with the hypothesized reduction in
variability.

(ref:new-ranges) Ranges of predictions for simulated
participants over the course of a trial. The ranges are most similar during the
first half of the trial when participants are at chance performance, and the
ranges are most different at the end of the trial as children reliably fixate on
the target image. The ranges of performance decreases with each year of the
study as children show less variability.

```{r new-ranges, echo = FALSE, fig.cap = "(ref:new-ranges)", fig.width = 6, fig.height = 3, out.width = "80%"}
by_draw <- lpred %>%
  group_by(Study, Time) %>%
  summarise(
    `95th` = quantile(plogis(.posterior_value), .95),
    `05th` = quantile(plogis(.posterior_value), .05),
    `75th` = quantile(plogis(.posterior_value), .75),
    `25th` = quantile(plogis(.posterior_value), .25),
    `95^th~vs.~05^th` = `95th` - `05th`,
    `75^th~vs.~25^th` = `75th` - `25th`) %>% 
  select(-matches("^..th$")) %>% 
  ungroup() %>% 
  tidyr::gather("range", "extent", -Study, -Time) %>% 
  mutate(range = factor(range, levels = unique(rev(sort(range))))) 
  
ggplot(by_draw) +
  aes(x = Time, y = extent, color = Study) +
  geom_point() + 
  facet_wrap("range", labeller = label_parsed) +
  labs(
    title = "Ranges of predictions for 1000 new participants",
    x = "Time after target onset",
    y = "Difference of percentiles") 
```


