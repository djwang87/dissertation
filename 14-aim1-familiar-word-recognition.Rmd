Analysis of familiar word recognition {#fam-rec}
========================================================================

```{r setup, include = FALSE}
d_m <- readr::read_csv("./data/aim1-model-ready.csv.gz")
knitr::read_chunk("./helpers.R")
if (interactive()) source("./helpers.R")
```

```{r helpers, include = FALSE, cache = FALSE}
```

```{r packages, include = FALSE}
library(rstanarm)
library(bayesplot)
theme_set(theme_grey())
library(stringr)
library(ggstance)
knitr::opts_chunk$set(cache = TRUE)

b <- readr::read_rds("./data/stan_aim1_cubic_model.rds.gz")
b$stan_function <- "stan_glmer"

nsamples <- nrow(as.data.frame(b))
```



Growth curve analysis
------------------------------------------------------------------------

Looks to the familiar image were analyzed using Bayesian mixed
effects logistic regression. I used *logistic* regression because
the outcome measurement is a probability (the log-odds of looking to the
target image versus a distractor). I used *mixed-effects* models
to estimate a separate growth curve for each child (to
measure individual differences in word recognition) but also treat each
child's individual growth curve as a draw from a distribution of related
curves. I used *Bayesian* techniques to study a generative model of the
data. Instead of reporting and describing a single, best-fitting model
of some data, Bayesian methods consider an entire distribution of
plausible models that are consistent with the data and any prior
information we have about the models. By using this approach, one can
explicitly quantify uncertainty about statistical effects and draw
inferences using estimates of uncertainty (instead of using statistical
significance—which is not a straightforward matter for mixed-effects
models).[^2]

[^2]: It is tempting to further justify this approach by comparing
    Bayesian versus classical/frequentist statistics, but my goals in
    using this method are simple: To estimate statistical effects and
    quantify uncertainty about those effects. This pragmatic brand of
    Bayesian statistics is illustrated in texts by @GelmanHill and 
    @RethinkingBook.

The eyetracking growth curves were fit using an orthogonal cubic
polynomial function of time [a now-conventional approach; see
@Mirman2014]. Put differently, I modeled the probability of looking to
the target during an eyetracking task as:

$$
\text{log-odds}(\textit{looking}\,) = 
  \beta_0 + 
  \beta_1\text{Time}^1 + 
  \beta_2\text{Time}^2 + 
  \beta_3\text{Time}^3
$$

That the time terms are *orthogonal* means that $\text{Time}^1$,
$\text{Time}^2$ and $\text{Time}^3$ are transformed so that they
are uncorrelated. Under this formulation, the parameters $\beta_0$ and
$\beta_1$ have a direct interpretation in terms of lexical processing
performance. The intercept, $\beta_0$, measures the area under the
growth curve—or the probability of fixating on the target word averaged
over the whole window. We can think of $\beta_0$ as a measure of *word
recognition reliability*. The linear time parameter, $\beta_1$,
estimates the steepness of the growth curve—or how the probability of
fixating changes from frame to frame. We can think of $\beta_1$ as a
measure of *processing efficiency*, because growth curves with stronger
linear features exhibit steeper frame-by-frame increases in looking
probability.[^3]


[^3]: The polynomial other terms are less important—or rather, they have
    do not map as neatly onto behavioral descriptions as the accuracy
    and efficiency parameters. The primary purpose of quadratic and
    cubic terms is to ensure that the estimated growth curve adequately
    fits the data. In this kind of data, there is a steady baseline at
    chance probability before the child hears the word, followed a
    window of increasing probability of fixating on the target as the
    child recognizes the word, followed by a period of plateauing and
    then diminishing looks to target. The cubic polynomial allows the
    growth curve to be fit with two inflection points: the point when
    the looks to target start to increase from baseline and the point
    when the looks to target stops increasing.

To study how word recognition changes over time, I modeled how the
growth curves change over developmental time. This amounted to studying
how the growth curve parameters changes year over year. I included
dummy-coded indicators for Age 3, Age 4, and Age 5 and allowed these
indicators interact with the growth curve parameters. These
year-by-growth-curve terms captured how the shape of the growth curves
changed each year. The model also included random effects to represent
child-by-year effects.


### Growth curve features as measures of word recognition performance

As mentioned above, two of the model's growth curve features have
straightforward interpretations in terms of lexical processing
performance: The model's intercept parameter corresponds to the average
proportion or probability of looking to the named image over the trial
window, and the linear time parameter corresponds to slope of the growth
curve or lexical processing efficiency. I also was interested in *peak*
proportion of looks to the target. I derived this value by computing the
growth curves from the model and taking the median of the five highest
points on the curve. Figure \@ref(fig:curve-features) shows three
simulated growth curves and how each of these growth curve features
relate to word recognition performance.

(ref:curve-features) Illustration of the three growth curve features and
how they describe lexical processing performance. The three curves used
are simulations of new participants at Age 4.

```{r curve-features, fig.cap = "(ref:curve-features)", echo = FALSE, out.width = "80%", fig.height = 5.5, fig.width = 5.5}
dummy_data <- d_m %>% 
  filter(Study == "TimePoint2") %>% 
  distinct(Study, Time, ot1, ot2, ot3) %>% 
  mutate(
    ResearchID = "NEW", 
    Primary = 0, 
    Others = 0)

set.seed(02062018)

lpred <- dummy_data %>% 
  tristan::augment_posterior_linpred(model = b, newdata = ., nsamples = 3) 

lpred_lm <- lpred %>% 
  tidyr::nest(-.draw) %>% 
  mutate(
    model = purrr::map(data, ~ lm(.posterior_value ~ ot1 + ot2 + ot3, .x)),
    coef = lapply(model, coef),
    auc = purrr::map_dbl(coef, purrr::pluck, 1),
    slope = purrr::map_dbl(coef, purrr::pluck, 2)) %>% 
  select(-model, -coef) %>% 
  tidyr::unnest(data) %>% 
  # Sort using the AUC values
  mutate(.draw = forcats::fct_reorder(as.factor(.draw), auc))

p1 <- ggplot(lpred_lm) + 
  aes(x = ot1, y = .posterior_value, group = .draw) + 
  geom_line(linetype = "dashed", color = constants$col_off_black) + 
  geom_hline(
    aes(yintercept = auc), 
    color = constants$col_blue_highlight, 
    size = .75) + 
  labs(
    x = NULL,
    y = constants$y_logodds_target,
    title = "Intercept term represents average probability") + 
  facet_wrap(".draw") + 
  theme_grey(base_size = 9) + 
  theme(
    strip.text = element_blank(), 
    axis.text.x = element_blank(),
    axis.title.y = element_text(colour = "white"),
    axis.ticks.x = element_blank())

p2 <- ggplot(lpred_lm) + 
  aes(x = ot1, y = .posterior_value, group = .draw) + 
  geom_line(linetype = "dashed", color = constants$col_off_black) + 
  geom_abline(
    aes(intercept = auc, slope = slope),
    color = constants$col_blue_highlight, 
    size = .75) + 
  facet_wrap(".draw") + 
  labs(
    x = NULL,
    y = constants$y_logodds_target,
    title = "Linear time term represents processing efficiency") +
  theme_grey(base_size = 9) + 
  theme(
    strip.text = element_blank(), 
    axis.text.x = element_blank(),
    axis.title.x = element_blank(),
    axis.ticks.x = element_blank())

p3 <- ggplot(lpred_lm) + 
  aes(x = ot1, y = .posterior_value, group = .draw) + 
  geom_line(linetype = "dashed", color = constants$col_off_black) + 
  geom_point(
    data = lpred_lm %>% group_by(.draw) %>% top_n(5, .posterior_value), 
    size = 2, color = constants$col_off_black) + 
  geom_hline(
    aes(yintercept = .posterior_value), 
    data = lpred_lm %>% 
      group_by(.draw) %>% 
      top_n(5, .posterior_value) %>% 
      summarise(.posterior_value = median(.posterior_value)), 
    size = .75, 
    color = constants$col_blue_highlight) + 
  facet_wrap(".draw") + 
  labs(
    x = constants$x_time,
    y = constants$y_logodds_target,
    title = "Peak probability is derived from median of top five points") + 
  theme_grey(base_size = 9) + 
  theme(
    strip.text = element_blank(), 
    axis.text.x = element_blank(),
    axis.title.y = element_text(colour = "white"),
    axis.ticks.x = element_blank())

cowplot::plot_grid(
  p1, p2, p3, ncol = 1, 
  # This fudge factor corrects how the bottom plot has an x-axis title and the
  # others do not. Without this fix, the spacing between the grid-lines would
  # differ between the top two and bottom plots. As far as I can tell, this
  # doesn't distort the text sizes.
  rel_heights = c(.32, .32, .36))
```



Year over year changes in word recognition performance
------------------------------------------------------------------------

The mixed-effects model estimated a population-average growth curve
("fixed" effects) and how individual children deviated from average
("random" effects). Figure \@ref(fig:average-growth-curves) shows 200
posterior samples of the average growth curves for each study. On
average, the growth curves become steeper and achieve higher looking
probabilities with each year of the study.

(ref:average-growth-curves) The model estimated an average word
recognition growth for each study, and the colored lines represent 200
posterior samples of these growth curves. The thick dark lines represent
the observed average growth curve in each study.

```{r average-growth-curves, fig.cap = "(ref:average-growth-curves)", echo = FALSE, out.width = "50%", fig.height = 3, fig.width = 4}
dummy_data <- d_m %>% 
  distinct(Study, Time, ot1, ot2, ot3) %>% 
  mutate(
    ResearchID = "NEW", 
    Primary = 0, 
    Others = 0)

set.seed(11102017)
lpred <- dummy_data %>% 
  tristan::augment_posterior_linpred(model = b, newdata = ., re.form = NA)

peaks <- lpred %>% 
  group_by(Study, .draw) %>% 
  top_n(5, .posterior_value) %>% 
  mutate(med = median(.posterior_value)) %>% 
  select(.draw, Study, peek_accuracy = med) %>% 
  distinct() %>% 
  ungroup() %>% 
  mutate(peek_accuracy = peek_accuracy) %>% 
  tidyr::spread(Study, peek_accuracy) %>% 
  select(
    `Peak~~(Age~~3)` = TimePoint1,
    `Peak~~(Age~~4)` = TimePoint2,
    `Peak~~(Age~~5)` = TimePoint3)

lpred %>% 
  tjmisc::sample_n_of(200, .draw) %>% 
  mutate(Study = convert_study_to_age(Study)) %>% 
  ggplot() + 
    aes(x = Time, y = plogis(.posterior_value), color = Study) +
    geom_hline(yintercept = .25, color = "white", size = 2) +
    geom_line(
      aes(group = interaction(Study, ResearchID, .draw)), 
      alpha = .1) +
    stat_summary(
      aes(y = Prop, group = Study), 
      data = d_m %>% 
        mutate(Study = convert_study_to_age(Study)), 
      fun.y = "mean", geom = "line", 
      color = constants$col_off_black, size = 1) + 
    theme(
      legend.position = c(.01, .99), 
      legend.justification = c(0, 1),
      legend.margin = margin(3, 6, 6, 6)) +
    guides(
      color = guide_legend(title = NULL, override.aes = list(alpha = 1))) +
    labs(
      title = "Observed means and 200 posterior fits",
      x = constants$x_time,
      y = constants$y_prop_target)
```

Figure \@ref(fig:effects2) depicts uncertainty intervals with
the model's average effects of each timepoint on the growth curve
features. The intercept and linear time effects increased each year,
confirming that children become more reliable and faster at recognizing
words as they grow older. The peak accuracy also increased each year.
For each effect, the change from age 3 to age 4 is approximately the
same as the change from age 4 to age 5, as visible in
Figure \@ref(fig:pairwise-effects).

(ref:effects2) Uncertainty intervals for the effects of study years on
growth curve features. The intercept and peak features were converted from
log-odds to proportions to ease interpretation.

```{r effects2, fig.cap = "(ref:effects2)", echo = FALSE, out.width = "80%", fig.height=4, fig.width=5}
# Column names will have mathematical formatting too
draws <- as.data.frame(b) %>% 
  as_tibble() %>% 
  transmute(
    `Intercept~~(Age~~3)` = `(Intercept)`,
    `Intercept~~(Age~~4)` = `(Intercept)` + StudyTimePoint2,
    `Intercept~~(Age~~5)` = `(Intercept)` + StudyTimePoint3,
    `Time~~(Age~~3)` = ot1,
    `Time~~(Age~~4)` = ot1 + `ot1:StudyTimePoint2`,
    `Time~~(Age~~5)` = ot1 + `ot1:StudyTimePoint3`,
    `Time^2~~(Age~~3)` = ot2,
    `Time^2~~(Age~~4)` = ot2 + `ot2:StudyTimePoint2`,
    `Time^2~~(Age~~5)` = ot2 + `ot2:StudyTimePoint3`,
    `Time^3~~(Age~~3)` = ot3,
    `Time^3~~(Age~~4)` = ot3 + `ot3:StudyTimePoint2`,
    `Time^3~~(Age~~5)` = ot3 + `ot3:StudyTimePoint3`) %>% 
  bind_cols(peaks)

logodds <- draws %>% 
  select(starts_with("Time")) %>% 
  mcmc_intervals_data(prob = 0.5, prob_outer = 0.9) %>% 
  mutate(Scale = "Log-odds") %>% 
  mutate(parameter = factor(parameter, levels = rev(names(draws))))

props <- draws %>% 
  select(starts_with("Intercept"), starts_with("Peak")) %>% 
  mutate_all(plogis) %>% 
  mcmc_intervals_data(prob = 0.5, prob_outer = 0.9) %>% 
  mutate(Scale = "Proportion") %>% 
  mutate(parameter = factor(parameter, levels = rev(names(draws))))

intervals2 <- bind_rows(props, logodds) %>% 
  group_by(Scale) %>% 
  mutate(min_x = min(ll), max_x = max(hh)) %>% 
  ungroup()

ggplot(intervals2) + 
  aes(y = parameter) +
  geom_vline(xintercept = 0, size = 2, color = "white") +
  geom_point(aes(x = m), size = 3, shape = 3) + 
  # Draw mock gridlines in case the point's + shape is wider than the interval.
  # All that's visible is then a | with a gap in the middle.
  geom_segment(
    aes(y = parameter, yend = parameter, x = min_x, xend = max_x), 
    size = .25, color = "white") +
  ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  scale_y_discrete(labels = parse_text) + 
  labs(
    x = NULL, 
    y = NULL, 
    title = "Average effects by study",
    caption = constants$cap_median_90_50) + 
  facet_wrap("Scale", scales = "free", ncol = 1, labeller = label_both)
```

(ref:pairwise-effects) Uncertainty intervals for the differences between
study timepoints. Again, the intercept and peak features were converted
to proportions.

```{r pairwise-effects, fig.cap = "(ref:pairwise-effects)", fig.show = 'hold', echo = FALSE, out.width = "50%", fig.height = c(4, 3), fig.width = c(4, 4)}
clean_names <- . %>% 
  stringr::str_replace_all("[()]", "") %>% 
  stringr::str_replace("Age~~", "Age ") %>% 
  stringr::str_replace("~~", "_") %>% 
  stringr::str_replace("TP1", "Age 3") %>% 
  stringr::str_replace("TP2", "Age 4") %>% 
  stringr::str_replace("TP3", "Age 5")

pairwise <- draws %>% 
  mutate_at(vars(starts_with("Intercept"), starts_with("Peak")), plogis) %>% 
  tibble::rowid_to_column(".draw") %>% 
  set_names(clean_names) %>% 
  tidyr::gather(parameter, value, -.draw) %>% 
  tidyr::separate(parameter, c("parameter", "year"), sep = "_") %>% 
  compare_pairs(year, value) %>% 
  tidyr::spread(pair, value) %>% 
  split(.$parameter) %>% 
  lapply(
    . %>% select(-.draw, -parameter) %>% 
      mcmc_intervals_data(prob = 0.5, prob_outer = 0.9) %>% 
      rename(pair = parameter)) %>% 
  bind_rows(.id = "parameter") %>% 
  mutate(
    # Using phantom(l) to create a thin space between Age and the number
    # Using a factor to set the sorting order
    pair = pair %>% 
      stringr::str_replace_all("Age (\\d)", "paste(Age, phantom(l), \\1)") %>% 
      factor(c(
        "paste(Age, phantom(l), 4)-paste(Age, phantom(l), 3)", 
        "paste(Age, phantom(l), 5)-paste(Age, phantom(l), 4)", 
        "paste(Age, phantom(l), 5)-paste(Age, phantom(l), 3)")))

p1 <- ggplot(pairwise %>% filter(!(parameter %in% c("Peak", "Intercept")))) + 
  aes(y = forcats::fct_rev(pair)) +
  geom_vline(xintercept = 0, size = 2, color = "white") +
  ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  geom_point(aes(x = m), size = 3, shape = 3) + 
  facet_wrap(
    facets = "parameter", 
    ncol = 1, 
    strip.position = "left", 
    labeller = label_parsed) + 
  scale_y_discrete(labels = parse_text) + 
  labs(
    x = NULL, 
    y = NULL, 
    title = "Differences in average effects",
    caption = " ") + 
  theme(
    strip.placement = "outside", 
    strip.background = element_rect(fill = NA),
    axis.text.y = element_text(size = rel(1.2)),
    strip.text.y = element_text(
      size = rel(1.4), 
      margin = margin(0, 10, 0, 5, "pt")))

p2 <- ggplot(pairwise %>% filter(parameter %in% c("Peak", "Intercept"))) + 
  aes(y = forcats::fct_rev(pair)) +
  geom_vline(xintercept = 0, size = 2, color = "white") +
  ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  geom_point(aes(x = m), size = 3, shape = 3) + 
  facet_wrap(
    facets = "parameter", 
    ncol = 1, 
    strip.position = "left", 
    labeller = label_parsed) + 
  scale_y_discrete(labels = parse_text) + 
  labs(
    x = NULL, 
    y = NULL, 
    title = " ",
    caption = constants$cap_median_90_50) + 
  theme(
    strip.placement = "outside", 
    strip.background = element_rect(fill = NA),
    axis.text.y = element_text(size = rel(1.2)),
    strip.text.y = element_text(
      size = rel(1.4), 
      margin = margin(0, 10, 0, 5, "pt")))

# p3 <- ggplot() + theme_void()
# p23 <- cowplot::plot_grid(p2, p3, ncol = 1, rel_heights = c(.66, .33))
# cowplot::plot_grid(p1, p23, ncol = 2)
p1
p2
```

```{r, include = FALSE}
pluck <- purrr::pluck

get_pts <- function(x, n = 2) {
  x %>% 
    lapply(. %>% pluck("m") %>% printy::fmt_fix_digits(n))
} 

get_uis <- function(x, n = 2) {
  fmt_ui <- . %>% 
    glue::glue_data(
      "{printy::fmt_fix_digits(ll,n)}--{printy::fmt_fix_digits(hh,n)}") 
  
  x %>% 
    lapply(fmt_ui) 
} 

get_cor_pts <- function(x, n = 2) {
  fmt_cor <- . %>%
    printy::fmt_fix_digits(n) %>% 
    printy::fmt_leading_zero() %>% 
    printy::fmt_minus_sign()
  
  x %>% 
    lapply(. %>% pluck("m") %>% fmt_cor())
}
  
get_cor_uis <- function(x, n = 2) {
  fmt_cor <- . %>%
    printy::fmt_fix_digits(n) %>% 
    printy::fmt_leading_zero() %>% 
    printy::fmt_minus_sign()
  
  x %>% 
    lapply(. %>% glue::glue_data("{fmt_cor(ll)}--{fmt_cor(hh)}")) 
}

props <- props %>% 
  mutate(
    parameter = parameter %>% 
      stringr::str_replace("~~.", "_") %>% 
      stringr::str_replace("\\)", "")) %>% 
  tidyr::separate(parameter, sep = "_", into = c("parameter", "study"))

prop_diffs <- pairwise %>% 
  filter(parameter %in% c("Intercept", "Peak"))

prop_list1 <- props %>% 
  filter(parameter == "Intercept") %>% 
  split(.$study) %>% 
  set_names(str_extract, "Age~~.") %>% 
  set_names(str_replace, "~~", "")

prop_list2 <- props %>% 
  filter(parameter == "Peak") %>% 
  split(.$study) %>% 
  set_names(str_extract, "Age~~.") %>% 
  set_names(str_replace, "~~", "")

int_pts <- get_pts(prop_list1)
int_uis <- get_uis(prop_list1)
peak_pts <- get_pts(prop_list2)
peak_uis <- get_uis(prop_list2)

diff_list1 <- prop_diffs %>% 
  filter(parameter == "Intercept") %>% 
  split(.$pair) %>% 
  set_names(str_replace_all, ".+(\\d).+(\\d).+", "Age\\1_\\2")

diff_list2 <- prop_diffs %>% 
  filter(parameter == "Peak") %>% 
  split(.$pair) %>% 
  set_names(str_replace_all, ".+(\\d).+(\\d).+", "Age\\1_\\2")

int_dpts <- get_pts(diff_list1)
int_duis <- get_uis(diff_list1)
peak_dpts <- get_pts(diff_list2)
peak_duis <- get_uis(diff_list2)
```

The average looking probability (intercept feature) was `r int_pts$Age3`
[90% UI: `r int_uis$Age3`] at age 3, `r int_pts$Age4`
[`r int_uis$Age4`] at age 4, and `r int_pts$Age5` [`r int_uis$Age5`] at
age 5. The averages increased by `r int_dpts$Age4_3`
[`r int_duis$Age4_3`] from age 3 to age 4 and by `r int_dpts$Age5_4`
[`r int_duis$Age5_4`] from age 4 to age 5. The peak looking
probability was `r peak_pts$Age3` [`r peak_uis$Age3`] at age 3,
`r peak_pts$Age4` [`r peak_uis$Age4`] at age 4, and `r peak_pts$Age5`
[`r peak_uis$Age5`] at age 5. The peak values increased by
`r peak_dpts$Age4_3` [`r peak_duis$Age4_3`] from age 3 to age 4 and
by `r peak_dpts$Age5_4` [`r peak_duis$Age5_4`] from age 4 to age 5.
These results numerically confirm the hypothesis that children would
improve in their word recognition reliability, both in terms of average
looking and in terms of peak accuracy, each year.

**Summary**. The average growth curve features increased year over year,
so that children looked to the target more quickly and more reliably.



Exploring plausible ranges of performance over time
------------------------------------------------------------------------

```{r, include = FALSE}
dummy_data <- d_m %>% 
  distinct(Study, Time, ot1, ot2, ot3) %>% 
  mutate(ResearchID = "NEW",
         Primary = 0, 
         Others = 0)

set.seed(11102017)
lpred <- dummy_data %>% 
  tristan::augment_posterior_linpred(b, newdata = ., nsamples = 1000)

new_peaks <- lpred %>% 
  mutate(Study = convert_study_to_age(Study)) %>% 
  distinct(.draw, .posterior_value, Study) %>% 
  group_by(Study, .draw) %>% 
  top_n(5, .posterior_value) %>% 
  summarise(peak = median(.posterior_value)) %>% 
  ungroup() %>% 
  mutate(peak = plogis(peak)) %>% 
  tidyr::spread(Study, peak) %>% 
  select(-.draw) %>% 
  mcmc_intervals_data(prob = 0.5, prob_outer = 0.9)

new_peaks_pts <- new_peaks %>% 
  split(.$parameter) %>% 
  get_pts() %>% 
  set_names(str_replace, " ", "")

new_peaks_uis <- new_peaks %>% 
  split(.$parameter) %>% 
  get_uis() %>% 
  set_names(str_replace, " ", "")

new_peaks_diff <- new_peaks %>% 
  split(.$parameter) %>% 
  lapply(function(l) round(l$hh - l$ll, 2)) %>% 
  set_names(str_replace, " ", "")
```

Bayesian models are generative; they describe how the data could have
been generated. This model assumed that each child's growth curve was
drawn from a population of related growth curves, and it tried to infer
the parameters over that distribution. These two aspects---a generative
model and learning about the population of growth curves---allow the
model to simulate new samples from that distribution of growth curves.
That is, we can predict a set of growth curves for a hypothetical,
unobserved child drawn from the same distribution as the
`r n_distinct(d_m$ResearchID)` observed children. This procedure allows
one to explore the plausible degrees of variability in performance at
each age.

Figure \@ref(fig:new-participants) shows the posterior predictions
for 1,000 simulated participants, which demonstrates how the model
expects new participants to improve longitudinally but also exhibit
stable individual differences over time. Figure
\@ref(fig:new-participants-intervals) shows uncertainty intervals for
these simulations. The model learned to predict less accurate and more
variable performance at age 3 with improving accuracy and narrowing
variability at age 4 and age 5.

(ref:new-participants) Posterior predictions for hypothetical
*unobserved* participants. Each line represents the predicted
performance for a new participant. The three dark lines highlight
predictions from one single simulated participant. The simulated
participant shows both longitudinal improvement in word recognition and
similar relative performance compared to other simulations each year,
indicating that the model would predict new children to improve year
over year and show stable individual differences over time.


```{r new-participants, echo = FALSE, fig.cap = "(ref:new-participants)", fig.width = 6, fig.height = 3, out.width = "80%"}
ggplot(lpred %>% mutate(Study = convert_study_to_age(Study))) + 
  aes(x = Time, y = plogis(.posterior_value), color = Study) +
  geom_hline(yintercept = .25, size = 2, color = "white") +
  geom_line(
    aes(group = interaction(Study, .draw)), 
    alpha = .1, show.legend = FALSE) +
  geom_line(
    aes(group = interaction(Study, .draw)), 
    data = lpred %>% 
      sample_n_of(1, .draw) %>% 
      mutate(Study = convert_study_to_age(Study)), 
    color = "grey20",
    show.legend = FALSE) +
  facet_wrap("Study") + 
  guides(color = guide_legend("none")) +
  labs(
    title = "Posterior predictions for 1,000 new participants",
    x = constants$x_time,
    y = constants$y_prop_target)
```

(ref:new-participants-intervals) Uncertainty intervals for the simulated
participants. Variability is widest at age 3 and narrowest at age 5,
consistent with the prediction that children become less variable as
they grow older.

```{r new-participants-intervals, echo = FALSE, fig.cap = "(ref:new-participants-intervals)", fig.width = 6, fig.height = 3, out.width = "80%"}
ggplot(lpred %>% mutate(Study = convert_study_to_age(Study))) + 
  aes(x = Time, y = plogis(.posterior_value), color = Study) +
  geom_hline(yintercept = .25, size = 2, color = "white") +
  stat_summary(
    fun.data = median_hilow, 
    fun.args = list(conf.int = .9),
    size = 1, 
    geom = "linerange") + 
  stat_summary(
    fun.data = median_hilow, 
    fun.args = list(conf.int = .5), 
    size = 1.5, 
    geom = "linerange") + 
  facet_wrap("Study") + 
  guides(color = guide_legend("none")) +
  labs(
    title = "Posterior predictions for 1,000 new participants",
    x = constants$x_time,
    y = constants$y_prop_target,
    caption = constants$cap_median_90_50)
```

I hypothesized that children would become less variable as they grew
older and converged on a mature level of performance. I address this
question by inspecting the ranges of predictions for the simulated
participants. The claim that children become less variable would imply
that the range of predictions should be narrower age 5 than for age 4
than age 3. Figure \@ref(fig:new-ranges) depicts the range of the
predictions, both in terms of the 90 percentile range (i.e., the range
of the middle 90% of the data) and in terms of the 50 percentile
(interquartile) range. The ranges of performance decrease from age 3 to
age 4 to age 5, consistent with the hypothesized reduction in
variability.

(ref:new-ranges) Ranges of predictions for simulated participants over
the course of a trial. The ranges are most similar during the first half
of the trial when participants are at chance performance, and the ranges
are most different at the end of the trial as children reliably fixate
on the target image. The ranges of performance decreases with each year
of the study as children show less variability.

```{r new-ranges, echo = FALSE, fig.cap = "(ref:new-ranges)", fig.width = 6, fig.height = 3, out.width = "80%"}
by_draw <- lpred %>%
  group_by(Study, Time) %>%
  summarise(
    `95th` = quantile(plogis(.posterior_value), .95),
    `05th` = quantile(plogis(.posterior_value), .05),
    `75th` = quantile(plogis(.posterior_value), .75),
    `25th` = quantile(plogis(.posterior_value), .25),
    `95^th~vs.~05^th` = `95th` - `05th`,
    `75^th~vs.~25^th` = `75th` - `25th`) %>% 
  select(-matches("^..th$")) %>% 
  ungroup() %>% 
  tidyr::gather("range", "extent", -Study, -Time) %>% 
  mutate(range = factor(range, levels = unique(rev(sort(range)))))
  
ggplot(by_draw %>% mutate(Study = convert_study_to_age(Study))) +
  aes(x = Time, y = extent, color = Study) +
  geom_point() + 
  facet_wrap("range", labeller = label_parsed) +
  labs(
    title = "Ranges of predictions for 1000 new participants",
    x = constants$x_time,
    y = "Difference of percentiles") + 
  theme(
    legend.position = c(.01, .99), 
    legend.justification = c(0, 1),
    legend.margin = margin(3, 6, 6, 6)) +
  guides(color = guide_legend(title = NULL))
```

The developmental pattern of increasing reliability and decreasing
variability was also observed for the growth curve peaks. For the
synthetic participants, the model predicted that individual peak
probabilities will increase each year, peak<sub>3</sub> =
`r new_peaks_pts$Age3` [90% UI: `r new_peaks_uis$Age3`],
peak<sub>4</sub> = `r new_peaks_pts$Age4` [`r new_peaks_uis$Age4`],
peak<sub>5</sub> = `r new_peaks_pts$Age5` [`r new_peaks_uis$Age5`].
Moreover, the range of plausible values for the individual peaks
narrowed each for the simulated data. For instance, the difference
between the 95^th^ and 5^th^ percentiles was `r new_peaks_diff$Age3` for
age 3, `r new_peaks_diff$Age4` for age 4, and `r new_peaks_diff$Age5`
for age 5.

**Summary**. I used the model's random effects estimates to simulate
growth curves from 1,000 hypothetical, unobserved participants. The
simulated dataset showed increasing looking probability and decreasing
variability with each year of the study. These simulations confirmed the
hypothesis that variability would be diminish as children converge on a
mature level of performance on this task.



Are individual differences stable over time?
------------------------------------------------------------------------

```{r compute-kendalls, include = FALSE}
fits <- readr::read_csv("./data/aim1-gca-features.csv.gz")

# Compute Kendall's coefficient of correspondence
tidy_kendall <- . %>%
  unclass() %>%
  as.data.frame(stringsAsFactors = FALSE)

# Add random ratings
new_coef <- fits %>%
  filter(coef == "intercept") %>%
  mutate(
    .posterior_value = runif(length(.posterior_value)),
    coef = "random values")

# Keep only data from participants who visited all three years
reduced_data <- fits %>%
  bind_rows(new_coef) %>%
  tidyr::spread(Study, .posterior_value) %>%
  tidyr::drop_na(TimePoint1:TimePoint3) 

n_rated <- n_distinct(reduced_data$ResearchID)

ws <- reduced_data %>%
  select(-ResearchID) %>%
  tidyr::nest(TimePoint1:TimePoint3) %>%
  mutate(ws = purrr::map(data, irr::kendall) %>% purrr::map(tidy_kendall)) %>%
  select(-data) %>%
  tidyr::unnest(ws)

posterior_w <- ws %>%
  select(.draw, coef, value) %>%
  tidyr::spread(coef, value) %>%
  rename(`random numbers` = `random values`) %>%
  select(-.draw)
```

I predicted that children would show stable individual differences such
that children who are faster and more reliable at recognizing words at
age 3 remain relatively faster and more reliable at age 5. To evaluate
this hypothesis, I used Kendall's *W* (the coefficient of correspondence
or concordance). This nonparametric statistic measures the degree of
agreement among *J* judges who are rating *I* items. For these purposes,
the items are the `r n_rated` children who provided reliable eyetracking
for all three years of the study. (That is, I excluded children who only
had reliable eyetracking data for one or two years.) The judges are the
sets of growth curve parameters from each year of study. For example,
the intercept term provides three sets of ratings: The participants'
intercept terms from year 1 are one set of ratings and the terms from
years 2 and 3 provide two more sets of ratings. These three ratings are
the "judges" used to compute the intercept's *W*. Thus, I computed five
groups of *W* coefficients, one for each set of growth curve features:
Intercept, Time^1^, Time^2^, Time^3^, and Peak looking probability.


```{r table-example-of-feature-ranks, echo = FALSE, eval = FALSE}
# (_Maybe: Table X illustrates some sample ratings of these participants._)

zero_pad_int <- function(xs) {
  formatter <- paste0("%0", max(nchar(xs)), "d")
  sprintf(formatter, xs)
}

val_rank <- function(xs) {
  a <- printy::fmt_minus_sign(printy::fmt_fix_digits(xs, 2))
  b <- zero_pad_int(rank(-xs))
  glue::glue("{a} ({b})")
}

reduced_data %>% 
  filter(coef == "intercept") %>% 
  group_by(ResearchID, coef) %>% 
  summarise_at(vars(starts_with("TimePoint")), median) %>% 
  ungroup() %>% 
  arrange(desc(TimePoint3)) %>% 
  mutate(
    TimePoint1 = val_rank(TimePoint1), 
    TimePoint2 = val_rank(TimePoint2), 
    TimePoint3 = val_rank(TimePoint3)) %>% 
  rename(
    `Participant ID` = ResearchID,
    `Growth curve feature` = coef,
    `Year 1` = `TimePoint1`,
    `Year 2` = `TimePoint2`,
    `Year 3` = `TimePoint3`) %>% 
  head(10) %>% 
  knitr::kable(align = c("l", "l", "r", "r", "r"))
```

Because I used a Bayesian model, there is a distribution of ratings and
thus a distribution of concordance statistics. Each sample of the
posterior distribution fits a growth curve for each child in each study,
so each posterior sample provides a set of ratings for concordance
coefficients. The distribution of *W*'s lets us quantify our uncertainty
because we can compute *W*'s for each of the `r nsamples` samples from
the posterior distribution.

One final matter is how to assess whether a concordance statistic is
meaningful. To tackle this question, I also included a "null rater", a
fake parameter that assigned each child in each year a random number. I
use the distribution of *W*'s generated by randomly rating children as a
benchmark for assessing whether the other concordance statistics differ
meaningfully from chance.

(ref:kendall-stats) Uncertainty intervals for the Kendall's coefficient
of concordance. Random ratings provide a baseline of null *W*
statistics. The intercept and linear time features are decisively
non-null, indicating a significant degree of correspondence in
children's relative word recognition reliability and efficiency over
three years of study.

```{r kendall-stats, fig.cap = "(ref:kendall-stats)", echo = FALSE, out.width = "80%", fig.height = 3, fig.width = 6}
subtitle <- glue::glue(
  "Kendall's W. Raters: 3 timepoints. Items: {n_rated} children.")

w_intervals <- posterior_w %>% 
  rename(
    Intercept = `intercept`, `Time` = ot1, `Time^2` = ot2, `Time^3` = ot3,
    Peak = peak_logit, `Random~ratings` = `random numbers`) %>% 
  mcmc_intervals_data(prob_outer = .9, prob = .5) %>% 
  mutate(
    parameter = factor(parameter, c(
      "Random~ratings", "Time^3", "Time^2", "Time", "Intercept", "Peak"))) 

ggplot(w_intervals) + 
  aes(y = parameter) +
  geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
  geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
  geom_point(aes(x = m), size = 3, shape = 3) + 
  scale_y_discrete(labels = parse_text) + 
  labs(
    x = NULL, 
    y = NULL, 
    title = "Concordance coefficients for growth curve features",
    caption = constants$cap_median_90_50,
    subtitle = subtitle)

# TODO format as correlations
w_pts <- w_intervals %>% 
  split(.$parameter) %>% 
  get_pts()

w_uis <- w_intervals %>% 
  split(.$parameter) %>% 
  get_uis()

rm(ws, reduced_data, posterior_w)
```

We used the `kendall()` function in the irr R package
[vers. `r packageVersion("irr")`; @irr] to compute concordance
statistics. Figure \@ref(fig:kendall-stats) depicts uncertainty intervals
for the Kendall *W*'s for these growth curve features. The 90%
uncertainty interval of *W* statistics from random ratings
[`r w_uis[["Random~ratings"]]`] subsumes the intervals for the Time^2^
effect [`r w_uis[["Time^2"]]`] and the Time^3^ effect
[`r w_uis[["Time^3"]]`], indicating that these values do not
differentiate children in a longitudinally stable way. That is, the
Time^2^ and Time^3^ features differentiate children across studies as
well as random numbers. Earlier, I stated that only the intercept,
linear time, and peak features have psychologically meaningful
interpretations and that the higher-order features of these models serve
to capture the shape of the growth curve data. These concordance
statistics support that assertion.

Concordance is strongest for the peak feature, *W* = `r w_pts[["Peak"]]`
[`r w_uis[["Peak"]]`] and the intercept term, *W* =
`r w_pts[["Intercept"]]` [`r w_uis[["Intercept"]]`], followed by the
linear time term, *W* = `r w_pts[["Time"]]` [`r w_uis[["Time"]]`].
Because these values are far removed from the statistics for random
ratings, I conclude that there is a credible degree of correspondence
across studies when ranking children using their peak looking
probability, average look probability (the intercept) or their growth
curve slope (linear time).

**Summary**. Growth curve features reflected individual differences in
word recognition performance. By using Kendall's *W* to
measure the degree of concordance among growth curve features over
time, I tested whether individual differences in lexical
processing persisted over development. I found that the peak looking
probability, average looking probability and linear time features were
stable over time.



Predicting future vocabulary size
------------------------------------------------------------------------

```{r vocab correlations, include = FALSE}
if (!exists("fits")) {
  fits <- readr::read_csv("./data/aim1-gca-features.csv.gz")
}

scores <- readr::read_csv("./data-raw/test_scores.csv") %>% 
  select(
    Study, ResearchID, Age, EVT_GSV, EVT_Standard, PPVT_GSV, PPVT_Standard)

wide_scores <- scores %>% 
  tidyr::gather("Test", "Value", -ResearchID, -Study) %>% 
  tidyr::unite("Col", Study, Test) %>% 
  tidyr::spread(Col, Value)

cor_complete <- function(...) cor(..., use = "pairwise.complete")

vocab_cors <- fits %>% 
  left_join(wide_scores) %>% 
  group_by(Study, coef, .draw) %>% 
  summarise(
    r_TimePoint3_EVT_Standard = .posterior_value %>% 
      cor_complete(TimePoint3_EVT_Standard),
    r_TimePoint3_EVT_GSV = .posterior_value %>% 
      cor_complete(TimePoint3_EVT_GSV),
    r_TimePoint2_PPVT_GSV = .posterior_value %>% 
      cor_complete(TimePoint2_PPVT_GSV),
    r_TimePoint2_PPVT_Standard = .posterior_value %>% 
      cor_complete(TimePoint2_PPVT_Standard)) %>% 
  ungroup()
```

```{r correlation intervals, include = FALSE}
# Compute the intervals of the correlations
c_intervals <- vocab_cors %>%
  tidyr::nest(-Study, -coef) %>% 
  mutate(
    data = data %>% 
      purrr::map(select, -.draw) %>% 
      purrr::map(mcmc_intervals_data, prob = .5, prob_outer = .9)) %>% 
  tidyr::unnest()

# Compute pairwise differences in vocabulary correlations
vocab_cor_diffs <- vocab_cors %>% 
  tidyr::gather("variable", "correlation", -Study, -coef, -.draw) %>% 
  tidyr::nest(Study, .draw, correlation) %>% 
  mutate(data = purrr::map(data, . %>% compare_pairs(Study, correlation))) %>% 
  tidyr::unnest() %>% 
  # Negative the pairwise differences and flip the labels. 
  # E.g., if TP2-TP1 = .2, then make TP1-TP2 = -.2.
  mutate(
    value = -value,
    pair = pair %>% 
           str_replace("TimePoint(\\d)-TimePoint(\\d)", "TP\\2 - TP\\1")) %>% 
  tidyr::spread(pair, value) %>% 
  tidyr::nest(-coef, -variable) %>% 
  mutate(
    data = data %>% 
      purrr::map(select, -.draw) %>% 
      purrr::map(mcmc_intervals_data, prob = .5, prob_outer = .9)) %>% 
  tidyr::unnest() %>% 
  mutate(parameter = as.character(parameter))
  
main_diffs <- c_intervals %>% 
  mutate(
    parameter = as.character(parameter),
    variable = parameter) %>% 
  bind_rows(vocab_cor_diffs %>% mutate(Study = parameter)) %>% 
  filter(coef %in% c("intercept", "ot1", "peak_logit"))

main_diffs$Study <- main_diffs$Study %>% 
  factor(
    c("TimePoint1", "TimePoint2", "TimePoint3", 
      "TP1 - TP2", "TP2 - TP3", "TP1 - TP3"))
```

```{r correlation-texts, include = FALSE}
# EVT Standard at TimePoint3
evt_tp3_diffs <- main_diffs %>% 
  filter(variable == "r_TimePoint3_EVT_Standard")

peak_evt_tp3_data <- evt_tp3_diffs %>% 
  filter(coef == "peak_logit") %>% 
  split(.$Study) 
peak_evt_tp3_pts <- get_cor_pts(peak_evt_tp3_data) 
peak_evt_tp3_uis <- get_cor_uis(peak_evt_tp3_data)

ot1_evt_tp3_data <- evt_tp3_diffs %>% 
  filter(coef == "ot1") %>% 
  split(.$Study)
ot1_evt_tp3_pts <- get_cor_pts(ot1_evt_tp3_data) 
ot1_evt_tp3_uis <- get_cor_uis(ot1_evt_tp3_data)

int_evt_tp3_data <- evt_tp3_diffs %>% 
  filter(coef == "intercept") %>% 
  split(.$Study) 
int_evt_tp3_pts <- get_cor_pts(int_evt_tp3_data) 
int_evt_tp3_uis <- get_cor_uis(int_evt_tp3_data)


# PPVT Standard at TimePoint2
ppvt_tp2_diffs <- main_diffs %>% 
  filter(variable == "r_TimePoint2_PPVT_Standard")

peak_ppvt_tp2_data <- ppvt_tp2_diffs %>% 
  filter(coef == "peak_logit") %>% 
  split(.$Study) 
peak_ppvt_tp2_pts <- get_cor_pts(peak_ppvt_tp2_data)
peak_ppvt_tp2_uis <- get_cor_uis(peak_ppvt_tp2_data)

int_ppvt_tp2_data <- ppvt_tp2_diffs %>% 
  filter(coef == "intercept") %>% 
  split(.$Study) 
int_ppvt_tp2_pts <- get_cor_pts(int_ppvt_tp2_data)
int_ppvt_tp2_uis <- get_cor_uis(int_ppvt_tp2_data)

ot1_ppvt_tp2_data <- ppvt_tp2_diffs %>% 
  filter(coef == "ot1") %>% 
  split(.$Study) 
ot1_ppvt_tp2_pts <- get_cor_pts(ot1_ppvt_tp2_data)
ot1_ppvt_tp2_uis <- get_cor_uis(ot1_ppvt_tp2_data)
```

I hypothesized that individual differences in word recognition at age 3
will be more discriminating and predictive future language outcomes than
differences at age 4 or age 5. To test this hypothesis, we calculated
the correlations of growth curve features with age 5 expressive
vocabulary size and age 4 receptive vocabulary. (The receptive test was
not administered during the last year of the study for logistical
reasons.) As with the concordance analysis, I computed each of the
correlations for each sample of the posterior distribution to obtain a
distribution of correlations.

Figure \@ref(fig:evt2-gca-cors) shows the correlations of the peak
looking probability, average looking probability and linear time
features with expressive vocabulary size at age 5, and
Figure \@ref(fig:ppvt4-gca-cors) shows analogous correlations for the
receptive vocabulary at age 4. For all cases, the strongest correlations
were found between the growth curve features at age 3.

Growth curve peaks from age 4 correlated with age 5 vocabulary with
*r* = `r peak_evt_tp3_pts$TimePoint1` [90% UI
`r peak_evt_tp3_uis$TimePoint1`], but the concurrent peaks from age 5
showed a correlation of just *r* = `r peak_evt_tp3_pts$TimePoint3`
[`r peak_evt_tp3_uis$TimePoint3`], a difference between age 3 and
age 5 of *r*<sub>3−5</sub> = `r peak_evt_tp3_pts[["TP1 - TP3"]]`
[`r peak_evt_tp3_uis[["TP1 - TP3"]]`]. A similar pattern held for
lexical processing efficiency values. Linear time features from age 3
correlated with age 5 vocabulary with *r* =
`r ot1_evt_tp3_pts$TimePoint1` [`r ot1_evt_tp3_uis$TimePoint1`],
whereas the concurrent lexical processing values from age 5 only showed
a correlation of *r* = `r ot1_evt_tp3_pts$TimePoint3`
[`r ot1_evt_tp3_uis$TimePoint3`], a difference of *r*<sub>3−5</sub> =
`r ot1_evt_tp3_pts[["TP1 - TP3"]]`
[`r ot1_evt_tp3_uis[["TP1 - TP3"]]`]. For the average looking
probabilities, the correlation for age 3, *r* =
`r int_evt_tp3_pts$TimePoint1` [`r ot1_evt_tp3_uis$TimePoint1`], was
probably only slightly greater than the correlation for age 4,
*r*<sub>3−4</sub> = `r  int_evt_tp3_pts[["TP1 - TP2"]]`
[`r int_evt_tp3_uis[["TP1 - TP2"]]`] but considerably greater than the
concurrent correlation at age 5, *r*<sub>3−5</sub> =
`r  int_evt_tp3_pts[["TP1 - TP3"]]`
[`r int_evt_tp3_uis[["TP1 - TP3"]]`].

(ref:evt2-gca-cors) Uncertainty intervals for the correlations of growth
curve features at each time point with expressive vocabulary (EVT-2
standard scores) at age 5. The bottom rows provide intervals for the
pairwise differences in correlations between timepoints.

```{r evt2-gca-cors, fig.cap = "(ref:evt2-gca-cors)", out.width = "80%", fig.height = 3, fig.width = 6, echo = FALSE}
study_map <- c(
  "TimePoint1" = "paste(Age, phantom(l), 3)", 
  "TimePoint2" = "paste(Age, phantom(l), 4)", 
  "TimePoint3" = "paste(Age, phantom(l), 5)", 
  "TP1 - TP2" = "paste(Age, phantom(l), 3)-paste(Age, phantom(l), 4)", 
  "TP2 - TP3" = "paste(Age, phantom(l), 4)-paste(Age, phantom(l), 5)", 
  "TP1 - TP3" = "paste(Age, phantom(l), 3)-paste(Age, phantom(l), 5)"
)

plot_names <- c(
  intercept = "Avg. Probability", 
  ot1 = "Linear Time",
  peak_logit = "Peak Probability")

main_diffs <- main_diffs %>% 
  mutate(plot_coef = plot_names[coef] %>% 
           factor(c("Peak Probability", "Avg. Probability", "Linear Time")))

main_diffs %>% 
  filter(variable == "r_TimePoint3_EVT_Standard") %>% 
  mutate(PlotStudy = factor(study_map[Study], levels = rev(study_map))) %>% 
  ggplot() + 
    aes(y = PlotStudy) +
    geom_vline(xintercept = 0, size = 2, color = "white") +
    geom_point(aes(x = m), size = 3, shape = 3) + 
    geom_segment(
      aes(y = PlotStudy, yend = PlotStudy, x = min_x, xend = max_x), 
      data = . %>% 
        group_by(plot_coef) %>% 
        mutate(min_x = min(ll), max_x = max(hh)) %>% 
        ungroup(),
      size = .25, color = "white") +
    ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
    ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
    scale_y_discrete(labels = parse_text) + 
    facet_wrap("plot_coef") +
    labs(
      x = NULL, 
      y = " ", 
      caption = constants$cap_median_90_50) + 
    ggtitle(
      "Correlations of curve features and expressive vocabulary at age 5") + 
    theme(title = element_text(size = rel(.9)))
```

Peak looking probabilities from age 3 were strongly correlated with
age 4 receptive vocabulary, *r* = `r peak_ppvt_tp2_pts$TimePoint1`
[`r peak_ppvt_tp2_uis$TimePoint1`], and this correlation was much
greater than the correlation observed for the age 4 growth curve peaks,
*r*<sub>3−4</sub> = `r peak_ppvt_tp2_pts[["TP1 - TP2"]]`
[`r peak_ppvt_tp2_pts[["TP1 - TP2"]]`]. The correlation of age 3
average looking probabilities, *r* = `r int_ppvt_tp2_pts$TimePoint1`
[`r int_ppvt_tp2_uis$TimePoint1`], was greater than the age 4
correlation, *r*<sub>3−4</sub> =
`r int_ppvt_tp2_pts[["TP1 - TP2"]]`
[`r int_ppvt_tp2_uis[["TP1 - TP2"]]`], and the correlation for age 3
linear time features, *r* = `r ot1_ppvt_tp2_pts$TimePoint1`
[`r ot1_ppvt_tp2_uis$TimePoint1`], was likewise greater,
*r*<sub>3−4</sub> = `r ot1_ppvt_tp2_pts[["TP1 - TP2"]]`
[`r ot1_ppvt_tp2_uis[["TP1 - TP2"]]`].

(ref:ppvt4-gca-cors) Uncertainty intervals for the correlations of
growth curve features at each time point with expressive vocabulary
(PPVT-4 standard scores) at age 4. The bottom row shows pairwise
differences between the correlations from timepoints.

```{r ppvt4-gca-cors, fig.cap = "(ref:ppvt4-gca-cors)", out.width = "80%", fig.height = 2, fig.width = 6, echo = FALSE}
main_diffs %>% 
  filter(variable == "r_TimePoint2_PPVT_Standard") %>% 
  filter(Study %in% c("TimePoint1", "TimePoint2", "TP1 - TP2")) %>% 
  mutate(PlotStudy = factor(study_map[Study], levels = rev(study_map))) %>% 
  ggplot() + 
    aes(y = PlotStudy) +
    geom_vline(xintercept = 0, size = 2, color = "white") +
    geom_point(aes(x = m), size = 3, shape = 3) + 
    geom_segment(
      aes(y = PlotStudy, yend = PlotStudy, x = min_x, xend = max_x), 
      data = . %>% 
        group_by(plot_coef) %>% 
        mutate(min_x = min(ll), max_x = max(hh)) %>% 
        ungroup(),
      size = .25, color = "white") +
    ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + 
    ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) +
    scale_y_discrete(labels = parse_text) + 
    facet_wrap("plot_coef") +
    labs(x = NULL, y = " ", caption = constants$cap_median_90_50) + 
    ggtitle(
      "Correlations of curve features and receptive vocabulary at age 4") + 
    theme(title = element_text(size = rel(.9)))
```

**Summary**. Although individual differences in word recognition were
stable over time, early differences were more significant than later
ones. The strongest predictors of future vocabulary size were the growth
curve features from age 3.




Discussion
------------------------------------------------------------------------

In the preceding analyses, I examined many aspects of children's
recognition of familiar words. First, I modeled how children's looking
patterns *on average* changed year over year. Children's word
recognition improved each year: The growth curves grew steeper, reached
higher peaks, and increased in their overall average value each year. This
result was unsurprising, but it was valuable because it confirmed that
this word recognition task scaled with development. The task was simple
enough that children could recognize words at age 3, but challenging
enough for children's performance to improve each year.

After establishing how the averages changed each year, I next asked how
variability changed each year. To tackle this question, I used posterior
predictive inference to have the model simulate samples of data, and in
particular, to simulate new participants. The range of performance
narrowed each year, so that children were most variable at age 3 and
least variable at age 5. This result is consistent with a model of
development children vary widely early on and converge on a more mature
level of performance. From this perspective, word recognition as a skill
is like articulation where most children grow out of immature speech
patterns by grade school. An alternative outcome would have been
troubling: Word recognition differences that expanded with age, the
emergence of a word recognition "gap".

Although the range of individual differences decreased with age,
individual differences did not disappear over time. When children at
each age were ranked using growth curve features, I found a high degree
of correspondence among these ratings. Children who were faster or more
accurate at age 3 remained relatively fast or accurate at age 5. Thus,
differences in word recognition were longitudinally stable over the
preschool years. Extrapolating forwards in time, these differences
likely would become smaller and smaller and become irrelevant for
everyday listening situations. It is plausible, however, that under
adverse listening conditions, individual differences might re-emerge and
differentiate children's word recognition performance.

Lastly, I analyzed how individual differences in word recognition
features correlated with future vocabulary outcomes. The peak looking
probabilities and growth curve slopes from age 3 showed the strongest
correlations with future vocabulary scores. This finding was remarkable:
Expressive vocabulary scores at age 5, for example, were more strongly
correlated with word recognition data collected two years earlier than
word recognition data collected during the same week. 

We can understand the predictive value of age-3 word recognition from
two perspectives. The first interpretation is statistical. Differences
in children's word recognition performance were greatest at age 3, so
word recognition features at age 3 provide more variance and more
information about the children and their future vocabulary size. The
second interpretation is conceptual. Correlations were strongest for the
growth curve peaks. We can think of this feature as measuring children's
maximum word recognition certainty. A child with a peak of .5, for
example, looked the target image half of the time when they were most
certain about the word. Although all of the words used were chosen to be
familiar to preschoolers, children with higher peaks knew those words
*better*. These children had a stronger foundation for word-learning
than children who show more uncertainty during word recognition, and as
a result, these children had developed larger vocabularies two years
later.

