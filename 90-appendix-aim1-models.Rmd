Computational Details for Specific Aim 1 {#aim1-gca-models}
========================================================================

Growth curve analysis models
------------------------------------------------------------------------

The models were fit in R (vers. 3.4.3) with the RStanARM package (vers. 2.16.3).

When I computed the orthogonal polynomial features for Time, they were
rescaled so that the linear feature ranged from −.5 to .5. Under this
scaling a unit change in Time^1^ was equal to change from the start to
the end of the analysis window. Table \@ref(tab:poly-feature-range) shows the 
ranges of the time features.

```{r poly-feature-range, echo = FALSE, message = FALSE, warning = FALSE}
library(dplyr)
d_m <- readr::read_csv("./data/aim1-model-ready.csv.gz")

d_m %>% 
  distinct(ot1, ot2, ot3) %>% 
  tidyr::gather("Feature", "Value") %>% 
  group_by(Feature) %>% 
  summarise(Min = min(Value), Max = max(Value), Range = Max - Min) %>% 
  mutate_if(
    is.numeric, 
    . %>% printy::fmt_fix_digits(2) %>% printy::fmt_minus_sign()) %>% 
  mutate(Feature = stringr::str_replace(Feature, "ot(\\d)", "Time^\\1^")) %>% 
  knitr::kable(
    align = c("l", "r", "r", "r"),
    caption = "Ranges of the polynomial time features")
```

Below is the code used to fit the model with RStanARM. It took
about 24 hours to run the model. `ot1`, `ot2`, and `ot3` are the
polynomial time features, `ResearchID` identifies children, and `Study`
identifies the age/year of the study. `Primary` counts the number of
looks to the target image at each time bin; `Others` counts looks to the
other three images. `cbind(Primary, Others)` is used to package both
counts together for a logistic regression.

```{r, eval = FALSE}
library(rstanarm)
# Run chains on different cores
options(mc.cores = parallel::detectCores())

m <- stan_glmer(
  cbind(Primary, Others) ~
    (ot1 + ot2 + ot3) * Study +
    (ot1 + ot2 + ot3 | ResearchID/Study),
  family = binomial,
  prior = normal(0, 1),
  prior_intercept = normal(0, 5),
  prior_covariance = decov(1, 1, 1),
  data = d_m)

# Save the output
readr::write_rds(m, "./data/stan_aim1_cubic_model.rds.gz")
```

The line `(ot1 + ot2 + ot3 | ResearchID/Study)` describes the random
effects structure of the model with the `/` indicating that data from
each `Study` is nested within each `ResearchID`. Thus, for each child,
we have a general intercept and general effects for Time^1^, Time^2^,
and Time^3^. These child-level effects are further adjusted using
`Study:ResearchID` effects. The effects in each level are allowed to
correlate. For example, I would expect that participants with average
looking probabilities (low intercepts) to have flatter growth curves
(low Time^1^ effects), and this relationship would be captured by one of
the random-effect correlation terms. All told, the random effect
structure and point estimates for the model was as follows:

```{r, include = FALSE}
b <- readr::read_rds("./data/stan_aim1_cubic_model.rds.gz")
b$stan_function <- "stan_glmer"
m <- b
print_out <- purrr::quietly(print)(b)
lines <- print_out$output %>% stringr::str_split("\\n") %>% unlist()
start <- lines %>% stringr::str_which(c("Error terms"))
end <- lines %>% stringr::str_which(c("Num. levels"))
```

```{r}
cat(lines[start:end], sep = "\n")
```

The model used the following priors:

```{r}
prior_summary(m)
```

The priors for the intercept and regression coefficients are wide, very weakly
informative normal distributions. These distributions are
centered at 0, so negative and positive effects are equally likely. The
intercept distribution as a standard deviation of 5, and the
coefficients have a distribution around 3. On the log-odds scale, 95%
looking to target would be 2.94, so effects of this magnitude are easily
included inside these distributions like Normal(0, 3) and Normal(0, 5). 

For the random-effects part of the model, I used RStanARM's `decov()`
prior which simultaneously sets a prior of the variances and
correlations of the model's random effect terms. I used the default
prior for the variance terms and applied a weakly informative LKJ(2)
prior on the random effect correlations. Figure \@ref(fig:lkj-prior)
shows samples from the prior distribution of models fit with the default
LKJ(1) prior and a weakly informative LKJ(2) prior. Under LKJ(2),
extreme correlations are less plausible. The prior shifts the
probability mass away from the ±1 edges towards the center. The
motivation for this kind of prior was *regularization*: We give the
model a small amount of information to nudge it away from extreme,
degenerate values.

(ref:lkj-prior) Samples of correlation effects drawn from the LKJ(1) and
LKJ(2) priors. 

```{r lkj-prior, fig.cap = "(ref:lkj-prior)", echo = FALSE, out.width = "50%", fig.height = 3, fig.width = 4, message = FALSE}
library(ggplot2)
theme_set(theme_grey())
correlations <- readr::read_csv("./data/aim1-lkj-prior-demo.csv.gz")

ggplot(correlations) +
  aes(x = sdcor) +
  geom_histogram(binwidth = .1, boundary = 0) +
  facet_wrap("Model", labeller = label_both) +
  labs(
    x = "Random effect correlation",
    y = "Num. prior samples",
    caption = "4,000 prior samples drawn per model")
```



<!-- For example, consider just the effect of Time^1^. If a -->
<!-- listener starts at chance performance, 25% or `r qlogis(.25) %>% round(2)` -->
<!-- logits, and increases to, say, 65% or `r qlogis(.65) %>% round(2)`, the effect -->
<!-- of a unit change in Time^1^ would be a change of -->
<!-- `r (qlogis(.65) - qlogis(.25)) %>% round(2)` logits. This magnitude of effect is -->
<!-- accommodated by our Normal(0, 1) prior.  -->










<div class = "infobox">

**Box 1: A brief comment about priors.**

Bayesian models require prior information (“priors”). Priors are also
commonly referred to as "prior beliefs", and Bayesian techniques are
criticized or dismissed for smuggling subjectivity into the scientific
enterprise. I find this unfortunate on two grounds. First, *belief*
overstates things. As George Box said, "all models are wrong, but some
are useful" [cite], so no part of a statistical model should be called
a "belief" when the whole thing is a convenient fiction. That's why I
prefer the term *prior information*. [Hat tip Gelman?] Second, other
parts of the statistical model are also subjective: likelihood
functions, what kind of ANOVA, what to covary, whether to transform
measurements, whether a *p* = .07 is a "marginal" effect or no effect at
all, and so on. This subjectivity seems reasonable, provided that we
scientists are open about modeling decisions.

<!-- For these models, I will use weakly to moderately informative priors. -->
<!-- For example, suppose *x* and *y* are scaled to mean 0 and standard -->
<!-- deviation 1. A weakly informative prior for the effect of *x* on *y* -->
<!-- might be Normal(0, 5)—a normal distribution with mean 0 and standard -->
<!-- deviation 5. If we fit a regression model and observed an effect size -->
<!-- of 12 SD units, our first assumption would be that something went wrong -->
<!-- with our software. The weakly informative prior captures this level of -->
<!-- prior information. A moderately informative prior would be Normal(0, 1). -->
<!-- This prior information captures our disciplinary experience that effect -->
<!-- sizes greater than ±1 relatively uncommon in child language research. A -->
<!-- strongly informative prior for this effect might be something like -->
<!-- Normal(.4, .1) which says that our model should be very skeptical of -->
<!-- negative effects and of effects larger than .8. For this project, I will -->
<!-- default to the first two levels of prior information. -->

</div>









