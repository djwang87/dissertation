Prepare and explore the data
===========================================================================

_Notebook status: Code is hidden. Now, the results and figures can be fleshed
out and finalized._

```{r include = FALSE}
knitr::read_chunk("./helpers.R")
if (interactive()) source("./helpers.R")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r helpers, message = FALSE, warning = FALSE}
```

```{r, message = FALSE, warnings = FALSE}
looks1 <- readr::read_csv("./data-raw/rwl_timepoint1_looks.csv.gz")
looks2 <- readr::read_csv("./data-raw/rwl_timepoint2_looks.csv.gz")
looks3 <- readr::read_csv("./data-raw/rwl_timepoint3_looks.csv.gz")
looks <- bind_rows(looks1, looks2, looks3) %>% 
  filter(Version == "Standard")

resp_def <- create_response_def(
  primary = "Target",
  others = c("PhonologicalFoil", "SemanticFoil", "Unrelated"),
  elsewhere = "tracked",
  missing = NA
)

# Keep only frames from -500 to 2000 plus or minus any frames to make the 
# number of frames divisible by 3 (for binning)
times_to_keep <- looks %>% 
  distinct(Time) %>% 
  trim_to_bin_width(3, time_var = Time, key_time = 0, key_position = 2, 
                    min_time = -500, max_time = 2000) %>% 
  pull(Time) %>% 
  range()

raw_data <- looks %>% 
  filter(between(Time, times_to_keep[1], times_to_keep[2])) %>% 
  aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI)
```

## Raw data visualization

First, let's plot the overall averages from each year.

```{r raw-aim1, out.width = "50%", fig.show='hold', fig.height=3, fig.width=3}
ggplot(raw_data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_summary(fun.data = mean_se) +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "Mean ± SE") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))

ggplot(raw_data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_smooth() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "GAM smooth") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))
```

The raw data plainly confirm hypothesis 1:

> Children’s accuracy and efficiency of recognizing words will improve each year.

We can also plot a spaghetti plot to invidual lines for each participant. We see
that they tighten from year to year.

```{r spaghetti-aim1, out.width = "100%", fig.height=2.5, fig.width=6}
ggplot(raw_data) + 
  aes(x = Time, y = Prop, group = ResearchID) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  geom_line(alpha = .15) +
  facet_grid(~ Study) +
  theme_grey(base_size = 9) +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "Lines: Individual participants") +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))
```



## Data screening

```{r, exclude = FALSE}
# We use the following options for data screening.
rules <- list(
  screening_window = c(0, 2000),
  missing_data_limit = .5,
  min_trials = 12
)
```

After mapping the gaze coordinates onto the onscreen images, we performed data
screening. We considered the time window from `r rules$screening_window[1]` to
`r rules$screening_window[2]` ms after target noun onset. We identified a trial
as _unreliable_ if at least `r rules$missing_data_limit * 100`% of the looks
were missing during the time window. We excluded an entire block of trials if it
had fewer than `r rules$min_trials` reliable trials. 

```{r, echo = FALSE, results = 'hide'}
# The first and last times are offset by 20 ms because we downsample our data
# into 50-ms (3-frame) bins. The frames at -16.65, 0, +16.65 ms make up the
# frames in the bin at time 0, so we need to capture the frame at -16.65. The
# same reasoning applies to the frame at 2016.65.
screening_times <- looks %>% 
  distinct(Time) %>% 
  trim_to_bin_width(3, 0, 2, Time, min_time = rules$screening_window[1] - 20, 
                    max_time = rules$screening_window[2] + 20) %>% 
  pull(Time) %>% 
  range()
screening_times

missing_data_by_trial <- looks %>% 
  filter(screening_times[1] <= Time, Time <= screening_times[2]) %>% 
  aggregate_looks(
    resp_def, 
    Study + Version + ResearchID + Basename + TrialNo ~ GazeByImageAOI) %>% 
  mutate(BadTrial = rules$missing_data_limit <= PropNA)

bad_trial_counts <- missing_data_by_trial %>% 
  count(Study, ResearchID, Basename, BadTrial) %>% 
  tidyr::spread(BadTrial, n) %>% 
  rename(n_bad = `TRUE`, n_good = `FALSE`) %>% 
  # Replace NAs with 0, in case there were 0 good trials in a block or 
  # 0 bad trials in a block
  mutate(n_bad = coalesce(n_bad, 0L),
         n_good = coalesce(n_good, 0L),
         trials = n_good + n_bad,
         prop_bad = round(n_bad / trials, 2)) 

blocks_to_drop <- bad_trial_counts %>% 
  filter(.5 <= prop_bad)
blocks_to_drop

leftover_bad_trials <- missing_data_by_trial %>% 
  anti_join(blocks_to_drop, by = c("Study", "ResearchID", "Basename")) %>% 
  filter(.5 <= PropNA)

clean_looks <- looks %>% 
  anti_join(blocks_to_drop, by = c("Study", "ResearchID", "Basename")) %>% 
  anti_join(leftover_bad_trials)
```

Table \@ref(tab:screening-counts) shows the numbers of participants and trials
excluded at each time point due to unreliable data.

```{r screening-counts, echo = FALSE}
screening_results <- list(Screened = clean_looks, Raw = looks) %>% 
  bind_rows(.id = "Dataset") %>% 
  distinct(Dataset, Study, ResearchID, TrialID) %>% 
  group_by(Dataset, Study) %>% 
  summarise(
    `Num Children` = n_distinct(ResearchID),
    `Num Trials` = n_distinct(TrialID))

screening_results <- list(Screened = clean_looks, Raw = looks) %>% 
  bind_rows(.id = "Dataset") %>% 
  distinct(Dataset, Study, ResearchID, Basename, TrialID) %>% 
  group_by(Dataset, Study) %>% 
  summarise(
    `N Children` = n_distinct(ResearchID),
    `N Blocks` = n_distinct(Basename),
    `N Trials` = n_distinct(TrialID)) %>% 
  ungroup()

# Add an extra set of rows computing difference
screening_results2 <- screening_results %>% 
  tidyr::gather("Stat", "Value", -Study, -Dataset) %>% 
  tidyr::spread(Dataset, Value) %>% 
  mutate(`Raw &minus; Screened` = Raw - Screened) %>% 
  tidyr::gather(Dataset, "Count", -Study, -Stat) %>% 
  tidyr::spread(Stat, Count) %>% 
  mutate(Dataset = Dataset %>% 
           factor(c("Raw", "Screened", "Raw &minus; Screened"))) %>% 
  select(Dataset, Study, `N Children`, `N Blocks`, `N Trials`) %>% 
  arrange(Dataset, Study) %>% 
  mutate(Dataset = Dataset %>% as.character() %>% replace_if_same_as_last(""))

knitr::kable(
  screening_results2,
  caption = "Eyetracking data before and after data screening.", 
  booktabs = TRUE)
```



Plot the data after partial data screening. We include the curves from the
earlier plots in gray. The data-cleaning process slightly increases the average
accuracy during the plateau-ed portion of the growth curve.

```{r, include = FALSE}
data <- clean_looks %>% 
  filter(between(Time, times_to_keep[1], times_to_keep[2])) %>% 
  readr::write_csv("./data/aim1-screened.csv.gz") 

agg_data <- data %>% 
  aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI)
```

```{r clean-aim1, echo = FALSE, message = FALSE, warning = FALSE, out.width = "50%", fig.show='hold', fig.height=3, fig.width=3}
ggplot(agg_data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_summary(aes(group = Study), data = raw_data, 
               color = "gray70") +
  stat_summary() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "Mean ± SE. Screened data drawn over raw data (gray).") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))

ggplot(agg_data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_smooth(aes(group = Study), data = raw_data, 
              color = "gray70") +
  stat_smooth() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "GAM smooth. Screened data drawn over raw data (gray).") +
  guides(colour = guide_legend(override.aes = list(fill = NA))) + 
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))
```

### Add a note about the bad version of the experiment

(_Skip for now._)

### Special case data screening

(_Skip for now._ This is where I review the participant notes and will remove 
children who have to be excluded for other reasons, like being diagnosed with a 
language disorder at TimePoint 3.)


```{r}
# code stub for saving the final-final data
```


### Interim summary

* Visual evidence that group averages get faster and more reliable at looking 
  to target each year. 
  
  
## Prepare the dataset for modeling

## Data preparation

Above we cleaned the data to remove trials with excessive missing data 
and blocks of trials with too few trials. Now we prepare the data for modelling.

```{r}
opts_model <- list(
  bin_width = 3,
  start_time = 250,
  end_time = 1500
)
opts_model$bin_length <- round(opts_model$bin_width * 16.67, -1)
opts_model
```

We downsampled the data into `r opts_model$bin_length`-ms
(`r opts_model$bin_width`-frame) bins in order to smooth the data. We modeled
the looks from `r opts_model$start_time` to `r opts_model$end_time` ms. Lastly, 
we aggregated looks by child, study and
time, and created orthogonal polynomials to use as time features for the model

```{r, echo = FALSE, message = FALSE, warnings = FALSE}
data <- readr::read_csv("./data/aim1-screened.csv.gz") %>% 
  select(Study, ResearchID, TrialID:GazeByImageAOI) %>% 
  assign_bins(bin_width = opts_model$bin_width, Time, TrialID)

# Compute time at center of each bin
bin_times <- data %>% 
  distinct(Time, .bin) %>% 
  group_by(.bin) %>% 
  mutate(BinTime = round(median(Time), -1)) %>% 
  ungroup()

# Attach bin times
binned <- data %>% 
  left_join(bin_times, by = c("Time", ".bin")) %>% 
  ungroup() %>% 
  select(-Time) %>% 
  rename(Time = BinTime) 

resp_def <- create_response_def(
  primary = "Target",
  others = c("PhonologicalFoil", "SemanticFoil", "Unrelated"),
  elsewhere = "tracked",
  missing = NA
)  
  
d <- binned %>% 
  aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI)

d_m <- d %>% 
  filter(opts_model$start_time <= Time, 
         Time <= opts_model$end_time) %>% 
  polypoly::poly_add_columns(Time, degree = 3, 
                             scale_width = 1, prefix = "ot")
```

Plot the model-ready data. For this plot, we use the so-called _empirical logit_
transformation because the regular logit (log-odds) generates too extreme of
values for plotting. 

```{r spaghetti-elogit, echo = FALSE, message = FALSE, out.width = "100%", fig.height=2.5, fig.width=6}
ggplot(d_m) + 
  aes(x = Time, y = empirical_logit(Primary, Others)) + 
  geom_line(aes(group = ResearchID), alpha = .2) + 
  stat_smooth() +
  theme_grey(base_size = 9) +
  facet_grid(. ~ Study) + 
  labs(x = "Time after target onset (smoothed to 50 ms bins)", 
       y = "Emp. logit looking to target")
```

Those extreme lines indicate sparse data where there are zero-to-few looks to
the distractors compared to the target. These are the 20 most extreme bins, to 
illustrate how empirical logit tames infinite values.

```{r, echo = FALSE}
d_m %>% 
  select(Study:Time, Primary, Others) %>% 
  arrange(desc(log(Primary / Others))) %>% 
  mutate(
    logit = log(Primary / Others) %>% round(2),
    elogit = empirical_logit(Primary, Others) %>% round(2)) %>% 
  head(20) %>% 
  knitr::kable()
```

```{r, echo = FALSE}
readr::write_csv(d_m, "./data/aim1-model-ready.csv.gz")
```

  
