Methods
===========================================================================

```{r include = FALSE}
knitr::read_chunk("./helpers.R")
if (interactive()) source("./helpers.R")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r helpers, message = FALSE, warning = FALSE}
```

```{r prep-raw-data, message = FALSE, warnings = FALSE}
looks1 <- readr::read_csv("./data-raw/rwl_timepoint1_looks.csv.gz")
looks2 <- readr::read_csv("./data-raw/rwl_timepoint2_looks.csv.gz")
looks3 <- readr::read_csv("./data-raw/rwl_timepoint3_looks.csv.gz")
looks <- bind_rows(looks1, looks2, looks3) %>% 
  filter(Version == "Standard")

resp_def <- create_response_def(
  primary = "Target",
  others = c("PhonologicalFoil", "SemanticFoil", "Unrelated"),
  elsewhere = "tracked",
  missing = NA
)

# Keep only frames from -500 to 2000 plus or minus any frames to make the 
# number of frames divisible by 3 (for binning)
times_to_keep <- looks %>% 
  distinct(Time) %>% 
  trim_to_bin_width(3, time_var = Time, key_time = 0, key_position = 2, 
                    min_time = -500, max_time = 2000) %>% 
  pull(Time) %>% 
  range()

raw_data <- looks %>% 
  filter(between(Time, times_to_keep[1], times_to_keep[2])) %>% 
  aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI)
```

## Methods

### Participants

### Procedure


*Visual World Paradigm Task.* In eyetracking studies with toddlers, two
familiar images are usually presented: a target and a distractor. This
experiment is a four-image eyetracking task that was designed to provide
a more demanding word recognition task for preschoolers. In this
procedure, four familiar images are presented onscreen followed by a
prompt to view one of the images (e.g., *find the bell!*). The four
images include the target word (e.g., *bell*), a semantically related
word (*drum*), a phonologically similar word (*bee*), and an unrelated
word (*swing*).

(ref:sample-vw-screen-cap) Example display for the target *bell* with
the semantic foil *drum*, the phonological foil *bee*, and the unrelated
*swing*.

```{r sample-vw-screen, echo = FALSE, fig.cap = "(ref:sample-vw-screen-cap)", out.width = "100%"}
knitr::include_graphics("./misc/rwl-screens/TimePoint1/actual/Block2_17_swing2_bell2_bee2_drum2_UpperRightImage_bell.png")
```


#### Stimuli

#### Administration















```{r raw-aim1, out.width = "50%", fig.show='hold', fig.height=3, fig.width=3, }
# ## Raw data visualization
# 
# First, let's plot the overall averages from each year.

ggplot(raw_data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_summary(fun.data = mean_se) +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "Mean ± SE") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))

ggplot(raw_data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_smooth() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "GAM smooth") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))
```

```{r spaghetti-aim1, out.width = "100%", fig.height=2.5, fig.width=6, include = FALSE, eval = FALSE}
# The raw data plainly confirm hypothesis 1:
# 
# > Children’s accuracy and efficiency of recognizing words will improve each year.
# 
# We can also plot a spaghetti plot to invidual lines for each participant. We see
# that they tighten from year to year.


ggplot(raw_data) + 
  aes(x = Time, y = Prop, group = ResearchID) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  geom_line(alpha = .15) +
  facet_grid(~ Study) +
  theme_grey(base_size = 9) +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "Lines: Individual participants") +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))
```



## Data preparation and screening

```{r, include = FALSE}
# We use the following options for data screening.
rules <- list(
  screening_window = c(0, 2000),
  missing_data_limit = .5,
  min_trials = 12
)
```

“We mapped the gaze *x*-*y* coordinates onto the images onscreen. We
performed *deblinking* by interpolating short windows of missing data
(up to 150 ms) if the child fixated on the same image before and after a
missing data window. In other words, if the gaze did not shift to
another image, and if the missing data window was short enough, that
window was classified as a blink and interpolated, using the fixated
image as the imputed value” (Mahr & Edwards, in revision).

After mapping the gaze coordinates onto the onscreen images, we performed data
screening. We considered the time window from `r rules$screening_window[1]` to
`r rules$screening_window[2]` ms after target noun onset. We identified a trial
as _unreliable_ if at least `r rules$missing_data_limit * 100`% of the looks
were missing during the time window. We excluded an entire block of trials if it
had fewer than `r rules$min_trials` reliable trials. 

```{r apply-data-screening-rules, echo = FALSE, results = 'hide'}
# The first and last times are offset by 20 ms because we downsample our data
# into 50-ms (3-frame) bins. The frames at -16.65, 0, +16.65 ms make up the
# frames in the bin at time 0, so we need to capture the frame at -16.65. The
# same reasoning applies to the frame at 2016.65.
screening_times <- looks %>% 
  distinct(Time) %>% 
  trim_to_bin_width(3, 0, 2, Time, min_time = rules$screening_window[1] - 20, 
                    max_time = rules$screening_window[2] + 20) %>% 
  pull(Time) %>% 
  range()
screening_times

missing_data_by_trial <- looks %>% 
  filter(screening_times[1] <= Time, Time <= screening_times[2]) %>% 
  aggregate_looks(
    resp_def, 
    Study + Version + ResearchID + Basename + TrialNo ~ GazeByImageAOI) %>% 
  mutate(BadTrial = rules$missing_data_limit <= PropNA)

bad_trial_counts <- missing_data_by_trial %>% 
  count(Study, ResearchID, Basename, BadTrial) %>% 
  tidyr::spread(BadTrial, n) %>% 
  rename(n_bad = `TRUE`, n_good = `FALSE`) %>% 
  # Replace NAs with 0, in case there were 0 good trials in a block or 
  # 0 bad trials in a block
  mutate(n_bad = coalesce(n_bad, 0L),
         n_good = coalesce(n_good, 0L),
         trials = n_good + n_bad,
         prop_bad = round(n_bad / trials, 2)) 

blocks_to_drop <- bad_trial_counts %>% 
  filter(.5 <= prop_bad)
blocks_to_drop

leftover_bad_trials <- missing_data_by_trial %>% 
  anti_join(blocks_to_drop, by = c("Study", "ResearchID", "Basename")) %>% 
  filter(.5 <= PropNA)

clean_looks <- looks %>% 
  anti_join(blocks_to_drop, by = c("Study", "ResearchID", "Basename")) %>% 
  anti_join(leftover_bad_trials)
```

```{r bad-version-counts, message = FALSE}
ids_by_version <- readr::read_csv("./data-raw/rwl_timepoint1_blocks.csv") %>% 
  select(ResearchID, Basename, Version) %>% 
  split(.$Version) %>% 
  lapply(getElement, "ResearchID")

n_lost_in_bad_version <- ids_by_version$`Early attention getter` %>% 
  setdiff(ids_by_version$Standard) %>% 
  length()
```

Table @ref(tab:screening-counts) shows the numbers of participants and trials
excluded at each time point due to unreliable data. There were more children in
the second timepoint than the first timepoint due to a timing error in the
initial version of this experiment, leading to the exclusion of
`r n_lost_in_bad_version` participants from the first timepoint.

```{r screening-counts, echo = FALSE}
screening_results <- list(Screened = clean_looks, Raw = looks) %>% 
  bind_rows(.id = "Dataset") %>% 
  distinct(Dataset, Study, ResearchID, TrialID) %>% 
  group_by(Dataset, Study) %>% 
  summarise(
    `Num Children` = n_distinct(ResearchID),
    `Num Trials` = n_distinct(TrialID))

screening_results <- list(Screened = clean_looks, Raw = looks) %>% 
  bind_rows(.id = "Dataset") %>% 
  distinct(Dataset, Study, ResearchID, Basename, TrialID) %>% 
  group_by(Dataset, Study) %>% 
  summarise(
    `N Children` = n_distinct(ResearchID),
    `N Blocks` = n_distinct(Basename),
    `N Trials` = n_distinct(TrialID)) %>% 
  ungroup()

# Add an extra set of rows computing difference
screening_results2 <- screening_results %>% 
  tidyr::gather("Stat", "Value", -Study, -Dataset) %>% 
  tidyr::spread(Dataset, Value) %>% 
  mutate(`Raw &minus; Screened` = Raw - Screened) %>% 
  tidyr::gather(Dataset, "Count", -Study, -Stat) %>% 
  tidyr::spread(Stat, Count) %>% 
  mutate(Dataset = Dataset %>% 
           factor(c("Raw", "Screened", "Raw &minus; Screened"))) %>% 
  select(Dataset, Study, `N Children`, `N Blocks`, `N Trials`) %>% 
  arrange(Dataset, Study) %>% 
  mutate(Dataset = Dataset %>% as.character() %>% replace_if_same_as_last(""))

knitr::kable(
  screening_results2,
  caption = "Eyetracking data before and after data screening.", 
  booktabs = TRUE)
```














```{r clean-aim1, echo = FALSE, message = FALSE, warning = FALSE, out.width = "50%", fig.show='hold', fig.height=3, fig.width=3}
# Plot the data after partial data screening. We include the curves from the
# earlier plots in gray. The data-cleaning process slightly increases the average
# accuracy during the plateau-ed portion of the growth curve.

data <- clean_looks %>% 
  filter(between(Time, times_to_keep[1], times_to_keep[2])) %>% 
  readr::write_csv("./data/aim1-screened.csv.gz") 

agg_data <- data %>% 
  aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI)

ggplot(agg_data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_summary(aes(group = Study), data = raw_data, 
               color = "gray70") +
  stat_summary() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "Mean ± SE. Screened data drawn over raw data (gray).") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))

ggplot(agg_data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_smooth(aes(group = Study), data = raw_data, 
              color = "gray70") +
  stat_smooth() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "GAM smooth. Screened data drawn over raw data (gray).") +
  guides(colour = guide_legend(override.aes = list(fill = NA))) + 
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))
```

### Special case data screening

(_Skip for now._ This is where I review the participant notes and will remove 
children who have to be excluded for other reasons, like being diagnosed with a 
language disorder at TimePoint 3.)


```{r}
# code stub for saving the final-final data
```


### Interim summary

* Visual evidence that group averages get faster and more reliable at looking 
  to target each year. 
  
  
## Prepare the dataset for modeling

```{r}
opts_model <- list(
  bin_width = 3,
  start_time = 250,
  end_time = 1500
)
opts_model$bin_length <- round(opts_model$bin_width * 16.67, -1)
opts_model
```

To prepare the data for modeling, we downsampled the data into
`r opts_model$bin_length`-ms (`r opts_model$bin_width`-frame) bins, “reducing
the eyetracking sampling rate from 60 Hz to 20 Hz. This procedure smoothed out
high-frequency noise in the data by pooling together data from adjacent frames”
(Mahr & Edwards, in revision). We modeled the looks from
`r opts_model$start_time` to `r opts_model$end_time` ms. Lastly, we aggregated
looks by child, study and time, and created orthogonal polynomials to use as
time features for the model.

```{r, echo = FALSE, message = FALSE, warnings = FALSE}
data <- readr::read_csv("./data/aim1-screened.csv.gz") %>% 
  select(Study, ResearchID, TrialID:GazeByImageAOI) %>% 
  assign_bins(bin_width = opts_model$bin_width, Time, TrialID)

# Compute time at center of each bin
bin_times <- data %>% 
  distinct(Time, .bin) %>% 
  group_by(.bin) %>% 
  mutate(BinTime = round(median(Time), -1)) %>% 
  ungroup()

# Attach bin times
binned <- data %>% 
  left_join(bin_times, by = c("Time", ".bin")) %>% 
  ungroup() %>% 
  select(-Time) %>% 
  rename(Time = BinTime) 

resp_def <- create_response_def(
  primary = "Target",
  others = c("PhonologicalFoil", "SemanticFoil", "Unrelated"),
  elsewhere = "tracked",
  missing = NA
)  
  
d <- binned %>% 
  aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI)

d_m <- d %>% 
  filter(opts_model$start_time <= Time, 
         Time <= opts_model$end_time) %>% 
  polypoly::poly_add_columns(Time, degree = 3, 
                             scale_width = 1, prefix = "ot")
```

Plot the model-ready data. For this plot, we use the so-called _empirical logit_
transformation because the regular logit (log-odds) generates too extreme of
values for plotting. 

```{r spaghetti-elogit, echo = FALSE, message = FALSE, out.width = "100%", fig.height=2.5, fig.width=6}
ggplot(d_m) + 
  aes(x = Time, y = empirical_logit(Primary, Others)) + 
  geom_line(aes(group = ResearchID), alpha = .2) + 
  stat_smooth() +
  theme_grey(base_size = 9) +
  facet_grid(. ~ Study) + 
  labs(x = "Time after target onset (smoothed to 50 ms bins)", 
       y = "Emp. logit looking to target")
```


```{r, echo = FALSE, eval = FALSE}
# Those extreme lines indicate sparse data where there are zero-to-few looks to
# the distractors compared to the target. These are the 20 most extreme bins, to 
# illustrate how empirical logit tames infinite values.

d_m %>% 
  select(Study:Time, Primary, Others) %>% 
  arrange(desc(log(Primary / Others))) %>% 
  mutate(
    logit = log(Primary / Others) %>% round(2),
    elogit = empirical_logit(Primary, Others) %>% round(2)) %>% 
  head(20) %>% 
  knitr::kable()
```

```{r, echo = FALSE}
readr::write_csv(d_m, "./data/aim1-model-ready.csv.gz")
```

  
