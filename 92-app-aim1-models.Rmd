Computational details for Study 1 {#aim1-gca-models}
========================================================================

```{r setup, include = FALSE}
knitr::read_chunk("./helpers.R")
if (interactive()) source("./helpers.R")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r helpers, message = FALSE, warning = FALSE}
```

Growth curve analyses
------------------------------------------------------------------------

These models were fit in R [vers. 3.4.3; @R-base] with the RStanARM package
[vers. 2.16.3; @R-rstanarm].

When I computed the orthogonal polynomial features for Time, they were
scaled so that the linear feature ranged from −.5 to .5. Under this
scaling a unit change in Time^1^ was equal to change from the start to
the end of the analysis window. Table \@ref(tab:poly-feature-ranges) shows
the ranges of the time features.

```{r poly-feature-ranges, echo = FALSE, message = FALSE, warning = FALSE}
library(dplyr)
library(rstanarm)
library(ggplot2)
theme_set(theme_teej())

d_m <- readr::read_csv("./data/aim1-model-ready.csv.gz")

d_m %>% 
  distinct(Time, ot1, ot2, ot3) %>% 
  rename(`Trial window (ms)` = Time) %>% 
  tidyr::gather("Feature", "Value") %>% 
  group_by(Feature) %>% 
  summarise(Min = min(Value), Max = max(Value), Range = Max - Min) %>% 
  mutate_if(
    is.numeric, 
    . %>% printy::fmt_fix_digits(2) %>% printy::fmt_minus_sign()) %>% 
  mutate(Feature = stringr::str_replace(Feature, "ot(\\d)", "Time^\\1^")) %>% 
  knitr::kable(
    align = c("l", "r", "r", "r"),
    caption = "Ranges of the polynomial time features.")
```

It took approximately 24 hours to run the model on four Monte Carlo
sampling chains with 1,000 warm-up iterations and 1,000 sampling
iterations. Warm-up iterations are discarded, so the model
comprises 4,000 samples from the posterior distribution.

The code used to fit the model with RStanARM is printed below. The
variables `ot1`, `ot2`, and `ot3` are the polynomial time features,
`ResearchID` identifies children, and `Study` identifies the age/year of
the longitudinal project. Mnemonically, `ot` stands for *orthogonal
time* and the number is the degree of the polynomial. This convention is
used by @Mirman2014. `Study` refers to the timepoint (year) of the
larger longitudinal investigation. Conceptually, I use *study* to mean a
data-collection unit, and I think of each wave of testing with their
somewhat different tasks and protocols as separate studies. `Primary`
counts the number of looks to the target image at each time bin;
`Others` counts looks to the other three images. 
`cbind(Primary, Others)` is used to package both counts together for a
logistic regression.

```{r, eval = FALSE, echo = TRUE}
library(rstanarm)

# Run chains on different cores
options(mc.cores = parallel::detectCores())

m <- stan_glmer(
  cbind(Primary, Others) ~
    (ot1 + ot2 + ot3) * Study +
    (ot1 + ot2 + ot3 | ResearchID/Study),
  family = binomial,
  prior = normal(0, 1),
  prior_intercept = normal(0, 5),
  prior_covariance = decov(2, 1, 1),
  control = list(
    adapt_delta = .95, 
    max_treedepth = 15),
  data = d_m)

# Save the output
readr::write_rds(m, "./data/stan_aim1_cubic_model.rds.gz")
```

The code `cbind(Primary, Others) ~ (ot1 + ot2 + ot3) * Study` fits a
cubic growth curve for each age. This code uses R's
formula syntax to regress the looking counts onto an intercept term
(implicitly included by default), `ot1`, `ot2`, `ot3` along with the
interactions of the `Study` variable with the intercept, `ot1`, `ot2`,
and `ot3`.

The line `(ot1 + ot2 + ot3 | ResearchID/Study)` describes the
random-effect structure of the model with the `/` indicating that data
from each `Study` is nested within each `ResearchID`. Put another way,
`... | ResearchID/Study` expands into `... | ResearchID` and 
`... | ResearchID:Study`. Thus, for each child, we have general `ResearchID`
effects for the intercept, Time^1^, Time^2^, and Time^3^. These
child-level effects are further adjusted using `Study:ResearchID`
effects. The effects in each level are allowed to correlate. For
example, I would expect that participants with low average looking
probabilities (low intercepts) to have flatter growth curves (low
Time^1^ effects), and this relationship would be captured by one of the
random-effect correlation terms.

Printing the model object reports the point estimates of the model fixed
effects and point-estimate correlation matrices for the random effects.

```{r, include = FALSE}
b <- readr::read_rds("./data/stan_aim1_cubic_model.rds.gz")
b$stan_function <- "stan_glmer"
m <- b
```

```{r, echo = TRUE}
print(m, digits = 2)
```

The model used the following priors:

```{r, echo = TRUE}
prior_summary(m)
```

The priors for the intercept and regression coefficients are wide, 
weakly informative normal distributions. These distributions are
centered at 0, so negative and positive effects are equally likely. The
intercept distribution as a standard deviation of 5, and the
coefficients have a standard deviation of around 3. On the log-odds
scale, 95% looking to target would be 2.94, so effects of this magnitude
are easily accommodated by distributions like 
Normal(0 [mean], 3 [SD]) and Normal(0, 5). 

These priors are very conservative, including information about the size
of an effect but not its direction. @Gelman2014 describe two types of
errors that can arise when estimating an effect or model parameter: Type
S errors where the *sign* of the estimated effect is wrong and Type M
errors where the magnitude of the estimated effect is wrong. From this
perspective, the priors here are uninformative in terms of the sign:
Both positive and negative effects are equally likely before seeing the
data. Future work on these models should incorporate sign information
into the priors: For example, it is
a safe bet that the linear time effect will be positive---the curves
goes up---so that prior can be adjusted to have a positive, non-zero
mean. For this model, I incorporated weak information regarding the
magnitude of the effects (an SD of 3 for all effects). On the basis of
the estimates here, I employed more informative priors in Study 2
with an SD of 2 for the linear time effect and an SD of 1 for other
effects.

For the random-effect part of the model, I used RStanARM's `decov()`
prior which simultaneously sets a prior on the variances and
correlations of the model's random effect terms. I used the default
prior for the variance terms and applied a weakly informative LKJ(2)
prior on the random-effect correlations. Figure \@ref(fig:lkj-prior)
shows samples from the prior distribution of two dummy models fit with
the default LKJ(1) prior and the weakly informative LKJ(2) prior used
here. Under LKJ(2), extreme correlations are less plausible; the prior
shifts the probability mass away from the ±1 boundaries towards the
center. The motivation for this kind of prior was *regularization*: I
give the model a small amount of information to nudge it away from
extreme, degenerate values.

(ref:lkj-prior) Samples of correlation effects drawn from LKJ(1) and LKJ(2) priors. 

```{r lkj-prior, fig.cap = "(ref:lkj-prior)", echo = FALSE, out.width = "50%", fig.height = 3, fig.width = 4, message = FALSE}
correlations <- readr::read_csv("./data/aim1-lkj-prior-demo.csv.gz")

ggplot(correlations) +
  aes(x = sdcor) +
  geom_histogram(binwidth = .1, boundary = 0, color = "grey92") +
  facet_wrap("Model", labeller = label_both) +
  labs(
    x = "Random effect correlation",
    y = "Num. prior samples",
    caption = "4,000 prior samples drawn per model")
```

Summary of the familiar word recognition model with diagnostics and 90%
uncertainty intervals:

```{r, echo = TRUE}
# the last 20 column names are the random effects
ranef_names <- tail(colnames(as_tibble(m)), 20)

summary(
  object = m, 
  pars = c("alpha", "beta", ranef_names),
  probs = c(.05, .95),
  digits = 3)
```


### Convergence diagnostics for Bayesian models

For the Bayesian models estimated in Study 1 and Study 2, I assessed
model convergence by checking software warnings and checking sampling
diagnostics. Stan programs emit warnings when the Hamiltonian Monte
Carlo sampler runs into problems like divergent transitions or a low
Bayesian Fraction of Missing Information statistic. When I encountered
these warnings, I handled them by adjusting the sampling controls, as
documented in &lt;<http://mc-stan.org/misc/warnings.html>&gt;, or incorporating
more information into the model's priors. In the above model, for
example, the `adapt_delta` and `max_treedepth` controls were increased
to help the model more carefully explore the posterior distribution.

```{r, message = TRUE, echo = TRUE}
rstan::check_hmc_diagnostics(m$stanfit)
```

Additionally, I also checked for convergence by using Markov Chain Monte
Carlo diagnostics. The models consisted of four sampling chains which
explore the posterior distribution from random starting locations. The
split $\hat{R}$ ("*R*-hat") diagnostic checks how well the sampling
chains mix together [@StanManual, p. 371; @BDA3, p. 285]. If the
chains are stuck in their own
neighborhoods of the parameter space, then the values sampled in each
chain will not mix very well. The *split* designation means that each
chain is first split in
half so that the also diagnostic checks for within-chain mixing. At
convergence $\hat{R}$ equals 1, so a rule of thumb is that $\hat{R}$
should be less than 1.1 [e.g., @BDA3, p. 285]. In the bayesplot
package [@bayesplot], we use the convention that values below 1.05 are
*good* and values above 1.05 but below than 1.1 are *okay*.

The other diagnostic I monitored was the number of effective samples. If
we think of a sampling chain as exploring a parameter space, then the
samples form a random walk with each step being a movement from an
earlier location. This situation raises the risk of *autocorrelation*
where neighboring sampling steps within a chain are correlated with each
other. The number of effective samples ("*n* eff.") diagnostic estimates
the number of posterior samples, taking into account sampling
autocorrelation [@StanManual, p. 373; @BDA3, p. 285]. The square root
of this number is used to calculate Monte Carlo standard error
statistics [e.g., @BDA3, p. 267], so I think of the number of
effective samples as the amount of precision available for a parameter
estimate. Interpreting this statistic depends on the quantity being
estimated and the amount of precision desired. As a rule of thumb, @BDA3
mentions 10 effective samples per chain as a baseline for diagnosing
non-convergence: "Having an effective sample size of 10 per sequence
should typically correspond to stability of all the simulated sequences.
For some purposes, more precision will be desired, and then a higher
effective sample size threshold can be used. [p. 287]"




Generalized additive models
------------------------------------------------------------------------

To model the looks to the competitor images, I used generalized additive
(mixed) models. The models were fit in R (vers. 3.4.3) using the mgcv R
package [vers. 1.8.23; @Wood2017] with support from tools in the
itsadug R package [vers. `r packageVersion("itsadug")`; @itsadug].

I will briefly walk through the code used to fit one of these models in
order to articulate the modeling decisions at play. I first convert
the categorical variables into the right types, so that the model can 
fit difference smooths.

```{r, eval = FALSE, echo = TRUE}
# Create a Study dummy variabe with Age 4 as the reference level
phon_d$S <- factor(phon_d$Study, c("TimePoint2", "TimePoint1", "TimePoint3"))

# Convert the ResearchID into a factor
phon_d$R <- as.factor(phon_d$ResearchID)

# Convert the Study factor (phon_d$S) into an ordered factor.
# This step is needed for the ti model estimate difference smooths.
phon_d$S2 <- as.ordered(phon_d$S)
contrasts(phon_d$S2) <- "contr.treatment"
contrasts(phon_d$S2)
```

I fit the generalized additive model with the code below. The outcome
`elog` is the empirical log-odds of looking to the phonological
competitor relative to the unrelated word.

```{r, eval = FALSE, echo = TRUE}
library(mgcv)

phon_gam <- bam(
  elog ~ S2 +
    s(Time) + s(Time, by = S2) +
    s(Time, R, bs = "fs", m = 1, k = 5),
  data = phon_d)

# Save the output
readr::write_rds(phon_gam, "./data/aim1-phon-random-smooths.rds.gz")
```

There is just one parametric term: `S2`. The term computes the average
effect of each study with Age 4 serving as the reference condition (and
as the model intercept).

Next come the smooth terms. `s(Time)` fits the shape of Time for the
reference condition (Age 4). `s(Time, by = S2)` fits the difference
smooths for Age 3 versus Age 4 and Age 5 versus Age 4. 
`s(Time, R, bs = "fs", m = 1, k = 5)` fits a smooth for each participant
(`R`). `bs = "fs"` means that the model should use a factor smooth
(`fs`) basis (`bs`)---that is, a "random effect" smooth for each
participant. `m = 1` changes the smoothness penalty so that the
random effects are pulled towards the group average; @Winter2016 and
@Baayen2016 suggest using this option. `k = 5` means to use 5 knots (`k`)
for the basis function. The other smooths use the default number of
knots (10). I used fewer knots for the by-child smooths because of
limited data. As a result, these smooths capture by-child variation by
making coarse adjustments to study-level growth curves. 

Summary of the phonological model:

```{r, include = FALSE}
library(mgcv)
```

```{r, message = FALSE, echo = TRUE}
m_p <- readr::read_rds("./data/aim1-phon-random-smooths.rds.gz")
summary(m_p)
```

Summary of the semantic model:

```{r, message = FALSE, echo = TRUE}
m_s <- readr::read_rds("./data/aim1-semy-random-smooths.rds.gz")
summary(m_s)
```

