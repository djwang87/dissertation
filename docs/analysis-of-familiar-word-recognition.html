<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Development of word recognition in preschoolers</title>
  <meta name="description" content="Development of word recognition in preschoolers">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Development of word recognition in preschoolers" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="tjmahr/dissertation" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Development of word recognition in preschoolers" />
  
  
  

<meta name="author" content="Tristan Mahr">


<meta name="date" content="2018-03-23">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="aim1-method.html">
<link rel="next" href="visualize-looks-to-each-image-type.html">
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="assets\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Word Recognition in Preschoolers</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>Prospectus</b></span></li>
<li class="chapter" data-level="1" data-path="prospectus-preamble.html"><a href="prospectus-preamble.html"><i class="fa fa-check"></i><b>1</b> Prospectus Preamble</a><ul>
<li class="chapter" data-level="" data-path="prospectus-preamble.html"><a href="prospectus-preamble.html#about-this-document"><i class="fa fa-check"></i>About This Document</a></li>
<li class="chapter" data-level="" data-path="prospectus-preamble.html"><a href="prospectus-preamble.html#committee-members"><i class="fa fa-check"></i>Dissertation Committee Members</a></li>
<li class="chapter" data-level="" data-path="prospectus-preamble.html"><a href="prospectus-preamble.html#planned-dissertation-format"><i class="fa fa-check"></i>Planned Dissertation Format</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="specific-aims.html"><a href="specific-aims.html"><i class="fa fa-check"></i><b>2</b> Specific Aims</a><ul>
<li class="chapter" data-level="2.1" data-path="specific-aims.html"><a href="specific-aims.html#specific-aim-1-familiar-word-recognition-and-lexical-competition"><i class="fa fa-check"></i><b>2.1</b> Specific Aim 1 (Familiar Word Recognition and Lexical Competition)</a></li>
<li class="chapter" data-level="2.2" data-path="specific-aims.html"><a href="specific-aims.html#specific-aim-2-referent-selection-and-mispronunciations"><i class="fa fa-check"></i><b>2.2</b> Specific Aim 2 (Referent Selection and Mispronunciations)</a></li>
<li class="chapter" data-level="2.3" data-path="specific-aims.html"><a href="specific-aims.html#summary"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="significance.html"><a href="significance.html"><i class="fa fa-check"></i><b>3</b> Significance</a><ul>
<li class="chapter" data-level="3.1" data-path="significance.html"><a href="significance.html#public-health-significance"><i class="fa fa-check"></i><b>3.1</b> Public Health Significance</a></li>
<li class="chapter" data-level="3.2" data-path="significance.html"><a href="significance.html#scientific-significance"><i class="fa fa-check"></i><b>3.2</b> Scientific Significance</a><ul>
<li class="chapter" data-level="3.2.1" data-path="significance.html"><a href="significance.html#lexical-processing-dynamics"><i class="fa fa-check"></i><b>3.2.1</b> Lexical Processing Dynamics</a></li>
<li class="chapter" data-level="3.2.2" data-path="significance.html"><a href="significance.html#individual-differences-in-word-recognition"><i class="fa fa-check"></i><b>3.2.2</b> Individual Differences in Word Recognition</a></li>
<li class="chapter" data-level="3.2.3" data-path="significance.html"><a href="significance.html#summary-1"><i class="fa fa-check"></i><b>3.2.3</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="research-hypotheses.html"><a href="research-hypotheses.html"><i class="fa fa-check"></i><b>4</b> Research Hypotheses</a><ul>
<li class="chapter" data-level="4.1" data-path="research-hypotheses.html"><a href="research-hypotheses.html#specific-aim-1-familiar-word-recognition-and-lexical-competition-1"><i class="fa fa-check"></i><b>4.1</b> Specific Aim 1 (Familiar Word Recognition and Lexical Competition)</a></li>
<li class="chapter" data-level="4.2" data-path="research-hypotheses.html"><a href="research-hypotheses.html#specific-aim-2-referent-selection-and-mispronunciations-1"><i class="fa fa-check"></i><b>4.2</b> Specific Aim 2 (Referent Selection and Mispronunciations)</a></li>
</ul></li>
<li class="part"><span><b>Aim 1: Familiar Word Recognition and Lexical Competition</b></span></li>
<li class="chapter" data-level="5" data-path="aim1-method.html"><a href="aim1-method.html"><i class="fa fa-check"></i><b>5</b> Method</a><ul>
<li class="chapter" data-level="5.1" data-path="aim1-method.html"><a href="aim1-method.html#participants"><i class="fa fa-check"></i><b>5.1</b> Participants</a><ul>
<li class="chapter" data-level="5.1.1" data-path="aim1-method.html"><a href="aim1-method.html#special-case-data-screening"><i class="fa fa-check"></i><b>5.1.1</b> Special case data screening</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="aim1-method.html"><a href="aim1-method.html#procedure"><i class="fa fa-check"></i><b>5.2</b> Procedure</a></li>
<li class="chapter" data-level="5.3" data-path="aim1-method.html"><a href="aim1-method.html#experiment-administration"><i class="fa fa-check"></i><b>5.3</b> Experiment Administration</a></li>
<li class="chapter" data-level="5.4" data-path="aim1-method.html"><a href="aim1-method.html#stimuli"><i class="fa fa-check"></i><b>5.4</b> Stimuli</a></li>
<li class="chapter" data-level="5.5" data-path="aim1-method.html"><a href="aim1-method.html#data-screening"><i class="fa fa-check"></i><b>5.5</b> Data screening</a></li>
<li class="chapter" data-level="5.6" data-path="aim1-method.html"><a href="aim1-method.html#prepare-the-dataset-for-modeling"><i class="fa fa-check"></i><b>5.6</b> Prepare the dataset for modeling</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="analysis-of-familiar-word-recognition.html"><a href="analysis-of-familiar-word-recognition.html"><i class="fa fa-check"></i><b>6</b> Analysis of familiar word recognition</a><ul>
<li class="chapter" data-level="6.1" data-path="analysis-of-familiar-word-recognition.html"><a href="analysis-of-familiar-word-recognition.html#growth-curve-analysis"><i class="fa fa-check"></i><b>6.1</b> Growth curve analysis</a><ul>
<li class="chapter" data-level="6.1.1" data-path="analysis-of-familiar-word-recognition.html"><a href="analysis-of-familiar-word-recognition.html#growth-curve-features-as-measures-of-word-recognition-performance"><i class="fa fa-check"></i><b>6.1.1</b> Growth curve features as measures of word recognition performance</a></li>
<li class="chapter" data-level="6.1.2" data-path="analysis-of-familiar-word-recognition.html"><a href="analysis-of-familiar-word-recognition.html#year-over-year-changes-in-word-recognition-performance"><i class="fa fa-check"></i><b>6.1.2</b> Year over year changes in word recognition performance</a></li>
<li class="chapter" data-level="6.1.3" data-path="analysis-of-familiar-word-recognition.html"><a href="analysis-of-familiar-word-recognition.html#exploring-plausible-ranges-of-performance-over-time"><i class="fa fa-check"></i><b>6.1.3</b> Exploring plausible ranges of performance over time</a></li>
<li class="chapter" data-level="6.1.4" data-path="analysis-of-familiar-word-recognition.html"><a href="analysis-of-familiar-word-recognition.html#are-individual-differences-stable-over-time"><i class="fa fa-check"></i><b>6.1.4</b> Are individual differences stable over time?</a></li>
<li class="chapter" data-level="6.1.5" data-path="analysis-of-familiar-word-recognition.html"><a href="analysis-of-familiar-word-recognition.html#predicting-future-vocabulary-size"><i class="fa fa-check"></i><b>6.1.5</b> Predicting future vocabulary size</a></li>
<li class="chapter" data-level="6.1.6" data-path="analysis-of-familiar-word-recognition.html"><a href="analysis-of-familiar-word-recognition.html#discussion"><i class="fa fa-check"></i><b>6.1.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="analysis-of-familiar-word-recognition.html"><a href="analysis-of-familiar-word-recognition.html#bayesian-model-results"><i class="fa fa-check"></i><b>6.2</b> Bayesian model results</a><ul>
<li class="chapter" data-level="6.2.1" data-path="analysis-of-familiar-word-recognition.html"><a href="analysis-of-familiar-word-recognition.html#plot-the-intervals-for-the-random-effect-parameters"><i class="fa fa-check"></i><b>6.2.1</b> Plot the intervals for the random effect parameters</a></li>
<li class="chapter" data-level="6.2.2" data-path="analysis-of-familiar-word-recognition.html"><a href="analysis-of-familiar-word-recognition.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>6.2.2</b> Posterior predictive checks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="visualize-looks-to-each-image-type.html"><a href="visualize-looks-to-each-image-type.html"><i class="fa fa-check"></i><b>7</b> Visualize looks to each image type</a><ul>
<li class="chapter" data-level="7.1" data-path="visualize-looks-to-each-image-type.html"><a href="visualize-looks-to-each-image-type.html#looks-to-the-phonological-foil"><i class="fa fa-check"></i><b>7.1</b> Looks to the phonological foil</a></li>
<li class="chapter" data-level="7.2" data-path="visualize-looks-to-each-image-type.html"><a href="visualize-looks-to-each-image-type.html#looks-to-the-semantic-foil"><i class="fa fa-check"></i><b>7.2</b> Looks to the semantic foil</a></li>
<li class="chapter" data-level="7.3" data-path="visualize-looks-to-each-image-type.html"><a href="visualize-looks-to-each-image-type.html#individual-differences-in-competitor-sensitivity"><i class="fa fa-check"></i><b>7.3</b> Individual differences in competitor sensitivity</a></li>
</ul></li>
<li class="part"><span><b>Miscellany</b></span></li>
<li class="chapter" data-level="8" data-path="scratch-paper.html"><a href="scratch-paper.html"><i class="fa fa-check"></i><b>8</b> Scratch paper</a><ul>
<li class="chapter" data-level="8.1" data-path="scratch-paper.html"><a href="scratch-paper.html#bookdown-cheatsheet"><i class="fa fa-check"></i><b>8.1</b> Bookdown cheatsheet</a><ul>
<li class="chapter" data-level="8.1.1" data-path="scratch-paper.html"><a href="scratch-paper.html#manual-section-label-demo"><i class="fa fa-check"></i><b>8.1.1</b> Cross-references to sections</a></li>
<li class="chapter" data-level="8.1.2" data-path="scratch-paper.html"><a href="scratch-paper.html#cross-references-to-appendices"><i class="fa fa-check"></i><b>8.1.2</b> Cross-references to appendices</a></li>
<li class="chapter" data-level="8.1.3" data-path="scratch-paper.html"><a href="scratch-paper.html#cross-references-to-tables"><i class="fa fa-check"></i><b>8.1.3</b> Cross-references to tables</a></li>
<li class="chapter" data-level="8.1.4" data-path="scratch-paper.html"><a href="scratch-paper.html#figure-references-and-using-text-references-as-captions"><i class="fa fa-check"></i><b>8.1.4</b> Figure references and using text references as captions</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="scratch-paper.html"><a href="scratch-paper.html#custom-blocks"><i class="fa fa-check"></i><b>8.2</b> Custom blocks</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="quick-testing-sandbox.html"><a href="quick-testing-sandbox.html"><i class="fa fa-check"></i><b>9</b> Quick testing sandbox</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="aim1-gca-models.html"><a href="aim1-gca-models.html"><i class="fa fa-check"></i><b>A</b> Computational Details for Specific Aim 1</a><ul>
<li class="chapter" data-level="A.1" data-path="aim1-gca-models.html"><a href="aim1-gca-models.html#growth-curve-analysis-models"><i class="fa fa-check"></i><b>A.1</b> Growth Curve Analysis Models</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="vw-experiment-items.html"><a href="vw-experiment-items.html"><i class="fa fa-check"></i><b>B</b> Items used in the visual world experiment</a></li>
<li class="chapter" data-level="C" data-path="mp-experiment-items.html"><a href="mp-experiment-items.html"><i class="fa fa-check"></i><b>C</b> Items used in the mispronunciation experiment</a></li>
<li class="chapter" data-level="D" data-path="related-work.html"><a href="related-work.html"><i class="fa fa-check"></i><b>D</b> Related Work</a></li>
<li class="chapter" data-level="" data-path="colophon.html"><a href="colophon.html"><i class="fa fa-check"></i>Colophon</a><ul>
<li class="chapter" data-level="D.1" data-path="colophon.html"><a href="colophon.html#debug-info"><i class="fa fa-check"></i><b>D.1</b> Debug info</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Created with bookdown</a></li>
<li><a href="https://tjmahr.github.io/" target="blank">Tristan Mahr</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Development of word recognition in preschoolers</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="analysis-of-familiar-word-recognition" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Analysis of familiar word recognition</h1>
<!-- General Todos: -->
<!-- * Determine and use consistent terminology for the studies and growth curve -->
<!--   features. -->
<!-- * Make sure axis labels and legend labels are consistent. -->
<div id="growth-curve-analysis" class="section level2">
<h2><span class="header-section-number">6.1</span> Growth curve analysis</h2>
<p>Looks to the familiar image were analyzed using Bayesian mixed effects logistic regression. I used <em>logistic</em> regression because the outcome measurement is a probability (the log-odds of looking to the target image versus a distractor). I used <em>mixed-effects</em> models to estimate a separate growth curve for each child (to measure individual differences in word recognition) but also treat each child’s individual growth curve as a draw from a distribution of related curves. I used <em>Bayesian</em> techniques to study a generative model of the data. Instead of reporting and describing a single, best-fitting model of some data, Bayesian methods consider an entire distribution of plausible models that are consistent with the data and any prior information we have about the models. By using this approach, one can explicitly quantify uncertainty about statistical effects and draw inferences using estimates of uncertainty (instead of using statistical significance—which is not a straightforward matter for mixed-effects models).<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>The eyetracking growth curves were fit using an orthogonal cubic polynomial function of time <span class="citation">(a now-conventional approach; see Mirman, <a href="#ref-Mirman2014">2014</a>)</span>. Put differently, I modeled the probability of looking to the target during an eyetracking task as:</p>
<p><span class="math display">\[
\text{log-odds}(\textit{looking}\,) = 
  \beta_0 + 
  \beta_1\text{Time}^1 + 
  \beta_2\text{Time}^2 + 
  \beta_3\text{Time}^3
\]</span></p>
<p>That the time terms are <em>orthogonal</em> means that <span class="math inline">\(\text{Time}^1\)</span>, <span class="math inline">\(\text{Time}^2\)</span> and <span class="math inline">\(\text{Time}^3\)</span> are transformed so that they are uncorrelated. Under this formulation, the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> have a direct interpretation in terms of lexical processing performance. The intercept, <span class="math inline">\(\beta_0\)</span>, measures the area under the growth curve—or the probability of fixating on the target word averaged over the whole window. We can think of <span class="math inline">\(\beta_0\)</span> as a measure of <em>word recognition reliability</em>. The linear time parameter, <span class="math inline">\(\beta_1\)</span>, estimates the steepness of the growth curve—or how the probability of fixating changes from frame to frame. We can think of <span class="math inline">\(\beta_1\)</span> as a measure of <em>processing efficiency</em>, because growth curves with stronger linear features exhibit steeper frame-by-frame increases in looking probability.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>To study how word recognition changes over time, I modeled how the growth curves change over developmental time. This amounted to studying how the growth curve parameters changes year over year. I included dummy-coded indicators for Age 3, Age 4, and Age 5 and allowed these indicators interact with the growth curve parameters. These year-by-growth-curve terms captured how the shape of the growth curves changed each year. The model also included random effects to represent child-by-year effects.</p>
<div id="growth-curve-features-as-measures-of-word-recognition-performance" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Growth curve features as measures of word recognition performance</h3>
<p>As mentioned above, two of the model’s growth curve features have straightforward interpretations in terms of lexical processing performance: The model’s intercept parameter corresponds to the average proportion or probability of looking to the named image over the trial window, and the linear time parameter corresponds to slope of the growth curve or lexical processing efficiency. I also was interested in <em>peak</em> proportion of looks to the target. I derived this value by computing the growth curves from the model and taking the median of the five highest points on the curve. Figure <a href="analysis-of-familiar-word-recognition.html#fig:curve-features">6.1</a> shows three simulated growth curves and how each of these growth curve features relate to word recognition performance.</p>

<div class="figure"><span id="fig:curve-features"></span>
<img src="12-aim1-notebook_files/figure-html/curve-features-1.png" alt="Illustration of the three growth curve features and how they describe lexical processing performance. The three curves used are simulations of new participants at Age 4." width="80%" />
<p class="caption">
Figure 6.1: Illustration of the three growth curve features and how they describe lexical processing performance. The three curves used are simulations of new participants at Age 4.
</p>
</div>
</div>
<div id="year-over-year-changes-in-word-recognition-performance" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Year over year changes in word recognition performance</h3>
<p>The mixed-effects model estimated a population-average growth curve (“fixed” effects) and how individual children deviated from average (“random” effects). Figure <a href="analysis-of-familiar-word-recognition.html#fig:average-growth-curves">6.2</a> shows 200 posterior samples of the average growth curves for each study. On average, the growth curves become steeper and achieve higher looking probabilities with each year of the study.</p>

<div class="figure"><span id="fig:average-growth-curves"></span>
<img src="12-aim1-notebook_files/figure-html/average-growth-curves-1.png" alt="The model estimated an average word recognition growth for each study, and the colored lines represent 200 posterior samples of these growth curves. The thick dark lines represent the observed average growth curve in each study." width="50%" />
<p class="caption">
Figure 6.2: The model estimated an average word recognition growth for each study, and the colored lines represent 200 posterior samples of these growth curves. The thick dark lines represent the observed average growth curve in each study.
</p>
</div>
<p>Figure <a href="analysis-of-familiar-word-recognition.html#fig:effects2">6.3</a> depicts uncertainty intervals with the model’s average effects of each timepoint on the growth curve features. The intercept and linear time effects increased each year, confirming that children become more reliable and faster at recognizing words as they grow older. The peak accuracy also increased each year. For each effect, the change from age 3 to age 4 is approximately the same as the change from age 4 to age 5, as visible in Figure <a href="analysis-of-familiar-word-recognition.html#fig:pairwise-effects">6.4</a>.</p>

<div class="figure"><span id="fig:effects2"></span>
<img src="12-aim1-notebook_files/figure-html/effects2-1.png" alt="Uncertainty intervals for the effects of study years on growth curve features. The intercept and peak features were converted from log-odds to proportions to ease interpretation." width="80%" />
<p class="caption">
Figure 6.3: Uncertainty intervals for the effects of study years on growth curve features. The intercept and peak features were converted from log-odds to proportions to ease interpretation.
</p>
</div>

<div class="figure"><span id="fig:pairwise-effects"></span>
<img src="12-aim1-notebook_files/figure-html/pairwise-effects-1.png" alt="Uncertainty intervals for the differences between study timepoints. Again, the intercept and peak features were converted to proportions." width="50%" /><img src="12-aim1-notebook_files/figure-html/pairwise-effects-2.png" alt="Uncertainty intervals for the differences between study timepoints. Again, the intercept and peak features were converted to proportions." width="50%" />
<p class="caption">
Figure 6.4: Uncertainty intervals for the differences between study timepoints. Again, the intercept and peak features were converted to proportions.
</p>
</div>
<p>The average looking probability (intercept feature) was [90% UI: ] at age 3, [] at age 4, and [] at age 5. The averages increased by [] from age 3 to age 4 and by [] from age 4 to age 5. The peak looking probability was [] at age 3, [] at age 4, and [] at age 5. The peak values increased by [] from age 3 to age 4 and by [] from age 4 to age 5. These results numerically confirm the hypothesis that children would improve in their word recognition reliability, both in terms of average looking and in terms of peak accuracy, each year.</p>
<p><strong>Summary</strong>. The average growth curve features increased year over year, so that children looked to the target more quickly and more reliably.</p>
<ul>
<li>This result is as expected.</li>
<li>It’s good that the task scaled with development so that there was room to grow each year.</li>
<li>The growth curve changes each year involved peak accuracy and steepness of the curve. They reach higher heights, and they hit year 1 peak earlier each year.</li>
</ul>
</div>
<div id="exploring-plausible-ranges-of-performance-over-time" class="section level3">
<h3><span class="header-section-number">6.1.3</span> Exploring plausible ranges of performance over time</h3>
<p>Bayesian models are generative; they describe how the data could have been generated. This model assumed that each child’s growth curve was drawn from a population of related growth curves, and it tried to infer the parameters over that distribution. These two aspects—a generative model and learning about the population of growth curves—allow the model to simulate new samples from that distribution of growth curves. That is, we can predict a set of growth curves for a hypothetical, unobserved child drawn from the same distribution as the 195 observed children. This procedure allows one to explore the plausible degrees of variability in performance at each age.</p>
<p>Figure <a href="analysis-of-familiar-word-recognition.html#fig:new-participants">6.5</a> shows the posterior predictions for 1,000 simulated participants, which demonstrates how the model expects new participants to improve longitudinally but also exhibit stable individual differences over time. Figure <a href="analysis-of-familiar-word-recognition.html#fig:new-participants-intervals">6.6</a> shows uncertainty intervals for these simulations. The model learned to predict less accurate and more variable performance at age 3 with improving accuracy and narrowing variability at age 4 and age 5.</p>

<div class="figure"><span id="fig:new-participants"></span>
<img src="12-aim1-notebook_files/figure-html/new-participants-1.png" alt="Posterior predictions for hypothetical unobserved participants. Each line represents the predicted performance for a new participant. The three dark lines highlight predictions from one single simulated participant. The simulated participant shows both longitudinal improvement in word recognition and similar relative performance compared to other simulations each year, indicating that the model would predict new children to improve year over year and show stable individual differences over time." width="80%" />
<p class="caption">
Figure 6.5: Posterior predictions for hypothetical <em>unobserved</em> participants. Each line represents the predicted performance for a new participant. The three dark lines highlight predictions from one single simulated participant. The simulated participant shows both longitudinal improvement in word recognition and similar relative performance compared to other simulations each year, indicating that the model would predict new children to improve year over year and show stable individual differences over time.
</p>
</div>

<div class="figure"><span id="fig:new-participants-intervals"></span>
<img src="12-aim1-notebook_files/figure-html/new-participants-intervals-1.png" alt="Uncertainty intervals for the simulated participants. Variability is widest at age 3 and narrowest at age 5, consistent with the prediction that children become less variable as they grow older." width="80%" />
<p class="caption">
Figure 6.6: Uncertainty intervals for the simulated participants. Variability is widest at age 3 and narrowest at age 5, consistent with the prediction that children become less variable as they grow older.
</p>
</div>
<p>I hypothesized that children would become less variable as they grew older and converged on a mature level of performance. I address this question by inspecting the ranges of predictions for the simulated participants. The claim that children become less variable would imply that the range of predictions should be narrower age 5 than for age 4 than age 3. Figure <a href="analysis-of-familiar-word-recognition.html#fig:new-ranges">6.7</a> depicts the range of the predictions, both in terms of the 90 percentile range (i.e., the range of the middle 90% of the data) and in terms of the 50 percentile (interquartile) range. The ranges of performance decrease from age 3 to age 4 to age 5, consistent with the hypothesized reduction in variability.</p>

<div class="figure"><span id="fig:new-ranges"></span>
<img src="12-aim1-notebook_files/figure-html/new-ranges-1.png" alt="Ranges of predictions for simulated participants over the course of a trial. The ranges are most similar during the first half of the trial when participants are at chance performance, and the ranges are most different at the end of the trial as children reliably fixate on the target image. The ranges of performance decreases with each year of the study as children show less variability." width="80%" />
<p class="caption">
Figure 6.7: Ranges of predictions for simulated participants over the course of a trial. The ranges are most similar during the first half of the trial when participants are at chance performance, and the ranges are most different at the end of the trial as children reliably fixate on the target image. The ranges of performance decreases with each year of the study as children show less variability.
</p>
</div>
<p>The developmental pattern of increasing reliability and decreasing variability was also observed for the growth curve peaks. For the synthetic participants, the model predicted that individual peak probabilities will increase each year, peak<sub>3</sub> = 0.55 [90% UI: 0.35–0.77], peak<sub>4</sub> = 0.69 [0.48–0.86], peak<sub>5</sub> = 0.78 [0.59–0.91]. Moreover, the range of plausible values for the individual peaks narrowed each for the simulated data. For instance, the difference between the 95<sup>th</sup> and 5<sup>th</sup> percentiles was 0.43 for age 3, 0.38 for age 4, and 0.32 for age 5.</p>
<p><strong>Summary</strong>. I used the model’s random effects estimates to simulate growth curves from 1,000 hypothetical, unobserved participants. The simulated dataset showed increasing looking probability and decreasing variability with each year of the study. These simulations confirm the hypothesis that variability would be diminish as children converge on a mature level of performance on this task.</p>
<ul>
<li>Word recognition performance is a skill where variation is greatest at younger ages.</li>
<li>What mechanisms might come to bear on this? Does variability narrow developmentally for vocabulary?</li>
<li>Children different in their word-learning trajectories, so the early differences in word recognition could be from younger children who are relatively early/late in word-learning. The SDs of the EVT-2 scores narrows a small amount each year, even when we only consider the children who participated at all three years.</li>
<li>(It will be easier to fold this in to the mechanism discussion once we have firmer results for the looks-to-foils analysis.)</li>
<li>If differences in word recognition matter (and they do) and the differences are greatest at younger ages, then they are most informative at younger ages.</li>
<li>Maybe a few words on why individual differences are worth studying?</li>
</ul>
</div>
<div id="are-individual-differences-stable-over-time" class="section level3">
<h3><span class="header-section-number">6.1.4</span> Are individual differences stable over time?</h3>
<p>I predicted that children would show stable individual differences such that children who are faster and more reliable at recognizing words at age 3 remain relatively faster and more reliable at age 5. To evaluate this hypothesis, I used Kendall’s <em>W</em> (the coefficient of correspondence or concordance). This nonparametric statistic measures the degree of agreement among <em>J</em> judges who are rating <em>I</em> items. For these purposes, the items are the 123 children who provided reliable eyetracking for all three years of the study. (That is, I excluded children who only had reliable eyetracking data for one or two years.) The judges are the sets of growth curve parameters from each year of study. For example, the intercept term provides three sets of ratings: The participants’ intercept terms from year 1 are one set of ratings and the terms from years 2 and 3 provide two more sets of ratings. These three ratings are the “judges” used to compute the intercept’s <em>W</em>. Thus, I computed five groups of <em>W</em> coefficients, one for each set of growth curve features: Intercept, Time<sup>1</sup>, Time<sup>2</sup>, Time<sup>3</sup>, and Peak looking probability.</p>
<p>Because I used a Bayesian model, there is a distribution of ratings and thus a distribution of concordance statistics. Each sample of the posterior distribution fits a growth curve for each child in each study, so each posterior sample provides a set of ratings for concordance coefficients. The distribution of <em>W</em>’s lets us quantify our uncertainty because we can compute <em>W</em>’s for each of the 4000 samples from the posterior distribution.</p>
<p>One final matter is how to assess whether a concordance statistic is meaningful. To tackle this question, I also included a “null rater”, a fake parameter that assigned each child in each year a random number. I use the distribution of <em>W</em>’s generated by randomly rating children as a benchmark for assessing whether the other concordance statistics differ meaningfully from chance.</p>

<div class="figure"><span id="fig:kendall-stats"></span>
<img src="12-aim1-notebook_files/figure-html/kendall-stats-1.png" alt="Uncertainty intervals for the Kendall’s coefficient of concordance. Random ratings provide a baseline of null W statistics. The intercept and linear time features are decisively non-null, indicating a significant degree of correspondence in children’s relative word recognition reliability and efficiency over three years of study." width="80%" />
<p class="caption">
Figure 6.8: Uncertainty intervals for the Kendall’s coefficient of concordance. Random ratings provide a baseline of null <em>W</em> statistics. The intercept and linear time features are decisively non-null, indicating a significant degree of correspondence in children’s relative word recognition reliability and efficiency over three years of study.
</p>
</div>
<p>We used the <code>kendall()</code> function in the irr R package (vers. 0.84, CITATION) to compute concordance statistics. Figure <a href="analysis-of-familiar-word-recognition.html#fig:kendall-stats">6.8</a> depicts uncertainty intervals for the Kendall <em>W</em>’s for these growth curve features. The 90% uncertainty interval of <em>W</em> statistics from random ratings [0.28–0.39] subsumes the intervals for the Time<sup>2</sup> effect [0.30–0.35] and the Time<sup>3</sup> effect [0.28–0.35], indicating that these values do not differentiate children in a longitudinally stable way. That is, the Time<sup>2</sup> and Time<sup>3</sup> features differentiate children across studies as well as random numbers. Earlier, I stated that only the intercept, linear time, and peak features have psychologically meaningful interpretations and that the higher-order features of these models serve to capture the shape of the growth curve data. These concordance statistics support that assertion.</p>
<p>Concordance is strongest for the peak feature, <em>W</em> = 0.59 [0.57–0.60] and the intercept term, <em>W</em> = 0.58 [0.57–0.60], followed by the linear time term, <em>W</em> = 0.50 [0.48–0.52]. Because these values are far removed from the statistics for random ratings, I conclude that there is a credible degree of correspondence across studies when ranking children using their peak looking probability, average look probability (the intercept) or their growth curve slope (linear time).</p>
<p><strong>Summary</strong>. Growth curve features reflect individual differences in word recognition reliability and efficiency. By using Kendall’s <em>W</em> to measure the degree of concordance among growth curve features over developmental time, I tested whether individual differences in lexical processing persisted over development. I found that the peak looking probability, average looking probability and linear time features were stable over time.</p>
<ul>
<li>Although the range of variability decreases, individual differences do not wash out.</li>
<li>Lexical processing is a stable ability over the preschool years.</li>
<li>Extrapolating outwards, the differences probably diminish to the point that they are not meaningful. But traces of those early differences can reappear years later on some test scores.</li>
</ul>
</div>
<div id="predicting-future-vocabulary-size" class="section level3">
<h3><span class="header-section-number">6.1.5</span> Predicting future vocabulary size</h3>
<p>I hypothesized that individual differences in word recognition at age 3 will be more discriminating and predictive future language outcomes than differences at age 4 or age 5. To test this hypothesis, we calculated the correlations of growth curve features with age 5 expressive vocabulary size and age 4 receptive vocabulary. (The receptive test was not administered during the last year of the study for logistical reasons.) As with the concordance analysis, I computed each of the correlations for each sample of the posterior distribution to obtain a distribution of correlations.</p>
<p>Figure <a href="analysis-of-familiar-word-recognition.html#fig:evt2-gca-cors">6.9</a> shows the correlations of the peak looking probability, average looking probability and linear time features with expressive vocabulary size at age 5, and Figure <a href="analysis-of-familiar-word-recognition.html#fig:ppvt4-gca-cors">6.10</a> shows analogous correlations for the receptive vocabulary at age 4. For all cases, the strongest correlations were found between the growth curve features at age 3.</p>
<p>Growth curve peaks from age 4 correlated with age 5 vocabulary with <em>r</em> = .52, 90% UI [.50–.54], but the concurrent peaks from age 5 showed a correlation of just <em>r</em> = .31, [.29–.33], a difference between age 3 and age 5 of <em>r</em><sub>3−5</sub> = .21, [.18–.24]. A similar pattern held for lexical processing efficiency values. Linear time features from age 3 correlated with age 5 vocabulary with <em>r</em> = .41, [.39–.44], whereas the concurrent lexical processing values from age 5 only showed a correlation of <em>r</em> = .28, [.26–.31], a difference of <em>r</em><sub>3−5</sub> = .13, [.10–.16]. For the average looking probabilities, the correlation for age 3, <em>r</em> = .39, [.39–.44], was probably only slightly greater than the correlation for age 4, <em>r</em><sub>3−4</sub> = .02, [−.01–.04] but considerably greater than the concurrent correlation at age 5, <em>r</em><sub>3−5</sub> = .08, [.05–.10].</p>

<div class="figure"><span id="fig:evt2-gca-cors"></span>
<img src="12-aim1-notebook_files/figure-html/evt2-gca-cors-1.png" alt="Uncertainty intervals for the correlations of growth curve features at each time point with expressive vocabulary (EVT-2 standard scores) at age 5. The bottom rows provide intervals for the pairwise differences in correlations between timepoints." width="80%" />
<p class="caption">
Figure 6.9: Uncertainty intervals for the correlations of growth curve features at each time point with expressive vocabulary (EVT-2 standard scores) at age 5. The bottom rows provide intervals for the pairwise differences in correlations between timepoints.
</p>
</div>
<p>Peak looking probabilities from age 3 were strongly correlated with age 4 receptive vocabulary, <em>r</em> = .62, [.61–.64], and this correlation was much greater than the correlation observed for the age 4 growth curve peaks, <em>r</em><sub>3−4</sub> = .26, [.26]. The correlation of age 3 average looking probabilities, <em>r</em> = .45, [.44–.47], was greater than the age 4 correlation, <em>r</em><sub>TP1−TP2</sub> = .08, [.08], and the correlation for age 3 linear time features, <em>r</em> = .51, [.49–.54], was likewise greater, <em>r</em><sub>3−4</sub> = .22, [.19–.26].</p>

<div class="figure"><span id="fig:ppvt4-gca-cors"></span>
<img src="12-aim1-notebook_files/figure-html/ppvt4-gca-cors-1.png" alt="Uncertainty intervals for the correlations of growth curve features at each time point with expressive vocabulary (PPVT-4 standard scores) at age 4. The bottom row shows pairwise differences between the correlations from timepoints." width="80%" />
<p class="caption">
Figure 6.10: Uncertainty intervals for the correlations of growth curve features at each time point with expressive vocabulary (PPVT-4 standard scores) at age 4. The bottom row shows pairwise differences between the correlations from timepoints.
</p>
</div>
<p><strong>Summary</strong>. Although individual differences in word recognition are stable over time, early differences are more significant than later ones. The strongest predictors of future vocabulary size were the growth curve features from age 3. That is, word recognition performance from age 3 was more strongly correlated with age 5 expressive vocabulary than word recognition performance at age 5. A similar pattern of results held for predicting receptive vocabulary at age 4.</p>
<ul>
<li>This finding is surprising because vocabulary scores from the same week as the eyetracking data are less correlated than scores from two year earlier.</li>
<li>This establishes that the differences are greatest and most predictive at younger ages.</li>
</ul>
<!-- ### Relationships with other child-level predictors -->
<!-- _TJM: This is where I would analyze the other test scores as we have discussed._ -->
</div>
<div id="discussion" class="section level3">
<h3><span class="header-section-number">6.1.6</span> Discussion</h3>
</div>
</div>
<div id="bayesian-model-results" class="section level2">
<h2><span class="header-section-number">6.2</span> Bayesian model results</h2>
<p>The output below contains the model quick view, a summary of the fixed effect terms, and a summary of the priors used.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">b

<span class="kw">summary</span>(b, <span class="dt">pars =</span> <span class="kw">names</span>(<span class="kw">fixef</span>(b)))

<span class="kw">prior_summary</span>(b)</code></pre></div>
<p>Let’s try to understand our model by making some plots.</p>
<div id="plot-the-intervals-for-the-random-effect-parameters" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Plot the intervals for the random effect parameters</h3>
<p>These are the parameters governing the random effect distributions. First, we plot the standard deviations. Recall that in our hierarchical model we suppose that each growth curve is drawn from a population of related curves. The model’s fixed effects estimate the means of the distribution. These terms estimate the variability around that mean. We did not have any a priori hypotheses about the values of these scales, so do not discuss them any further.</p>
<p><img src="12-aim1-notebook_files/figure-html/posterior-sds-1.png" width="80%" /></p>
<p>Then the correlations.</p>
<p><img src="12-aim1-notebook_files/figure-html/posterior-cors-1.png" width="80%" /></p>
</div>
<div id="posterior-predictive-checks" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Posterior predictive checks</h3>
<p>Bayesian models are generative; they describe how the data could have been generated. One way to evaluate the model is to have it simulate new observations. If the simulated data closely resembles the observed data, then we have some confidence that our model has learned an approximation of how the data could have been generated. Figure <a href="analysis-of-familiar-word-recognition.html#fig:post-pred">6.11</a> depicts the density of the observed data from each year of the study versus 200 posterior simulations. Because the simulations closely track the density of the observed data, we can infer that the model has learned how to generate data from each year of the study.</p>

<div class="figure"><span id="fig:post-pred"></span>
<img src="12-aim1-notebook_files/figure-html/post-pred-1.png" alt="Posterior predictive density for the observed data from each year of the study. The x-axis represents the outcome measure—the proportion of looks to the target image—and the y-axis is the density of those values at year. At age 3, there is a large density of looks around chance performance (.25) with a rightward skew (above-chance looks are common). At age 4 and age 5, a bimodal distribution emerges, reflecting how looks start at chance and reliably increase to above-chance performance. Each light line is a simulation of the observed data from the model, and the thick lines are the observed data. Because the thick line is surrounded by light lines, we visually infer that the the model faithfully approximates the observed data." width="80%" />
<p class="caption">
Figure 6.11: Posterior predictive density for the observed data from each year of the study. The <em>x</em>-axis represents the outcome measure—the proportion of looks to the target image—and the <em>y</em>-axis is the density of those values at year. At age 3, there is a large density of looks around chance performance (.25) with a rightward skew (above-chance looks are common). At age 4 and age 5, a bimodal distribution emerges, reflecting how looks start at chance and reliably increase to above-chance performance. Each light line is a simulation of the observed data from the model, and the thick lines are the observed data. Because the thick line is surrounded by light lines, we visually infer that the the model faithfully approximates the observed data.
</p>
</div>
<p>We can ask the model make even more specific posterior predictions. Below we plot the posterior predictions for random participants. This is the model simulating new data for these participants.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">09272017</span>)

ppred &lt;-<span class="st"> </span>d_m <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">sample_n_of</span>(<span class="dv">8</span>, ResearchID) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>tristan<span class="op">::</span><span class="kw">augment_posterior_predict</span>(b, <span class="dt">newdata =</span> ., <span class="dt">nsamples =</span> <span class="dv">100</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">trials =</span> Primary <span class="op">+</span><span class="st"> </span>Others)

<span class="kw">ggplot</span>(ppred) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">aes</span>(<span class="dt">x =</span> Time, <span class="dt">y =</span> Prop, <span class="dt">color =</span> Study, <span class="dt">group =</span> Study) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> .posterior_value <span class="op">/</span><span class="st"> </span>trials, 
                <span class="dt">group =</span> <span class="kw">interaction</span>(.draw, Study)), 
            <span class="dt">alpha =</span> .<span class="dv">20</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">color =</span> <span class="st">&quot;grey50&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="st">&quot;ResearchID&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme</span>(
    <span class="dt">legend.position =</span> <span class="kw">c</span>(.<span class="dv">95</span>, <span class="dv">0</span>), 
    <span class="dt">legend.justification =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>),
    <span class="dt">legend.margin =</span> <span class="kw">margin</span>(<span class="dv">0</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">guides</span>(<span class="dt">color =</span> <span class="kw">guide_legend</span>(<span class="dt">title =</span> <span class="ot">NULL</span>, <span class="dt">override.aes =</span> <span class="kw">list</span>(<span class="dt">alpha =</span> <span class="dv">1</span>))) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">title =</span> <span class="st">&quot;Observed means and 100 simulations of new data&quot;</span>,
    <span class="dt">x =</span> <span class="st">&quot;Time after target onset [ms]&quot;</span>,
    <span class="dt">y =</span> <span class="st">&quot;Proportion looks to target&quot;</span>) </code></pre></div>
<p><img src="12-aim1-notebook_files/figure-html/posterior-lines-1.png" width="100%" /></p>
<p>Or we can plot the linear predictions. These are posterior predictions of the log-odds of looking to target before adding binomial noise.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lpred &lt;-<span class="st"> </span>d_m <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">sample_n_of</span>(<span class="dv">8</span>, ResearchID) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>tristan<span class="op">::</span><span class="kw">augment_posterior_linpred</span>(b, <span class="dt">newdata =</span> ., <span class="dt">nsamples =</span> <span class="dv">100</span>)

<span class="kw">ggplot</span>(lpred) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">aes</span>(<span class="dt">x =</span> Time, <span class="dt">y =</span> .posterior_value, <span class="dt">color =</span> Study) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">group =</span> <span class="kw">interaction</span>(Study, ResearchID, .draw)), 
            <span class="dt">alpha =</span> .<span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="st">&quot;ResearchID&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> <span class="kw">qlogis</span>(Prop)), <span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme</span>(
    <span class="dt">legend.position =</span> <span class="kw">c</span>(.<span class="dv">95</span>, <span class="dv">0</span>), 
    <span class="dt">legend.justification =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>),
    <span class="dt">legend.margin =</span> <span class="kw">margin</span>(<span class="dv">0</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">guides</span>(<span class="dt">color =</span> <span class="kw">guide_legend</span>(<span class="dt">title =</span> <span class="ot">NULL</span>, <span class="dt">override.aes =</span> <span class="kw">list</span>(<span class="dt">alpha =</span> <span class="dv">1</span>))) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">title =</span> <span class="st">&quot;Observed data and 100 posterior predictions&quot;</span>,
    <span class="dt">x =</span> <span class="st">&quot;Time after target onset [ms]&quot;</span>,
    <span class="dt">y =</span> <span class="st">&quot;Posterior log-odds&quot;</span>)</code></pre></div>
<p><img src="12-aim1-notebook_files/figure-html/posterior-mean-lines-1.png" width="100%" /></p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Mirman2014">
<p>Mirman, D. (2014). <em>Growth curve analysis and visualization using R</em>. Boca Raton, FL: Chapman &amp; Hall/CRC.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>It is tempting to further justify this approach by comparing Bayesian versus classical/frequentist statistics, but my goals in using this method are simple: To estimate statistical effects and quantify uncertainty about those effects. This pragmatic brand of Bayesian statistics is illustrated in texts by <span class="citation">Gelman &amp; Hill (<a href="#ref-GelmanHill">2007</a>)</span> and <span class="citation">McElreath (<a href="#ref-RethinkingBook">2016</a>)</span>.<a href="analysis-of-familiar-word-recognition.html#fnref1">↩</a></p></li>
<li id="fn2"><p>The polynomial other terms are less important—or rather, they have do not map as neatly onto behavioral descriptions as the accuracy and efficiency parameters. The primary purpose of quadratic and cubic terms is to ensure that the estimated growth curve adequately fits the data. In this kind of data, there is a steady baseline at chance probability before the child hears the word, followed a window of increasing probability of fixating on the target as the child recognizes the word, followed by a period of plateauing and then diminishing looks to target. The cubic polynomial allows the growth curve to be fit with two inflection points: the point when the looks to target start to increase from baseline and the point when the looks to target stops increasing.<a href="analysis-of-familiar-word-recognition.html#fnref2">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="aim1-method.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="visualize-looks-to-each-image-type.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["dissertation.pdf"],
"toc": {
"collapse": "subsection"
},
"split_bib": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
