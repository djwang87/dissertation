<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Development of word recognition in preschoolers</title>
  <meta name="description" content="Development of word recognition in preschoolers">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Development of word recognition in preschoolers" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="assets/cover.png" />
  
  <meta name="github-repo" content="tjmahr/dissertation" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Development of word recognition in preschoolers" />
  
  
  <meta name="twitter:image" content="assets/cover.png" />

<meta name="author" content="Tristan Mahr">


<meta name="date" content="2018-09-04">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="aim1-method.html">
<link rel="next" href="lex-competitors.html">
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="assets\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="dedication.html"><a href="dedication.html"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="overview-and-aims.html"><a href="overview-and-aims.html"><i class="fa fa-check"></i><b>1</b> Overview and aims</a><ul>
<li class="chapter" data-level="" data-path="overview-and-aims.html"><a href="overview-and-aims.html#study-1-familiar-word-recognition-and-lexical-competition"><i class="fa fa-check"></i>Study 1: Familiar word recognition and lexical competition</a></li>
<li class="chapter" data-level="" data-path="overview-and-aims.html"><a href="overview-and-aims.html#study-2-referent-selection-and-mispronunciations"><i class="fa fa-check"></i>Study 2: Referent selection and mispronunciations</a></li>
<li class="chapter" data-level="" data-path="overview-and-aims.html"><a href="overview-and-aims.html#summary"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="research-hypotheses.html"><a href="research-hypotheses.html"><i class="fa fa-check"></i><b>2</b> Research hypotheses</a><ul>
<li class="chapter" data-level="" data-path="research-hypotheses.html"><a href="research-hypotheses.html#study1-familiar-word-recognition-and-lexical-competition"><i class="fa fa-check"></i>Study 1: Familiar word recognition and lexical competition</a></li>
<li class="chapter" data-level="" data-path="research-hypotheses.html"><a href="research-hypotheses.html#study2-referent-selection-and-mispronunciations"><i class="fa fa-check"></i>Study 2: Referent selection and mispronunciations</a></li>
</ul></li>
<li class="part"><span><b>Study 1: Familiar word recognition and lexical competition</b></span></li>
<li class="chapter" data-level="3" data-path="aim1-introduction.html"><a href="aim1-introduction.html"><i class="fa fa-check"></i><b>3</b> Familiar word recognition</a><ul>
<li class="chapter" data-level="3.1" data-path="aim1-introduction.html"><a href="aim1-introduction.html#lexical-processing-dynamics"><i class="fa fa-check"></i><b>3.1</b> Lexical processing dynamics</a></li>
<li class="chapter" data-level="3.2" data-path="aim1-introduction.html"><a href="aim1-introduction.html#individual-differences-in-word-recognition"><i class="fa fa-check"></i><b>3.2</b> Individual differences in word recognition</a></li>
<li class="chapter" data-level="3.3" data-path="aim1-introduction.html"><a href="aim1-introduction.html#the-current-study"><i class="fa fa-check"></i><b>3.3</b> The current study</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="aim1-method.html"><a href="aim1-method.html"><i class="fa fa-check"></i><b>4</b> Method</a><ul>
<li class="chapter" data-level="4.1" data-path="aim1-method.html"><a href="aim1-method.html#aim1-participants"><i class="fa fa-check"></i><b>4.1</b> Participants</a></li>
<li class="chapter" data-level="4.2" data-path="aim1-method.html"><a href="aim1-method.html#aim1-procedure"><i class="fa fa-check"></i><b>4.2</b> Visual World Paradigm</a></li>
<li class="chapter" data-level="4.3" data-path="aim1-method.html"><a href="aim1-method.html#experiment-administration"><i class="fa fa-check"></i><b>4.3</b> Experiment administration</a></li>
<li class="chapter" data-level="4.4" data-path="aim1-method.html"><a href="aim1-method.html#stimuli"><i class="fa fa-check"></i><b>4.4</b> Stimuli</a></li>
<li class="chapter" data-level="4.5" data-path="aim1-method.html"><a href="aim1-method.html#data-screening"><i class="fa fa-check"></i><b>4.5</b> Data screening</a></li>
<li class="chapter" data-level="4.6" data-path="aim1-method.html"><a href="aim1-method.html#model-preparation"><i class="fa fa-check"></i><b>4.6</b> Model preparation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="fam-rec.html"><a href="fam-rec.html"><i class="fa fa-check"></i><b>5</b> Analysis of familiar word recognition</a><ul>
<li class="chapter" data-level="5.1" data-path="fam-rec.html"><a href="fam-rec.html#growth-curve-analysis"><i class="fa fa-check"></i><b>5.1</b> Growth curve analysis</a><ul>
<li class="chapter" data-level="5.1.1" data-path="fam-rec.html"><a href="fam-rec.html#growth-curve-features-as-measures-of-word-recognition-performance"><i class="fa fa-check"></i><b>5.1.1</b> Growth curve features as measures of word recognition performance</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="fam-rec.html"><a href="fam-rec.html#year-over-year-changes-in-word-recognition-performance"><i class="fa fa-check"></i><b>5.2</b> Year over year changes in word recognition performance</a></li>
<li class="chapter" data-level="5.3" data-path="fam-rec.html"><a href="fam-rec.html#exploring-plausible-ranges-of-performance-over-time"><i class="fa fa-check"></i><b>5.3</b> Exploring plausible ranges of performance over time</a></li>
<li class="chapter" data-level="5.4" data-path="fam-rec.html"><a href="fam-rec.html#are-individual-differences-stable-over-time"><i class="fa fa-check"></i><b>5.4</b> Are individual differences stable over time?</a></li>
<li class="chapter" data-level="5.5" data-path="fam-rec.html"><a href="fam-rec.html#predicting-future-vocabulary-size"><i class="fa fa-check"></i><b>5.5</b> Predicting future vocabulary size</a></li>
<li class="chapter" data-level="5.6" data-path="fam-rec.html"><a href="fam-rec.html#discussion"><i class="fa fa-check"></i><b>5.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lex-competitors.html"><a href="lex-competitors.html"><i class="fa fa-check"></i><b>6</b> Effects of phonological and semantic competitors</a><ul>
<li class="chapter" data-level="6.1" data-path="lex-competitors.html"><a href="lex-competitors.html#looks-to-the-phonological-competitor"><i class="fa fa-check"></i><b>6.1</b> Looks to the phonological competitor</a></li>
<li class="chapter" data-level="6.2" data-path="lex-competitors.html"><a href="lex-competitors.html#looks-to-the-semantic-competitor"><i class="fa fa-check"></i><b>6.2</b> Looks to the semantic competitor</a></li>
<li class="chapter" data-level="6.3" data-path="lex-competitors.html"><a href="lex-competitors.html#child-level-differences-in-competitor-sensitivity-at-age-3"><i class="fa fa-check"></i><b>6.3</b> Child-level differences in competitor sensitivity at age 3</a></li>
<li class="chapter" data-level="6.4" data-path="lex-competitors.html"><a href="lex-competitors.html#discussion-1"><i class="fa fa-check"></i><b>6.4</b> Discussion</a><ul>
<li class="chapter" data-level="6.4.1" data-path="lex-competitors.html"><a href="lex-competitors.html#immediate-activation-of-phonological-neighbors"><i class="fa fa-check"></i><b>6.4.1</b> Immediate activation of phonological neighbors</a></li>
<li class="chapter" data-level="6.4.2" data-path="lex-competitors.html"><a href="lex-competitors.html#late-activation-of-semantic-neighbors"><i class="fa fa-check"></i><b>6.4.2</b> Late activation of semantic neighbors</a></li>
<li class="chapter" data-level="6.4.3" data-path="lex-competitors.html"><a href="lex-competitors.html#lexical-competitors-and-child-level-predictors"><i class="fa fa-check"></i><b>6.4.3</b> Lexical competitors and child-level predictors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="aim1-discussion.html"><a href="aim1-discussion.html"><i class="fa fa-check"></i><b>7</b> General discussion</a><ul>
<li class="chapter" data-level="7.1" data-path="aim1-discussion.html"><a href="aim1-discussion.html#how-to-improve-word-recognition"><i class="fa fa-check"></i><b>7.1</b> How to improve word recognition</a></li>
<li class="chapter" data-level="7.2" data-path="aim1-discussion.html"><a href="aim1-discussion.html#learn-words-and-learn-connections-between-words"><i class="fa fa-check"></i><b>7.2</b> Learn words and learn connections between words</a></li>
<li class="chapter" data-level="7.3" data-path="aim1-discussion.html"><a href="aim1-discussion.html#individual-differences-are-most-important-at-younger-ages"><i class="fa fa-check"></i><b>7.3</b> Individual differences are most important at younger ages</a></li>
<li class="chapter" data-level="7.4" data-path="aim1-discussion.html"><a href="aim1-discussion.html#limitations-and-implications"><i class="fa fa-check"></i><b>7.4</b> Limitations and implications</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="aim1-h-check.html"><a href="aim1-h-check.html"><i class="fa fa-check"></i><b>8</b> Hypothesis check</a></li>
<li class="part"><span><b>Study 2: Referent selection and mispronunciations</b></span></li>
<li class="chapter" data-level="9" data-path="aim2-introduction.html"><a href="aim2-introduction.html"><i class="fa fa-check"></i><b>9</b> Mispronunciations and referent selection</a><ul>
<li class="chapter" data-level="9.1" data-path="aim2-introduction.html"><a href="aim2-introduction.html#how-phonetically-detailed-are-childrens-words"><i class="fa fa-check"></i><b>9.1</b> How phonetically detailed are children’s words?</a></li>
<li class="chapter" data-level="9.2" data-path="aim2-introduction.html"><a href="aim2-introduction.html#how-to-handle-nonwords"><i class="fa fa-check"></i><b>9.2</b> How to handle nonwords</a></li>
<li class="chapter" data-level="9.3" data-path="aim2-introduction.html"><a href="aim2-introduction.html#the-current-study-1"><i class="fa fa-check"></i><b>9.3</b> The current study</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="aim2-method.html"><a href="aim2-method.html"><i class="fa fa-check"></i><b>10</b> Method</a><ul>
<li class="chapter" data-level="10.1" data-path="aim2-method.html"><a href="aim2-method.html#mispronunciation-task"><i class="fa fa-check"></i><b>10.1</b> Mispronunciation task</a></li>
<li class="chapter" data-level="10.2" data-path="aim2-method.html"><a href="aim2-method.html#visual-stimuli"><i class="fa fa-check"></i><b>10.2</b> Visual stimuli</a></li>
<li class="chapter" data-level="10.3" data-path="aim2-method.html"><a href="aim2-method.html#novel-word-retention-tests"><i class="fa fa-check"></i><b>10.3</b> Novel word retention tests</a></li>
<li class="chapter" data-level="10.4" data-path="aim2-method.html"><a href="aim2-method.html#aim2-screening"><i class="fa fa-check"></i><b>10.4</b> Data screening</a><ul>
<li class="chapter" data-level="10.4.1" data-path="aim2-method.html"><a href="aim2-method.html#classifying-trials-based-on-initial-fixation-location"><i class="fa fa-check"></i><b>10.4.1</b> Classifying trials based on initial fixation location</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="aim2-method.html"><a href="aim2-method.html#model-preparation-1"><i class="fa fa-check"></i><b>10.5</b> Model preparation</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="real-nonword-selection.html"><a href="real-nonword-selection.html"><i class="fa fa-check"></i><b>11</b> Development of referent selection</a><ul>
<li class="chapter" data-level="11.1" data-path="real-nonword-selection.html"><a href="real-nonword-selection.html#nonwords-versus-familiar-words"><i class="fa fa-check"></i><b>11.1</b> Nonwords versus familiar words</a></li>
<li class="chapter" data-level="11.2" data-path="real-nonword-selection.html"><a href="real-nonword-selection.html#does-age-3-referent-selection-better-predict-age-5-vocabulary"><i class="fa fa-check"></i><b>11.2</b> Does age-3 referent selection better predict age-5 vocabulary?</a></li>
<li class="chapter" data-level="11.3" data-path="real-nonword-selection.html"><a href="real-nonword-selection.html#discussion-2"><i class="fa fa-check"></i><b>11.3</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html"><i class="fa fa-check"></i><b>12</b> Sensitivity to mispronunciations</a><ul>
<li class="chapter" data-level="12.1" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#unfamiliar-initial-trials-move-along-now"><i class="fa fa-check"></i><b>12.1</b> Unfamiliar-initial trials: Move along now</a><ul>
<li class="chapter" data-level="12.1.1" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#child-level-predictors"><i class="fa fa-check"></i><b>12.1.1</b> Child-level predictors</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#familiar-initial-trials-should-i-stay-or-should-i-gonow"><i class="fa fa-check"></i><b>12.2</b> Familiar-initial trials: Should I stay or should I go now?</a><ul>
<li class="chapter" data-level="12.2.1" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#child-level-predictors-and-different-listening-behaviors"><i class="fa fa-check"></i><b>12.2.1</b> Child-level predictors and different listening behaviors</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#looking-behaviors-and-word-learning"><i class="fa fa-check"></i><b>12.3</b> Looking behaviors and word learning</a></li>
<li class="chapter" data-level="12.4" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#discussion-3"><i class="fa fa-check"></i><b>12.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="aim2-discussion.html"><a href="aim2-discussion.html"><i class="fa fa-check"></i><b>13</b> General discussion</a><ul>
<li class="chapter" data-level="13.1" data-path="aim2-discussion.html"><a href="aim2-discussion.html#a-lexical-processing-account-of-the-results"><i class="fa fa-check"></i><b>13.1</b> A lexical processing account of the results</a></li>
<li class="chapter" data-level="13.2" data-path="aim2-discussion.html"><a href="aim2-discussion.html#a-nonword-is-just-a-word-you-havent-learned-yet"><i class="fa fa-check"></i><b>13.2</b> A nonword is just a word you haven’t learned yet</a></li>
<li class="chapter" data-level="13.3" data-path="aim2-discussion.html"><a href="aim2-discussion.html#limitations-and-implications-1"><i class="fa fa-check"></i><b>13.3</b> Limitations and implications</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="aim2-h-check.html"><a href="aim2-h-check.html"><i class="fa fa-check"></i><b>14</b> Hypothesis check</a></li>
<li class="part"><span><b>Overall discussion</b></span></li>
<li class="chapter" data-level="15" data-path="both-studies-discussion.html"><a href="both-studies-discussion.html"><i class="fa fa-check"></i><b>15</b> General discussion of both studies</a><ul>
<li class="chapter" data-level="15.1" data-path="both-studies-discussion.html"><a href="both-studies-discussion.html#mechanisms-of-word-recognition"><i class="fa fa-check"></i><b>15.1</b> Mechanisms of word recognition</a><ul>
<li class="chapter" data-level="15.1.1" data-path="both-studies-discussion.html"><a href="both-studies-discussion.html#open-questions-about-word-recognition-mechanisms"><i class="fa fa-check"></i><b>15.1.1</b> Open questions about word recognition mechanisms</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="both-studies-discussion.html"><a href="both-studies-discussion.html#clinical-implications"><i class="fa fa-check"></i><b>15.2</b> Clinical implications</a></li>
<li class="chapter" data-level="15.3" data-path="both-studies-discussion.html"><a href="both-studies-discussion.html#contributions"><i class="fa fa-check"></i><b>15.3</b> Contributions</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="vw-experiment-items.html"><a href="vw-experiment-items.html"><i class="fa fa-check"></i><b>A</b> Items used in Study 1</a></li>
<li class="chapter" data-level="B" data-path="aim1-gca-models.html"><a href="aim1-gca-models.html"><i class="fa fa-check"></i><b>B</b> Computational details for Study 1</a><ul>
<li class="chapter" data-level="B.1" data-path="aim1-gca-models.html"><a href="aim1-gca-models.html#growth-curve-analyses"><i class="fa fa-check"></i><b>B.1</b> Growth curve analyses</a><ul>
<li class="chapter" data-level="B.1.1" data-path="aim1-gca-models.html"><a href="aim1-gca-models.html#convergence-diagnostics-for-bayesian-models"><i class="fa fa-check"></i><b>B.1.1</b> Convergence diagnostics for Bayesian models</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="aim1-gca-models.html"><a href="aim1-gca-models.html#generalized-additive-models"><i class="fa fa-check"></i><b>B.2</b> Generalized additive models</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="mp-experiment-items.html"><a href="mp-experiment-items.html"><i class="fa fa-check"></i><b>C</b> Items used in Study 2</a></li>
<li class="chapter" data-level="D" data-path="aim2-gca-models.html"><a href="aim2-gca-models.html"><i class="fa fa-check"></i><b>D</b> Computational details for Study 2</a><ul>
<li class="chapter" data-level="D.1" data-path="aim2-gca-models.html"><a href="aim2-gca-models.html#real-words-versus-nonwords-growth-curves"><i class="fa fa-check"></i><b>D.1</b> Real words versus nonwords growth curves</a></li>
<li class="chapter" data-level="D.2" data-path="aim2-gca-models.html"><a href="aim2-gca-models.html#mispronunciation-growth-curves"><i class="fa fa-check"></i><b>D.2</b> Mispronunciation growth curves</a></li>
<li class="chapter" data-level="D.3" data-path="aim2-gca-models.html"><a href="aim2-gca-models.html#item-response-analysis-for-novel-word-retention"><i class="fa fa-check"></i><b>D.3</b> Item-response analysis for novel word retention</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="aim2-mp-items.html"><a href="aim2-mp-items.html"><i class="fa fa-check"></i><b>E</b> Effects of specific mispronunciations in Study 2</a></li>
<li class="chapter" data-level="F" data-path="related-work.html"><a href="related-work.html"><i class="fa fa-check"></i><b>F</b> Related work</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Created with bookdown</a></li>
<li><a href="https://tjmahr.github.io/" target="blank">Tristan Mahr</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Development of word recognition in preschoolers</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="fam-rec" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Analysis of familiar word recognition</h1>
<div id="growth-curve-analysis" class="section level2">
<h2><span class="header-section-number">5.1</span> Growth curve analysis</h2>
<p>The outcome measure of interest here is how the probability of fixating on the target image versus the distractors changes over time. There are many possible techniques one can employ for modeling time series data. In this chapter, I used growth curve analysis which uses polynomial functions of time (a linear trend, a quadratic trend, etc.) to estimate a time series. <span class="citation">Barr (<a href="#ref-Barr2008">2008</a>)</span> and <span class="citation">Mirman, Dixon, and Magnuson (<a href="#ref-Mirman2008">2008</a>)</span> are important early tutorials for this technique of modeling looking probabilities. (Incidentally, the two articles were published together in a special issue of <em>Journal of Memory and Language</em> about “emerging” statistical techniques.) <span class="citation">Mirman (<a href="#ref-Mirman2014">2014</a>)</span> also provides a textbook treatment of growth curve analysis for eyetracking data. This approach is well suited for time series where the trajectory is relatively simple with one or two inflection points. Alternatively, one can forgo polynomial trends and use generalized additive (mixed) models to fit a more general nonlinear shape. I apply this now-emerging technique in <a href="lex-competitors.html#lex-competitors">Chapter <a href="lex-competitors.html#lex-competitors">6</a></a> to model wigglier growth curve shapes. A third possibility is to use nonlinear, functional growth curves. For the polynomial and additive models, underlying time features are weighted and summed to fit a nonlinear shape. For the functional growth curve, the nonlinear shape is fixed in advance and the model estimates a set of curve parameters so the shape approximates the data. For example, <span class="citation">Oleson, Cavanaugh, McMurray, and Brown (<a href="#ref-bdots17">2017</a>)</span> and <span class="citation">Seedorff, Oleson, and McMurray (<a href="#ref-bdots18">2018</a>)</span> model eyetracking data by assuming an s-shaped curve (a logistic function) and then estimating the left and right asymptotes, the slope at the steepest point, and the point where the steepest rise occurs. These parameters are directly interpretable in terms of looking behaviors, but I have found that the technique is not flexible enough to handle the noisier shapes of children’s eyetracking data.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> For the following analyses, therefore, I used polynomial growth curves.</p>
<p>Looks to the familiar image were analyzed using Bayesian, mixed effects logistic regression. I used <em>logistic</em> regression because the outcome measurement is a probability (the log-odds of looking to the target image versus the distractors). I used <em>mixed-effects</em> models to estimate a separate growth curve for each child (to measure individual differences in word recognition) but also treat each child’s individual growth curve as a draw from a distribution of related curves. I used <em>Bayesian</em> techniques to study a generative model of the data. Instead of reporting and describing a single, best fit of some data, Bayesian methods consider an entire distribution of plausible fits that are consistent with the data and any prior information we have about the model parameters. By using this approach, one can explicitly quantify uncertainty about statistical effects and draw inferences using estimates of uncertainty (instead of using statistical significance—which is not a straightforward matter for mixed-effects models).<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<p>Word recognition growth curves—that is, looks to the target versus the distractors at 250 ms, 300 ms, etc.—were fit using an orthogonal cubic polynomial function of time. Put differently, I modeled the probability of looking to the target during an eyetracking task as:</p>
<p><span class="math display">\[
\text{log\,odds}(\text{looking}) = 
  \beta_0 + 
  \beta_1\text{Time}^1 + 
  \beta_2\text{Time}^2 + 
  \beta_3\text{Time}^3
\]</span></p>
<p>That the time terms are <em>orthogonal</em> means that <span class="math inline">\(\text{Time}^1\)</span>, <span class="math inline">\(\text{Time}^2\)</span> and <span class="math inline">\(\text{Time}^3\)</span> are transformed so that they are uncorrelated. See Box 1. Under this formulation, the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> have a direct interpretation in terms of lexical processing performance. The intercept, <span class="math inline">\(\beta_0\)</span>, measures the area under the growth curve—or the probability of fixating on the target word averaged over the whole window. We can think of <span class="math inline">\(\beta_0\)</span> as a measure of <em>word recognition reliability</em>. The linear time parameter, <span class="math inline">\(\beta_1\)</span>, estimates the steepness of the growth curve—or how the probability of fixating changes from frame to frame. We can think of <span class="math inline">\(\beta_1\)</span> as a measure of <em>processing efficiency</em>, because growth curves with stronger linear features exhibit steeper frame-by-frame increases in looking probability.</p>

<div class="infobox">
<p><strong>Box 1: Orthogonal time</strong>.</p>
<p>I used orthogonal polynomial features of Time for these growth curve models. Unlike natural polynomials, these features are uncorrelated. This aspect makes these models more flexible: I do not have to worry about any collinearity between Time<sup>1</sup> and Time<sup>2</sup>. Moreover, adding an orthogonal cubic Time<sup>3</sup> term to a quadratic model will not change any of the estimates for Time<sup>1</sup> or Time<sup>2</sup> because the added predictor is not correlated with the others. One disadvantage of this approach is that the features are not as straightforward to interpret.</p>
<p>The figure below shows the orthogonal polynomials used by the model and how they can be weighted and summed to fit a growth curve.</p>
<p><img src="14-aim1-familiar-word-recognition_files/figure-html/infobox-ot-figs-1.png" width="66%" style="display: block; margin: auto;" /></p>
<p>Note that the time features and weighted features are vertically centered around 0. The curves are adjusted up or down to their correct position by the model’s intercept term. Conceptually, one can also think of the intercept as a Time<sup>0</sup> feature—that is, a horizontal line at <em>y</em> = 1 which is weighted to move the whole curve vertically. This is why in these models, the intercept is not the value at some time 0 but rather the average value of the fitted growth curve. To reiterate, for these word recognition models, the intercept is the average probability of the curve.</p>
<p>For all the polynomial growth curves models I used in this project, I scaled the features so that Time<sup>1</sup> ranges from −.5 to .5. In other words, a 1-unit change on Time<sup>1</sup> marks the whole traversal across the analysis window. After scaling, Time<sup>2</sup> ranges from −.33 to .60 and Time<sup>3</sup> ranges from −.63 to .63.</p>
<p>In my experience, only the intercept terms and linear time trends of an orthogonal polynomial model have a behaviorally straightforward interpretation. The polynomial other terms are less important—or rather, they do not map as neatly onto behavioral descriptions as the accuracy and efficiency parameters. The primary purpose of quadratic and cubic terms is to ensure that the estimated growth curve adequately fits the data. In this kind of data, there is a steady baseline at chance probability before the child hears the word, followed a window of increasing probability of fixating on the target as the child recognizes the word, followed by a period of plateauing and then diminishing looks to target. The cubic polynomial allows the growth curve to be fit with two inflection points: the point when the looks to target start to increase from baseline and the point when the looks to target stops increasing.</p>
</div>

<p>To study how word recognition changes over time, I modeled how the growth curves change over developmental time. This amounted to studying how the growth curve parameters changes year over year. I included dummy-coded indicators for age 3, age 4, and age 5 and allowed these indicators to interact with the growth curve parameters:</p>
<p><span class="math display">\[
\small
\begin{align*}
  \text{log\,odds}(\text{looking}) =\
    &amp;\beta_0 + 
      \beta_1\text{Time}^1 + 
      \beta_2\text{Time}^2 + 
      \beta_3\text{Time}^3\ + 
      &amp;\text{[age 3 growth curve]} \\
    (&amp;\gamma_{0} + 
      \gamma_{1}\text{Time}^1 + 
      \gamma_{2}\text{Time}^2 +
      \gamma_{3}\text{Time}^3)*\text{Age\,4}\ + \
      &amp;\text{[age 4 adjustments]} \\
    (&amp;\delta_{0} + 
      \delta_{1}\text{Time}^1 + 
      \delta_{2}\text{Time}^2 +
      \delta_{3}\text{Time}^3)*\text{Age\,5} \
      &amp;\text{[age 5 adjustments]} \\
\end{align*}
\normalsize
\]</span></p>
<p>These year-by-growth-curve-feature terms captured how the shape of the growth curves changed each year. The model also included random effects to represent by-child and by-child-by-year effects to estimate a general growth curve for each child and to estimate how each child’s growth curve changed each year.</p>
<p>The models were fit in R (vers. 3.4.3) with the RStanARM package (vers. 2.16.3). <a href="aim1-gca-models.html#aim1-gca-models">Appendix <a href="aim1-gca-models.html#aim1-gca-models">B</a></a> contains the R code used to fit the model along with a description of the model specifications represented in the model syntax.</p>
<p>I used Bayesian <em>uncertainty intervals</em> to draw statistical inferences. A Bayesian model is the posterior distribution: It is a distribution of plausible parameter values, given the data, a data-generating model and any prior information we have about those parameter values. In practice, these distributions are hard to calculate, so we use Markov Chain Monte Carlo to get a sample of thousands of values from the posterior distribution. Thus, rather than having a single best-fitting estimate of some effect <span class="math inline">\(\beta\)</span>, we have a sample of, say, 4,000 plausible values for <span class="math inline">\(\beta\)</span>. We can quantify our uncertainty about <span class="math inline">\(\beta\)</span> by describing the distribution of those values. I use typically two statistics to describe that distribution. The median provides a <em>point estimate</em> for the distribution, and the 90% uncertainty interval provides bounds for the effect. These intervals have an intuitive interpretation. Suppose that for <span class="math inline">\(\beta\)</span> we get a median of 8 and 90% uncertainty interval of [5, 21]. This interval means that we can be 90% certain that the “true” value of <span class="math inline">\(\beta\)</span> is between 5 and 21, given the data, our model, and our prior information. Moreover, by inspecting the interval, we pinpoint areas of uncertainty. In this example, we can conclude that the effect is likely to be positive. The lower interval value of 5 tells us that 90% of the plausible values are greater than 5. A wide range of values are covered by the interval, however, so we would also conclude that we are not very certain about the size of the effect. It bears noting that one cannot interpret frequentist confidence intervals in this way. See <span class="citation">Kruschke and Liddell (<a href="#ref-Kruschke2017">2017</a>)</span> for a recent review of frequentist versus Bayesian statistics.</p>
<div id="growth-curve-features-as-measures-of-word-recognition-performance" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Growth curve features as measures of word recognition performance</h3>
<p>As mentioned above, two of the model’s growth curve features have straightforward interpretations in terms of lexical processing performance: The model’s intercept parameter corresponds to the average proportion or probability of looking to the named image over the trial window, and the linear time parameter corresponds to slope of the growth curve or lexical processing efficiency. I also was interested in <em>peak</em> proportion of looks to the target. I derived this value by computing the growth curves from the model and taking the median of the five highest points on the curve. Figure <a href="fam-rec.html#fig:curve-features">5.1</a> shows three simulated growth curves and how each of these growth curve features relate to word recognition performance.</p>


<div class="figure" style="text-align: center"><span id="fig:curve-features"></span>
<img src="14-aim1-familiar-word-recognition_files/figure-html/curve-features-1.png" alt="Illustration of the three growth curve features and how they describe lexical processing performance. The three curves used are simulations of new participants at age 4." width="80%" />
<p class="caption">
Figure 5.1: Illustration of the three growth curve features and how they describe lexical processing performance. The three curves used are simulations of new participants at age 4.
</p>
</div>
</div>
</div>
<div id="year-over-year-changes-in-word-recognition-performance" class="section level2">
<h2><span class="header-section-number">5.2</span> Year over year changes in word recognition performance</h2>
<p>The mixed-effects model estimated a population-average growth curve (“fixed” effects) and how individual children deviated from the average (“random” effects). Figure <a href="fam-rec.html#fig:average-growth-curves">5.2</a> shows 200 posterior samples of the population-average growth curves for each year. On average, the growth curves become steeper and achieve higher looking probabilities with each year of the study.</p>


<div class="figure" style="text-align: center"><span id="fig:average-growth-curves"></span>
<img src="14-aim1-familiar-word-recognition_files/figure-html/average-growth-curves-1.png" alt="Population-average (“fixed effects”) word recognition growth curves at each age. Colored lines represent 200 posterior samples of these growth curves; these are included to visualize the uncertainty about the population averages. The thick light lines represent the observed average growth curve at each age." width="50%" />
<p class="caption">
Figure 5.2: Population-average (“fixed effects”) word recognition growth curves at each age. Colored lines represent 200 posterior samples of these growth curves; these are included to visualize the uncertainty about the population averages. The thick light lines represent the observed average growth curve at each age.
</p>
</div>
<p>Figure <a href="fam-rec.html#fig:effects2">5.3</a> depicts uncertainty intervals with the model’s average effects of each timepoint on the growth curve features. The intercept and linear time effects increased each year, confirming that children become more reliable and faster at recognizing words as they grow older. The peak probability also increased each year. For each effect, the change from age 3 to age 4 is approximately the same as the change from age 4 to age 5, as illustrated in Figure <a href="fam-rec.html#fig:pairwise-effects">5.4</a>.</p>


<div class="figure" style="text-align: center"><span id="fig:effects2"></span>
<img src="14-aim1-familiar-word-recognition_files/figure-html/effects2-1.png" alt="Uncertainty intervals for growth curve features at each age. The intercept and peak features were converted from log-odds to proportions to ease interpretation." width="100%" />
<p class="caption">
Figure 5.3: Uncertainty intervals for growth curve features at each age. The intercept and peak features were converted from log-odds to proportions to ease interpretation.
</p>
</div>


<div class="figure" style="text-align: center"><span id="fig:pairwise-effects"></span>
<img src="14-aim1-familiar-word-recognition_files/figure-html/pairwise-effects-1.png" alt="Uncertainty intervals for the differences in growth curve features between ages. Again, the intercept and peak features were converted to proportions." width="100%" />
<p class="caption">
Figure 5.4: Uncertainty intervals for the differences in growth curve features between ages. Again, the intercept and peak features were converted to proportions.
</p>
</div>
<p>The average looking probability (intercept feature) was 0.38 [90% UI: 0.37, 0.40] at age 3, 0.49 [0.47, 0.50] at age 4, and 0.56 [0.54, 0.57] at age 5. The averages increased by 0.10 [0.09, 0.11] from age 3 to age 4 and by 0.07 [0.06, 0.09] from age 4 to age 5. The peak looking probability was 0.55 [0.53, 0.57] at age 3, 0.68 [0.67, 0.70] at age 4, and 0.77 [0.76, 0.78] at age 5. The peak values increased by 0.13 [0.11, 0.16] from age 3 to age 4 and by 0.09 [0.07, 0.10] from age 4 to age 5. These results numerically confirm the hypothesis that children would improve in their word recognition reliability, both in terms of average looking and in terms of peak looking, each year. The changes in peak probability were also rather large: children’s probability fixating on the target increased by approximately .1 each year. These growths indicate the task scaled with children’s development because they had room to improve each year.</p>
<p><strong>Summary</strong>. The average growth curve features increased year over year, so that children looked to the target more quickly and more reliably as they grew older.</p>
</div>
<div id="exploring-plausible-ranges-of-performance-over-time" class="section level2">
<h2><span class="header-section-number">5.3</span> Exploring plausible ranges of performance over time</h2>
<p>Bayesian models are generative; they describe how the data could have been generated. This model assumed that each child’s growth curve was drawn from a population of related growth curves, and it tried to infer the parameters over that distribution. These two aspects—a generative model and learning about the population of growth curves—allow the model to simulate new samples from that distribution of growth curves. That is, we can predict a set of growth curves for a hypothetical, unobserved child drawn from the same distribution as the 195 children observed in this study. This procedure of studying model implications by having the model generate new data is called <em>posterior predictive inference</em>, and in this case, it allows one to explore the plausible degrees of variability in performance at each age.</p>
<p>Figure <a href="fam-rec.html#fig:new-participants">5.5</a> shows the posterior predictions for 1,000 simulated participants, and it demonstrates how the model expects new participants to improve longitudinally but also exhibit stable individual features over time. Figure <a href="fam-rec.html#fig:new-participants-intervals">5.6</a> shows uncertainty intervals for these simulations. The model learned to predict less accurate and more variable performance at age 3 with improving accuracy and narrowing variability at age 4 and age 5.</p>


<div class="figure" style="text-align: center"><span id="fig:new-participants"></span>
<img src="14-aim1-familiar-word-recognition_files/figure-html/new-participants-1.png" alt="Posterior predictions for hypothetical unobserved participants. Each line represents the predicted performance for a new participant. The three light lines highlight predictions from one single simulated participant. The simulated participant shows both longitudinal improvement in word recognition and similar relative performance compared to other simulations each year, indicating that the model would predict new children to improve year over year and show stable individual differences over time." width="80%" />
<p class="caption">
Figure 5.5: Posterior predictions for hypothetical <em>unobserved</em> participants. Each line represents the predicted performance for a new participant. The three light lines highlight predictions from one single simulated participant. The simulated participant shows both longitudinal improvement in word recognition and similar relative performance compared to other simulations each year, indicating that the model would predict new children to improve year over year and show stable individual differences over time.
</p>
</div>


<div class="figure" style="text-align: center"><span id="fig:new-participants-intervals"></span>
<img src="14-aim1-familiar-word-recognition_files/figure-html/new-participants-intervals-1.png" alt="Uncertainty intervals for the simulated participants. Variability is widest at age 3 and narrowest at age 5, consistent with the prediction that children become less variable as they grow older." width="80%" />
<p class="caption">
Figure 5.6: Uncertainty intervals for the simulated participants. Variability is widest at age 3 and narrowest at age 5, consistent with the prediction that children become less variable as they grow older.
</p>
</div>
<p>I hypothesized that children would become less variable as they grew older and converged on a mature level of performance. To address this question, I inspected the ranges of predictions for the simulated participants. The claim that children become less variable would imply that the range of predictions should be narrower for age 5 than for age 4 and narrower for age 4 than for age 3. Figure <a href="fam-rec.html#fig:new-ranges">5.7</a> depicts the range of the predictions, both in terms of the 90-percentile range (i.e., the range of the middle 90% of the data) and in terms of the 50-percentile (interquartile) range. The ranges of performance decrease from age 3 to age 4 to age 5, consistent with the hypothesized reduction in variability.</p>


<div class="figure" style="text-align: center"><span id="fig:new-ranges"></span>
<img src="14-aim1-familiar-word-recognition_files/figure-html/new-ranges-1.png" alt="Ranges of predictions for simulated participants over the course of a trial. The ranges are most similar during the first half of the trial when participants are at chance performance, and the ranges are most different at the end of the trial as children reliably fixate on the target image. The ranges of performance decrease with each year of the study as children show less variability." width="80%" />
<p class="caption">
Figure 5.7: Ranges of predictions for simulated participants over the course of a trial. The ranges are most similar during the first half of the trial when participants are at chance performance, and the ranges are most different at the end of the trial as children reliably fixate on the target image. The ranges of performance decrease with each year of the study as children show less variability.
</p>
</div>
<p>The developmental pattern of increasing reliability and decreasing variability was also observed for the growth curve peaks. For the synthetic participants, the model predicted that individual peak probabilities will increase each year, peak<sub>3</sub> = 0.55 [90% UI: 0.35, 0.77], peak<sub>4</sub> = 0.69 [0.48, 0.86], peak<sub>5</sub> = 0.78 [0.59, 0.91]. Moreover, the range of plausible values for the individual peaks narrowed each for the simulated data. For instance, the difference between the 95<sup>th</sup> and 5<sup>th</sup> percentiles was 0.43 for age 3, 0.38 for age 4, and 0.32 for age 5.</p>
<p><strong>Summary</strong>. I used the model’s random effects estimates to simulate growth curves from 1,000 hypothetical, unobserved participants. The simulated dataset showed increasing looking probability and decreasing variability with each year of the study. These simulations confirmed the hypothesis that variability would diminish as children began to demonstrate a mature degree of performance for this task.</p>
</div>
<div id="are-individual-differences-stable-over-time" class="section level2">
<h2><span class="header-section-number">5.4</span> Are individual differences stable over time?</h2>
<p>I predicted that children would show stable individual differences such that children who are faster and more reliable at recognizing words at age 3 remain relatively faster and more reliable at age 5. To evaluate this hypothesis, I used Kendall’s <em>W</em> (the coefficient of correspondence or concordance). This nonparametric statistic measures the degree of agreement among <em>J</em> judges who are rating <em>I</em> items. For these purposes, the items are the 123 children who provided reliable eyetracking for all three years of the study. (That is, I excluded children who only had reliable eyetracking data for one or two years.) The judges are the sets of growth curve parameters from each year of study. For example, the intercept term provides three sets of ratings: The participants’ intercept terms from age 3 are one set of ratings and the terms from ages 4 and 5 provide two more sets of ratings. These three ratings are the “judges” used to compute the intercept’s <em>W</em>. Thus, I computed five groups of <em>W</em> coefficients, one for each set of growth curve features: Time<sup>1</sup>, Time<sup>2</sup>, Time<sup>3</sup>, average looking probability, and peak looking probability.</p>
<p>Because I used a Bayesian model, there is a distribution of ratings and thus a distribution of concordance statistics. Each sample of the posterior distribution fits a growth curve for each child in each year, so each posterior sample provides a set of ratings for concordance coefficients. This distribution of <em>W</em>’s lets us quantify our uncertainty because we can compute <em>W</em>’s for each of the 4000 samples from the posterior distribution.</p>
<p>One final matter is how to assess whether a concordance statistic is meaningful. To tackle this question, I also included a “null rater”, a fake parameter that assigned each child in each year a random number. I use the distribution of <em>W</em>’s generated by randomly rating children as a benchmark for assessing whether the other concordance statistics differ meaningfully from chance.</p>


<div class="figure" style="text-align: center"><span id="fig:kendall-stats"></span>
<img src="14-aim1-familiar-word-recognition_files/figure-html/kendall-stats-1.png" alt="Uncertainty intervals for the Kendall’s coefficient of concordance. Random ratings provide a baseline of null W statistics. The peak, intercept and linear time features are decisively non-null, indicating a significant degree of correspondence in children’s relative word recognition reliability and efficiency over the three years of the study." width="80%" />
<p class="caption">
Figure 5.8: Uncertainty intervals for the Kendall’s coefficient of concordance. Random ratings provide a baseline of null <em>W</em> statistics. The peak, intercept and linear time features are decisively non-null, indicating a significant degree of correspondence in children’s relative word recognition reliability and efficiency over the three years of the study.
</p>
</div>
<p>I used the <code>kendall()</code> function in the irr R package <span class="citation">(vers. 0.84; Gamer, Lemon, &amp; Singh, <a href="#ref-irr">2012</a>)</span> to compute concordance statistics. Figure <a href="fam-rec.html#fig:kendall-stats">5.8</a> depicts uncertainty intervals for the Kendall <em>W</em>’s for these growth curve features. The 90% uncertainty interval of <em>W</em> statistics from random ratings, [.28, .39], subsumes the intervals for the Time<sup>2</sup> effect [.30, .35] and the Time<sup>3</sup> effect [.28, .35], indicating that these values do not differentiate children in a longitudinally stable way. Earlier, I claimed that only the intercept, linear time, and peak features have psychologically meaningful interpretations and that the higher-order time features mainly act to capture the curvature of the data. These null concordance statistics support that claim because the Time<sup>2</sup> and Time<sup>3</sup> features differentiate children across years as well as random numbers.</p>
<p>Concordance is strongest for the peak feature, <em>W</em> = .59 [.57, .60] and the intercept term, <em>W</em> = .58 [.57, .60], followed by the linear time term, <em>W</em> = .50 [.48, .52]. Because these values are far removed from the statistics for random ratings, I conclude that there is a credible degree of correspondence across years when ranking children using their peak looking probability, average look probability (the intercept) or their growth curve slope (linear time).</p>
<p><strong>Summary</strong>. Growth curve features measured individual differences in word recognition performance. By using Kendall’s <em>W</em> to measure the degree of concordance among growth curve features over time, I tested whether individual differences in lexical processing persisted over development. I found that the peak looking probability, average looking probability and linear time features were stable over time. Children who were relatively fast (or reliable) at word recognition at one age were also relatively fast (or reliable) at other ages too.</p>
</div>
<div id="predicting-future-vocabulary-size" class="section level2">
<h2><span class="header-section-number">5.5</span> Predicting future vocabulary size</h2>
<p>I hypothesized that individual differences in word recognition at age 3 will be more discriminating and predictive of future language outcomes than differences at age 4 or age 5. To test this hypothesis, I calculated the correlations of growth curve features with age 5 expressive vocabulary size and age 4 receptive vocabulary. (The receptive test was not administered during the last year of the study for logistical reasons.) As with the concordance analysis, I computed each of the correlations for each sample of the posterior distribution to obtain a distribution of correlations.</p>
<p>Figure <a href="fam-rec.html#fig:evt2-gca-cors">5.9</a> shows the correlations of the peak looking probability, average looking probability and linear time features with expressive vocabulary size at age 5, and Figure <a href="fam-rec.html#fig:ppvt4-gca-cors">5.10</a> shows analogous correlations for the receptive vocabulary at age 4. For all cases, the strongest correlations were found between the growth curve features at age 3.</p>
<p>Growth curve peaks from age 3 correlated with age 5 vocabulary with <em>r</em> = .52 [90% UI .50, .54], but the concurrent peaks from age 5 showed a correlation of just <em>r</em> = .31 [.29, .33], a difference between age-3 and age-5 correlations of <em>r</em><sub>3−5</sub> = .21 [.18, .24]. A similar pattern held for lexical processing efficiency values. Linear time features from age 3 correlated with age 5 vocabulary with <em>r</em> = .41 [.39, .44], whereas the concurrent lexical processing values from age 5 only showed a correlation of <em>r</em> = .28 [.26, .31], a difference of <em>r</em><sub>3−5</sub> = .13 [.10, .16]. For the average looking probabilities, the correlation for age 3, <em>r</em> = .39 [.39, .44], was probably only slightly greater than the correlation for age 4, <em>r</em><sub>3−4</sub> = .02 [−.01, .04] but considerably greater than the concurrent correlation at age 5, <em>r</em><sub>3−5</sub> = .08 [.05, .10].</p>


<div class="figure" style="text-align: center"><span id="fig:evt2-gca-cors"></span>
<img src="14-aim1-familiar-word-recognition_files/figure-html/evt2-gca-cors-1.png" alt="Uncertainty intervals for the correlations of growth curve features at each age with age-5 expressive vocabulary (EVT-2 standard scores). The bottom rows provide intervals for the pairwise differences in correlations between timepoints. For example, the top row of the left panel is the correlation between age-3 peak probability and age-5 expressive vocabulary." width="80%" />
<p class="caption">
Figure 5.9: Uncertainty intervals for the correlations of growth curve features at each age with age-5 expressive vocabulary (EVT-2 standard scores). The bottom rows provide intervals for the pairwise differences in correlations between timepoints. For example, the top row of the left panel is the correlation between age-3 peak probability and age-5 expressive vocabulary.
</p>
</div>
<p>Peak looking probabilities from age 3 were strongly correlated with age 4 receptive vocabulary, <em>r</em> = .62 [.61, .64], and this correlation was much greater than the correlation observed for the age 4 growth curve peaks, <em>r</em><sub>3−4</sub> = .26 [.23, .29]. The correlation for age 3 average looking probabilities, <em>r</em> = .45 [.44, .47], was greater than the age 4 correlation, <em>r</em><sub>3−4</sub> = .08 [.06, .11], and the correlation for age 3 linear time features, <em>r</em> = .51 [.49, .54], was likewise greater, <em>r</em><sub>3−4</sub> = .22 [.19, .26].</p>


<div class="figure" style="text-align: center"><span id="fig:ppvt4-gca-cors"></span>
<img src="14-aim1-familiar-word-recognition_files/figure-html/ppvt4-gca-cors-1.png" alt="Uncertainty intervals for the correlations of growth curve features from age 3 and age 4 with age-4 receptive vocabulary (PPVT-4 standard scores). The bottom row shows pairwise differences between the age-3 and age-4 correlations." width="80%" />
<p class="caption">
Figure 5.10: Uncertainty intervals for the correlations of growth curve features from age 3 and age 4 with age-4 receptive vocabulary (PPVT-4 standard scores). The bottom row shows pairwise differences between the age-3 and age-4 correlations.
</p>
</div>
<p><strong>Summary</strong>. Although individual differences in word recognition were stable over time, early differences were more significant than later ones. The strongest predictors of future vocabulary size were the growth curve features from age 3. Of these features, correlations were strongest for peak looking probabilities.</p>
</div>
<div id="discussion" class="section level2">
<h2><span class="header-section-number">5.6</span> Discussion</h2>
<p>In the preceding analyses, I examined many aspects of children’s recognition of familiar words. First, I modeled how children’s looking patterns <em>on average</em> changed year over year. Children’s word recognition improved each year: The growth curves grew steeper, reached higher peaks, and increased in their overall average value each year. This result was unsurprising, but it was valuable because it confirmed that this word recognition task scaled with development. The task was simple enough that children could recognize words at age 3 but challenging enough for children’s performance to improve each year.</p>
<p>After establishing how the averages changed each year, I next asked how variability changed each year. To tackle this question, I used posterior predictive inference to have the model simulate samples of data, and in particular, to simulate new participants. The range of performance narrowed each year, so that children were most variable at age 3 and least variable at age 5. This result is consistent with a model of development where children vary widely early on and converge on a more mature level of performance. From this perspective, word recognition is a skill where children “grow out” of immature and highly variable performance patterns. An alternative outcome would have been concerning: Word recognition differences that expanded with age with some children falling behind their peers.</p>
<p>Although the range of individual differences decreased with age, individual differences did not disappear over time. When children at each age were ranked using growth curve features, I found a high degree of correspondence among these ratings. Children who were faster or more accurate at age 3 remained relatively fast or accurate at age 5. Thus, differences in word recognition were longitudinally stable over the preschool years. Extrapolating forwards in time, these differences likely become smaller and smaller and become irrelevant for everyday listening situations. It is plausible, however, that under adverse listening conditions, individual differences might re-emerge and differentiate children’s word recognition performance.</p>
<p>Lastly, I analyzed how individual differences in word recognition features correlated with future vocabulary outcomes. The peak looking probabilities and growth curve slopes from age 3 showed the strongest correlations with future vocabulary scores. This finding was remarkable: Expressive vocabulary scores at age 5, for example, were more strongly correlated with word recognition data collected two years earlier than word recognition data collected during the same week.</p>
<p>We can understand the predictive value of age-3 word recognition performance from two perspectives. The first interpretation is statistical. Differences in children’s word recognition performance were greatest at age 3, so word recognition features at age 3 provide more variance and more information about the children and their future vocabulary size. The second interpretation is conceptual. Correlations were strongest for the growth curve peaks. We can think of this feature as measuring children’s maximum word recognition certainty. A child with a peak of .5, for example, looked the target image half of the time when they were most certain about the word. Although all of the words used were familiar to preschoolers, children with higher peaks knew those words <em>better</em>. These children had a stronger foundation for word-learning than children who show more uncertainty during word recognition, and as a result, these children had developed larger vocabularies two years later.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Barr2008">
<p>Barr, D. J. (2008). Analyzing ‘visual world’ eyetracking data using multilevel logistic regression. <em>Journal of Memory and Language</em>, <em>59</em>(4), 457–474. doi:<a href="https://doi.org/10.1016/j.jml.2007.09.002">10.1016/j.jml.2007.09.002</a></p>
</div>
<div id="ref-Mirman2008">
<p>Mirman, D., Dixon, J. A., &amp; Magnuson, J. S. (2008). Statistical and computational models of the visual world paradigm: Growth curves and individual differences. <em>Journal of Memory and Language</em>, <em>59</em>(4), 475–494. doi:<a href="https://doi.org/10.1016/j.jml.2007.11.006">10.1016/j.jml.2007.11.006</a></p>
</div>
<div id="ref-Mirman2014">
<p>Mirman, D. (2014). <em>Growth curve analysis and visualization using R</em>. Boca Raton, FL: CRC Press/Taylor &amp; Francis Group.</p>
</div>
<div id="ref-bdots17">
<p>Oleson, J. J., Cavanaugh, J. E., McMurray, B., &amp; Brown, G. (2017). Detecting time-specific differences between temporal nonlinear curves: Analyzing data from the visual world paradigm. <em>Statistical Methods in Medical Research</em>, <em>26</em>(6), 2708–2725. doi:<a href="https://doi.org/10.1177/0962280215607411">10.1177/0962280215607411</a></p>
</div>
<div id="ref-bdots18">
<p>Seedorff, M., Oleson, J. J., &amp; McMurray, B. (2018). Detecting when timeseries differ: Using the Bootstrapped Differences of Timeseries (BDOTS) to analyze Visual World Paradigm data (and more). <em>Journal of Memory and Language</em>, <em>102</em>, 55–67. doi:<a href="https://doi.org/https://doi.org/10.1016/j.jml.2018.05.004">https://doi.org/10.1016/j.jml.2018.05.004</a></p>
</div>
<div id="ref-Kruschke2017">
<p>Kruschke, J. K., &amp; Liddell, T. M. (2017). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. <em>Psychonomic Bulletin &amp; Review</em>, 1–29. doi:<a href="https://doi.org/10.3758/s13423-016-1221-4">10.3758/s13423-016-1221-4</a></p>
</div>
<div id="ref-irr">
<p>Gamer, M., Lemon, J., &amp; Singh, I. F. P. (2012). irr: Various coefficients of interrater reliability and agreement. Retrieved from <a href="https://CRAN.R-project.org/package=irr" class="uri">https://CRAN.R-project.org/package=irr</a></p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>More generally, I think of there being a flexibility–interpretability tradeoff with additive models being the most flexible but having the least interpretable parameters, functional curves being the least flexible but having the most interpretable parameters, and polynomials falling in between the two.<a href="fam-rec.html#fnref3">↩</a></p></li>
<li id="fn4"><p>My goals in using this method were simply to estimate model effects and quantify the uncertainty about those effects. This pragmatic, estimation-based approach of Bayesian statistics is illustrated in texts by <span class="citation">Gelman and Hill (<a href="#ref-GelmanHill">2007</a>)</span> and <span class="citation">McElreath (<a href="#ref-RethinkingBook">2016</a>)</span>.<a href="fam-rec.html#fnref4">↩</a></p></li>
</ol>
</div>
<script type="text/javascript">
  $("div.page-wrapper > h3").appendTo(".page-inner > section");
  $("div.page-wrapper > #refs").appendTo(".page-inner > section");
  $("div.page-wrapper > .footnotes").appendTo(".page-inner > section");
  $("div.body-inner > h3").appendTo(".page-inner > section");
  $("div.body-inner > #refs").appendTo(".page-inner > section");
  $("div.body-inner > .footnotes").appendTo(".page-inner > section");
</script>
            </section>

          </div>
        </div>
      </div>
<a href="aim1-method.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lex-competitors.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["dissertation.pdf"],
"toc": {
"collapse": "subsection"
},
"split_bib": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
