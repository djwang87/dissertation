<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Development of word recognition in preschoolers</title>
  <meta name="description" content="Development of word recognition in preschoolers">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Development of word recognition in preschoolers" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="assets/cover.png" />
  
  <meta name="github-repo" content="tjmahr/dissertation" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Development of word recognition in preschoolers" />
  
  
  <meta name="twitter:image" content="assets/cover.png" />

<meta name="author" content="Tristan Mahr">


<meta name="date" content="2018-09-04">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="vw-experiment-items.html">
<link rel="next" href="mp-experiment-items.html">
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="assets\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="dedication.html"><a href="dedication.html"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="overview-and-aims.html"><a href="overview-and-aims.html"><i class="fa fa-check"></i><b>1</b> Overview and aims</a><ul>
<li class="chapter" data-level="" data-path="overview-and-aims.html"><a href="overview-and-aims.html#study-1-familiar-word-recognition-and-lexical-competition"><i class="fa fa-check"></i>Study 1: Familiar word recognition and lexical competition</a></li>
<li class="chapter" data-level="" data-path="overview-and-aims.html"><a href="overview-and-aims.html#study-2-referent-selection-and-mispronunciations"><i class="fa fa-check"></i>Study 2: Referent selection and mispronunciations</a></li>
<li class="chapter" data-level="" data-path="overview-and-aims.html"><a href="overview-and-aims.html#summary"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="research-hypotheses.html"><a href="research-hypotheses.html"><i class="fa fa-check"></i><b>2</b> Research hypotheses</a><ul>
<li class="chapter" data-level="" data-path="research-hypotheses.html"><a href="research-hypotheses.html#study1-familiar-word-recognition-and-lexical-competition"><i class="fa fa-check"></i>Study 1: Familiar word recognition and lexical competition</a></li>
<li class="chapter" data-level="" data-path="research-hypotheses.html"><a href="research-hypotheses.html#study2-referent-selection-and-mispronunciations"><i class="fa fa-check"></i>Study 2: Referent selection and mispronunciations</a></li>
</ul></li>
<li class="part"><span><b>Study 1: Familiar word recognition and lexical competition</b></span></li>
<li class="chapter" data-level="3" data-path="aim1-introduction.html"><a href="aim1-introduction.html"><i class="fa fa-check"></i><b>3</b> Familiar word recognition</a><ul>
<li class="chapter" data-level="3.1" data-path="aim1-introduction.html"><a href="aim1-introduction.html#lexical-processing-dynamics"><i class="fa fa-check"></i><b>3.1</b> Lexical processing dynamics</a></li>
<li class="chapter" data-level="3.2" data-path="aim1-introduction.html"><a href="aim1-introduction.html#individual-differences-in-word-recognition"><i class="fa fa-check"></i><b>3.2</b> Individual differences in word recognition</a></li>
<li class="chapter" data-level="3.3" data-path="aim1-introduction.html"><a href="aim1-introduction.html#the-current-study"><i class="fa fa-check"></i><b>3.3</b> The current study</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="aim1-method.html"><a href="aim1-method.html"><i class="fa fa-check"></i><b>4</b> Method</a><ul>
<li class="chapter" data-level="4.1" data-path="aim1-method.html"><a href="aim1-method.html#aim1-participants"><i class="fa fa-check"></i><b>4.1</b> Participants</a></li>
<li class="chapter" data-level="4.2" data-path="aim1-method.html"><a href="aim1-method.html#aim1-procedure"><i class="fa fa-check"></i><b>4.2</b> Visual World Paradigm</a></li>
<li class="chapter" data-level="4.3" data-path="aim1-method.html"><a href="aim1-method.html#experiment-administration"><i class="fa fa-check"></i><b>4.3</b> Experiment administration</a></li>
<li class="chapter" data-level="4.4" data-path="aim1-method.html"><a href="aim1-method.html#stimuli"><i class="fa fa-check"></i><b>4.4</b> Stimuli</a></li>
<li class="chapter" data-level="4.5" data-path="aim1-method.html"><a href="aim1-method.html#data-screening"><i class="fa fa-check"></i><b>4.5</b> Data screening</a></li>
<li class="chapter" data-level="4.6" data-path="aim1-method.html"><a href="aim1-method.html#model-preparation"><i class="fa fa-check"></i><b>4.6</b> Model preparation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="fam-rec.html"><a href="fam-rec.html"><i class="fa fa-check"></i><b>5</b> Analysis of familiar word recognition</a><ul>
<li class="chapter" data-level="5.1" data-path="fam-rec.html"><a href="fam-rec.html#growth-curve-analysis"><i class="fa fa-check"></i><b>5.1</b> Growth curve analysis</a><ul>
<li class="chapter" data-level="5.1.1" data-path="fam-rec.html"><a href="fam-rec.html#growth-curve-features-as-measures-of-word-recognition-performance"><i class="fa fa-check"></i><b>5.1.1</b> Growth curve features as measures of word recognition performance</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="fam-rec.html"><a href="fam-rec.html#year-over-year-changes-in-word-recognition-performance"><i class="fa fa-check"></i><b>5.2</b> Year over year changes in word recognition performance</a></li>
<li class="chapter" data-level="5.3" data-path="fam-rec.html"><a href="fam-rec.html#exploring-plausible-ranges-of-performance-over-time"><i class="fa fa-check"></i><b>5.3</b> Exploring plausible ranges of performance over time</a></li>
<li class="chapter" data-level="5.4" data-path="fam-rec.html"><a href="fam-rec.html#are-individual-differences-stable-over-time"><i class="fa fa-check"></i><b>5.4</b> Are individual differences stable over time?</a></li>
<li class="chapter" data-level="5.5" data-path="fam-rec.html"><a href="fam-rec.html#predicting-future-vocabulary-size"><i class="fa fa-check"></i><b>5.5</b> Predicting future vocabulary size</a></li>
<li class="chapter" data-level="5.6" data-path="fam-rec.html"><a href="fam-rec.html#discussion"><i class="fa fa-check"></i><b>5.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lex-competitors.html"><a href="lex-competitors.html"><i class="fa fa-check"></i><b>6</b> Effects of phonological and semantic competitors</a><ul>
<li class="chapter" data-level="6.1" data-path="lex-competitors.html"><a href="lex-competitors.html#looks-to-the-phonological-competitor"><i class="fa fa-check"></i><b>6.1</b> Looks to the phonological competitor</a></li>
<li class="chapter" data-level="6.2" data-path="lex-competitors.html"><a href="lex-competitors.html#looks-to-the-semantic-competitor"><i class="fa fa-check"></i><b>6.2</b> Looks to the semantic competitor</a></li>
<li class="chapter" data-level="6.3" data-path="lex-competitors.html"><a href="lex-competitors.html#child-level-differences-in-competitor-sensitivity-at-age-3"><i class="fa fa-check"></i><b>6.3</b> Child-level differences in competitor sensitivity at age 3</a></li>
<li class="chapter" data-level="6.4" data-path="lex-competitors.html"><a href="lex-competitors.html#discussion-1"><i class="fa fa-check"></i><b>6.4</b> Discussion</a><ul>
<li class="chapter" data-level="6.4.1" data-path="lex-competitors.html"><a href="lex-competitors.html#immediate-activation-of-phonological-neighbors"><i class="fa fa-check"></i><b>6.4.1</b> Immediate activation of phonological neighbors</a></li>
<li class="chapter" data-level="6.4.2" data-path="lex-competitors.html"><a href="lex-competitors.html#late-activation-of-semantic-neighbors"><i class="fa fa-check"></i><b>6.4.2</b> Late activation of semantic neighbors</a></li>
<li class="chapter" data-level="6.4.3" data-path="lex-competitors.html"><a href="lex-competitors.html#lexical-competitors-and-child-level-predictors"><i class="fa fa-check"></i><b>6.4.3</b> Lexical competitors and child-level predictors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="aim1-discussion.html"><a href="aim1-discussion.html"><i class="fa fa-check"></i><b>7</b> General discussion</a><ul>
<li class="chapter" data-level="7.1" data-path="aim1-discussion.html"><a href="aim1-discussion.html#how-to-improve-word-recognition"><i class="fa fa-check"></i><b>7.1</b> How to improve word recognition</a></li>
<li class="chapter" data-level="7.2" data-path="aim1-discussion.html"><a href="aim1-discussion.html#learn-words-and-learn-connections-between-words"><i class="fa fa-check"></i><b>7.2</b> Learn words and learn connections between words</a></li>
<li class="chapter" data-level="7.3" data-path="aim1-discussion.html"><a href="aim1-discussion.html#individual-differences-are-most-important-at-younger-ages"><i class="fa fa-check"></i><b>7.3</b> Individual differences are most important at younger ages</a></li>
<li class="chapter" data-level="7.4" data-path="aim1-discussion.html"><a href="aim1-discussion.html#limitations-and-implications"><i class="fa fa-check"></i><b>7.4</b> Limitations and implications</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="aim1-h-check.html"><a href="aim1-h-check.html"><i class="fa fa-check"></i><b>8</b> Hypothesis check</a></li>
<li class="part"><span><b>Study 2: Referent selection and mispronunciations</b></span></li>
<li class="chapter" data-level="9" data-path="aim2-introduction.html"><a href="aim2-introduction.html"><i class="fa fa-check"></i><b>9</b> Mispronunciations and referent selection</a><ul>
<li class="chapter" data-level="9.1" data-path="aim2-introduction.html"><a href="aim2-introduction.html#how-phonetically-detailed-are-childrens-words"><i class="fa fa-check"></i><b>9.1</b> How phonetically detailed are children’s words?</a></li>
<li class="chapter" data-level="9.2" data-path="aim2-introduction.html"><a href="aim2-introduction.html#how-to-handle-nonwords"><i class="fa fa-check"></i><b>9.2</b> How to handle nonwords</a></li>
<li class="chapter" data-level="9.3" data-path="aim2-introduction.html"><a href="aim2-introduction.html#the-current-study-1"><i class="fa fa-check"></i><b>9.3</b> The current study</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="aim2-method.html"><a href="aim2-method.html"><i class="fa fa-check"></i><b>10</b> Method</a><ul>
<li class="chapter" data-level="10.1" data-path="aim2-method.html"><a href="aim2-method.html#mispronunciation-task"><i class="fa fa-check"></i><b>10.1</b> Mispronunciation task</a></li>
<li class="chapter" data-level="10.2" data-path="aim2-method.html"><a href="aim2-method.html#visual-stimuli"><i class="fa fa-check"></i><b>10.2</b> Visual stimuli</a></li>
<li class="chapter" data-level="10.3" data-path="aim2-method.html"><a href="aim2-method.html#novel-word-retention-tests"><i class="fa fa-check"></i><b>10.3</b> Novel word retention tests</a></li>
<li class="chapter" data-level="10.4" data-path="aim2-method.html"><a href="aim2-method.html#aim2-screening"><i class="fa fa-check"></i><b>10.4</b> Data screening</a><ul>
<li class="chapter" data-level="10.4.1" data-path="aim2-method.html"><a href="aim2-method.html#classifying-trials-based-on-initial-fixation-location"><i class="fa fa-check"></i><b>10.4.1</b> Classifying trials based on initial fixation location</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="aim2-method.html"><a href="aim2-method.html#model-preparation-1"><i class="fa fa-check"></i><b>10.5</b> Model preparation</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="real-nonword-selection.html"><a href="real-nonword-selection.html"><i class="fa fa-check"></i><b>11</b> Development of referent selection</a><ul>
<li class="chapter" data-level="11.1" data-path="real-nonword-selection.html"><a href="real-nonword-selection.html#nonwords-versus-familiar-words"><i class="fa fa-check"></i><b>11.1</b> Nonwords versus familiar words</a></li>
<li class="chapter" data-level="11.2" data-path="real-nonword-selection.html"><a href="real-nonword-selection.html#does-age-3-referent-selection-better-predict-age-5-vocabulary"><i class="fa fa-check"></i><b>11.2</b> Does age-3 referent selection better predict age-5 vocabulary?</a></li>
<li class="chapter" data-level="11.3" data-path="real-nonword-selection.html"><a href="real-nonword-selection.html#discussion-2"><i class="fa fa-check"></i><b>11.3</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html"><i class="fa fa-check"></i><b>12</b> Sensitivity to mispronunciations</a><ul>
<li class="chapter" data-level="12.1" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#unfamiliar-initial-trials-move-along-now"><i class="fa fa-check"></i><b>12.1</b> Unfamiliar-initial trials: Move along now</a><ul>
<li class="chapter" data-level="12.1.1" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#child-level-predictors"><i class="fa fa-check"></i><b>12.1.1</b> Child-level predictors</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#familiar-initial-trials-should-i-stay-or-should-i-gonow"><i class="fa fa-check"></i><b>12.2</b> Familiar-initial trials: Should I stay or should I go now?</a><ul>
<li class="chapter" data-level="12.2.1" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#child-level-predictors-and-different-listening-behaviors"><i class="fa fa-check"></i><b>12.2.1</b> Child-level predictors and different listening behaviors</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#looking-behaviors-and-word-learning"><i class="fa fa-check"></i><b>12.3</b> Looking behaviors and word learning</a></li>
<li class="chapter" data-level="12.4" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#discussion-3"><i class="fa fa-check"></i><b>12.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="aim2-discussion.html"><a href="aim2-discussion.html"><i class="fa fa-check"></i><b>13</b> General discussion</a><ul>
<li class="chapter" data-level="13.1" data-path="aim2-discussion.html"><a href="aim2-discussion.html#a-lexical-processing-account-of-the-results"><i class="fa fa-check"></i><b>13.1</b> A lexical processing account of the results</a></li>
<li class="chapter" data-level="13.2" data-path="aim2-discussion.html"><a href="aim2-discussion.html#a-nonword-is-just-a-word-you-havent-learned-yet"><i class="fa fa-check"></i><b>13.2</b> A nonword is just a word you haven’t learned yet</a></li>
<li class="chapter" data-level="13.3" data-path="aim2-discussion.html"><a href="aim2-discussion.html#limitations-and-implications-1"><i class="fa fa-check"></i><b>13.3</b> Limitations and implications</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="aim2-h-check.html"><a href="aim2-h-check.html"><i class="fa fa-check"></i><b>14</b> Hypothesis check</a></li>
<li class="part"><span><b>Overall discussion</b></span></li>
<li class="chapter" data-level="15" data-path="both-studies-discussion.html"><a href="both-studies-discussion.html"><i class="fa fa-check"></i><b>15</b> General discussion of both studies</a><ul>
<li class="chapter" data-level="15.1" data-path="both-studies-discussion.html"><a href="both-studies-discussion.html#mechanisms-of-word-recognition"><i class="fa fa-check"></i><b>15.1</b> Mechanisms of word recognition</a><ul>
<li class="chapter" data-level="15.1.1" data-path="both-studies-discussion.html"><a href="both-studies-discussion.html#open-questions-about-word-recognition-mechanisms"><i class="fa fa-check"></i><b>15.1.1</b> Open questions about word recognition mechanisms</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="both-studies-discussion.html"><a href="both-studies-discussion.html#clinical-implications"><i class="fa fa-check"></i><b>15.2</b> Clinical implications</a></li>
<li class="chapter" data-level="15.3" data-path="both-studies-discussion.html"><a href="both-studies-discussion.html#contributions"><i class="fa fa-check"></i><b>15.3</b> Contributions</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="vw-experiment-items.html"><a href="vw-experiment-items.html"><i class="fa fa-check"></i><b>A</b> Items used in Study 1</a></li>
<li class="chapter" data-level="B" data-path="aim1-gca-models.html"><a href="aim1-gca-models.html"><i class="fa fa-check"></i><b>B</b> Computational details for Study 1</a><ul>
<li class="chapter" data-level="B.1" data-path="aim1-gca-models.html"><a href="aim1-gca-models.html#growth-curve-analyses"><i class="fa fa-check"></i><b>B.1</b> Growth curve analyses</a><ul>
<li class="chapter" data-level="B.1.1" data-path="aim1-gca-models.html"><a href="aim1-gca-models.html#convergence-diagnostics-for-bayesian-models"><i class="fa fa-check"></i><b>B.1.1</b> Convergence diagnostics for Bayesian models</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="aim1-gca-models.html"><a href="aim1-gca-models.html#generalized-additive-models"><i class="fa fa-check"></i><b>B.2</b> Generalized additive models</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="mp-experiment-items.html"><a href="mp-experiment-items.html"><i class="fa fa-check"></i><b>C</b> Items used in Study 2</a></li>
<li class="chapter" data-level="D" data-path="aim2-gca-models.html"><a href="aim2-gca-models.html"><i class="fa fa-check"></i><b>D</b> Computational details for Study 2</a><ul>
<li class="chapter" data-level="D.1" data-path="aim2-gca-models.html"><a href="aim2-gca-models.html#real-words-versus-nonwords-growth-curves"><i class="fa fa-check"></i><b>D.1</b> Real words versus nonwords growth curves</a></li>
<li class="chapter" data-level="D.2" data-path="aim2-gca-models.html"><a href="aim2-gca-models.html#mispronunciation-growth-curves"><i class="fa fa-check"></i><b>D.2</b> Mispronunciation growth curves</a></li>
<li class="chapter" data-level="D.3" data-path="aim2-gca-models.html"><a href="aim2-gca-models.html#item-response-analysis-for-novel-word-retention"><i class="fa fa-check"></i><b>D.3</b> Item-response analysis for novel word retention</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="aim2-mp-items.html"><a href="aim2-mp-items.html"><i class="fa fa-check"></i><b>E</b> Effects of specific mispronunciations in Study 2</a></li>
<li class="chapter" data-level="F" data-path="related-work.html"><a href="related-work.html"><i class="fa fa-check"></i><b>F</b> Related work</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Created with bookdown</a></li>
<li><a href="https://tjmahr.github.io/" target="blank">Tristan Mahr</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Development of word recognition in preschoolers</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="aim1-gca-models" class="section level1">
<h1><span class="header-section-number">B</span> Computational details for Study 1</h1>
<div id="growth-curve-analyses" class="section level2">
<h2><span class="header-section-number">B.1</span> Growth curve analyses</h2>
<p>These models were fit in R <span class="citation">(vers. 3.4.3; R Core Team, <a href="#ref-R-base">2018</a>)</span> with the RStanARM package <span class="citation">(vers. 2.16.3; Gabry &amp; Goodrich, <a href="#ref-R-rstanarm">2018</a>)</span>.</p>
<p>When I computed the orthogonal polynomial features for Time, they were scaled so that the linear feature ranged from −.5 to .5. Under this scaling a unit change in Time<sup>1</sup> was equal to change from the start to the end of the analysis window. Table <a href="aim1-gca-models.html#tab:poly-feature-ranges">B.1</a> shows the ranges of the time features.</p>
<table>
<caption><span id="tab:poly-feature-ranges">Table B.1: </span>Ranges of the polynomial time features.</caption>
<thead>
<tr class="header">
<th align="left">Feature</th>
<th align="right">Min</th>
<th align="right">Max</th>
<th align="right">Range</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Time<sup>1</sup></td>
<td align="right">−0.50</td>
<td align="right">0.50</td>
<td align="right">1.00</td>
</tr>
<tr class="even">
<td align="left">Time<sup>2</sup></td>
<td align="right">−0.33</td>
<td align="right">0.60</td>
<td align="right">0.93</td>
</tr>
<tr class="odd">
<td align="left">Time<sup>3</sup></td>
<td align="right">−0.63</td>
<td align="right">0.63</td>
<td align="right">1.26</td>
</tr>
<tr class="even">
<td align="left">Trial window (ms)</td>
<td align="right">250</td>
<td align="right">1500</td>
<td align="right">1250</td>
</tr>
</tbody>
</table>
<p>It took approximately 24 hours to run the model on four Monte Carlo sampling chains with 1,000 warm-up iterations and 1,000 sampling iterations. Warm-up iterations are discarded, so the model comprises 4,000 samples from the posterior distribution.</p>
<p>The code used to fit the model with RStanARM is printed below. The variables <code>ot1</code>, <code>ot2</code>, and <code>ot3</code> are the polynomial time features, <code>ResearchID</code> identifies children, and <code>Study</code> identifies the age/year of the longitudinal project. Mnemonically, <code>ot</code> stands for <em>orthogonal time</em> and the number is the degree of the polynomial. This convention is used by <span class="citation">Mirman (<a href="#ref-Mirman2014">2014</a>)</span>. <code>Study</code> refers to the timepoint (year) of the larger longitudinal investigation. Conceptually, I use <em>study</em> to mean a data-collection unit, and I think of each wave of testing with their somewhat different tasks and protocols as separate studies. <code>Primary</code> counts the number of looks to the target image at each time bin; <code>Others</code> counts looks to the other three images. <code>cbind(Primary, Others)</code> is used to package both counts together for a logistic regression.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rstanarm)

<span class="co"># Run chains on different cores</span>
<span class="kw">options</span>(<span class="dt">mc.cores =</span> parallel<span class="op">::</span><span class="kw">detectCores</span>())

m &lt;-<span class="st"> </span><span class="kw">stan_glmer</span>(
  <span class="kw">cbind</span>(Primary, Others) <span class="op">~</span>
<span class="st">    </span>(ot1 <span class="op">+</span><span class="st"> </span>ot2 <span class="op">+</span><span class="st"> </span>ot3) <span class="op">*</span><span class="st"> </span>Study <span class="op">+</span>
<span class="st">    </span>(ot1 <span class="op">+</span><span class="st"> </span>ot2 <span class="op">+</span><span class="st"> </span>ot3 <span class="op">|</span><span class="st"> </span>ResearchID<span class="op">/</span>Study),
  <span class="dt">family =</span> binomial,
  <span class="dt">prior =</span> <span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>),
  <span class="dt">prior_intercept =</span> <span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">5</span>),
  <span class="dt">prior_covariance =</span> <span class="kw">decov</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>),
  <span class="dt">control =</span> <span class="kw">list</span>(
    <span class="dt">adapt_delta =</span> .<span class="dv">95</span>, 
    <span class="dt">max_treedepth =</span> <span class="dv">15</span>),
  <span class="dt">data =</span> d_m)

<span class="co"># Save the output</span>
readr<span class="op">::</span><span class="kw">write_rds</span>(m, <span class="st">&quot;./data/stan_aim1_cubic_model.rds.gz&quot;</span>)</code></pre></div>
<p>The code <code>cbind(Primary, Others) ~ (ot1 + ot2 + ot3) * Study</code> fits a cubic growth curve for each age. This code uses R’s formula syntax to regress the looking counts onto an intercept term (implicitly included by default), <code>ot1</code>, <code>ot2</code>, <code>ot3</code> along with the interactions of the <code>Study</code> variable with the intercept, <code>ot1</code>, <code>ot2</code>, and <code>ot3</code>.</p>
<p>The line <code>(ot1 + ot2 + ot3 | ResearchID/Study)</code> describes the random-effect structure of the model with the <code>/</code> indicating that data from each <code>Study</code> is nested within each <code>ResearchID</code>. Put another way, <code>... | ResearchID/Study</code> expands into <code>... | ResearchID</code> and <code>... | ResearchID:Study</code>. Thus, for each child, we have general <code>ResearchID</code> effects for the intercept, Time<sup>1</sup>, Time<sup>2</sup>, and Time<sup>3</sup>. These child-level effects are further adjusted using <code>Study:ResearchID</code> effects. The effects in each level are allowed to correlate. For example, I would expect that participants with low average looking probabilities (low intercepts) to have flatter growth curves (low Time<sup>1</sup> effects), and this relationship would be captured by one of the random-effect correlation terms.</p>
<p>Printing the model object reports the point estimates of the model fixed effects and point-estimate correlation matrices for the random effects.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(m, <span class="dt">digits =</span> <span class="dv">2</span>)
<span class="co">#&gt; stan_glmer</span>
<span class="co">#&gt;  family:       binomial [logit]</span>
<span class="co">#&gt;  formula:      cbind(Primary, Others) ~ </span>
<span class="co">#&gt;                  (ot1 + ot2 + ot3) * Study + </span>
<span class="co">#&gt;                  (ot1 + ot2 + ot3 | ResearchID/Study)</span>
<span class="co">#&gt;  observations: 12584</span>
<span class="co">#&gt; ------</span>
<span class="co">#&gt;                     Median MAD_SD</span>
<span class="co">#&gt; (Intercept)         -0.47   0.03 </span>
<span class="co">#&gt; ot1                  1.57   0.06 </span>
<span class="co">#&gt; ot2                  0.05   0.04 </span>
<span class="co">#&gt; ot3                 -0.18   0.03 </span>
<span class="co">#&gt; StudyTimePoint2      0.41   0.03 </span>
<span class="co">#&gt; StudyTimePoint3      0.70   0.04 </span>
<span class="co">#&gt; ot1:StudyTimePoint2  0.56   0.08 </span>
<span class="co">#&gt; ot1:StudyTimePoint3  1.10   0.08 </span>
<span class="co">#&gt; ot2:StudyTimePoint2 -0.16   0.05 </span>
<span class="co">#&gt; ot2:StudyTimePoint3 -0.35   0.05 </span>
<span class="co">#&gt; ot3:StudyTimePoint2 -0.12   0.04 </span>
<span class="co">#&gt; ot3:StudyTimePoint3 -0.21   0.04 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Error terms:</span>
<span class="co">#&gt;  Groups           Name        Std.Dev. Corr             </span>
<span class="co">#&gt;  Study:ResearchID (Intercept) 0.3054                    </span>
<span class="co">#&gt;                   ot1         0.6914    0.20            </span>
<span class="co">#&gt;                   ot2         0.4367   -0.11  0.02      </span>
<span class="co">#&gt;                   ot3         0.2938   -0.11 -0.44 -0.06</span>
<span class="co">#&gt;  ResearchID       (Intercept) 0.2635                    </span>
<span class="co">#&gt;                   ot1         0.4228    0.78            </span>
<span class="co">#&gt;                   ot2         0.1251   -0.75 -0.56      </span>
<span class="co">#&gt;                   ot3         0.0576   -0.23 -0.31  0.19</span>
<span class="co">#&gt; Num. levels: Study:ResearchID 484, ResearchID 195 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Sample avg. posterior predictive distribution of y:</span>
<span class="co">#&gt;          Median MAD_SD</span>
<span class="co">#&gt; mean_PPD 49.86   0.06 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; ------</span>
<span class="co">#&gt; For info on the priors used see help(&#39;prior_summary.stanreg&#39;).</span></code></pre></div>
<p>The model used the following priors:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prior_summary</span>(m)
<span class="co">#&gt; Priors for model &#39;m&#39; </span>
<span class="co">#&gt; ------</span>
<span class="co">#&gt; Intercept (after predictors centered)</span>
<span class="co">#&gt;  ~ normal(location = 0, scale = 5)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients</span>
<span class="co">#&gt;  ~ normal(location = [0,0,0,...], scale = [1,1,1,...])</span>
<span class="co">#&gt;      **adjusted scale = [3.33,3.33,3.33,...]</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Covariance</span>
<span class="co">#&gt;  ~ decov(reg. = 2, conc. = 1, shape = 1, scale = 1)</span>
<span class="co">#&gt; ------</span>
<span class="co">#&gt; See help(&#39;prior_summary.stanreg&#39;) for more details</span></code></pre></div>
<p>The priors for the intercept and regression coefficients are wide, weakly informative normal distributions. These distributions are centered at 0, so negative and positive effects are equally likely. The intercept distribution as a standard deviation of 5, and the coefficients have a standard deviation of around 3. On the log-odds scale, 95% looking to target would be 2.94, so effects of this magnitude are easily accommodated by distributions like Normal(0 [mean], 3 [SD]) and Normal(0, 5).</p>
<p>These priors are very conservative, including information about the size of an effect but not its direction. <span class="citation">Gelman and Carlin (<a href="#ref-Gelman2014">2014</a>)</span> describe two types of errors that can arise when estimating an effect or model parameter: Type S errors where the <em>sign</em> of the estimated effect is wrong and Type M errors where the magnitude of the estimated effect is wrong. From this perspective, the priors here are uninformative in terms of the sign: Both positive and negative effects are equally likely before seeing the data. Future work on these models should incorporate sign information into the priors: For example, it is a safe bet that the linear time effect will be positive—the curves goes up—so that prior can be adjusted to have a positive, non-zero mean. For this model, I incorporated weak information regarding the magnitude of the effects (an SD of 3 for all effects). On the basis of the estimates here, I employed more informative priors in Study 2 with an SD of 2 for the linear time effect and an SD of 1 for other effects.</p>
<p>For the random-effect part of the model, I used RStanARM’s <code>decov()</code> prior which simultaneously sets a prior on the variances and correlations of the model’s random effect terms. I used the default prior for the variance terms and applied a weakly informative LKJ(2) prior on the random-effect correlations. Figure <a href="aim1-gca-models.html#fig:lkj-prior">B.1</a> shows samples from the prior distribution of two dummy models fit with the default LKJ(1) prior and the weakly informative LKJ(2) prior used here. Under LKJ(2), extreme correlations are less plausible; the prior shifts the probability mass away from the ±1 boundaries towards the center. The motivation for this kind of prior was <em>regularization</em>: I give the model a small amount of information to nudge it away from extreme, degenerate values.</p>

<div class="figure" style="text-align: center"><span id="fig:lkj-prior"></span>
<img src="92-app-aim1-models_files/figure-html/lkj-prior-1.png" alt="Samples of correlation effects drawn from LKJ(1) and LKJ(2) priors." width="50%" />
<p class="caption">
Figure B.1: Samples of correlation effects drawn from LKJ(1) and LKJ(2) priors.
</p>
</div>
<p>Summary of the familiar word recognition model with diagnostics and 90% uncertainty intervals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># the last 20 column names are the random effects</span>
ranef_names &lt;-<span class="st"> </span><span class="kw">tail</span>(<span class="kw">colnames</span>(<span class="kw">as_tibble</span>(m)), <span class="dv">20</span>)

<span class="kw">summary</span>(
  <span class="dt">object =</span> m, 
  <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, ranef_names),
  <span class="dt">probs =</span> <span class="kw">c</span>(.<span class="dv">05</span>, .<span class="dv">95</span>),
  <span class="dt">digits =</span> <span class="dv">3</span>)
<span class="co">#&gt; </span>
<span class="co">#&gt; Model Info:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  function:     stan_glmer</span>
<span class="co">#&gt;  family:       binomial [logit]</span>
<span class="co">#&gt;  formula:      cbind(Primary, Others) ~ </span>
<span class="co">#.                  (ot1 + ot2 + ot3) * Study + </span>
<span class="co">#.                  (ot1 + ot2 + ot3 | ResearchID/Study)</span>
<span class="co">#&gt;  algorithm:    sampling</span>
<span class="co">#&gt;  priors:       see help(&#39;prior_summary&#39;)</span>
<span class="co">#&gt;  sample:       4000 (posterior sample size)</span>
<span class="co">#&gt;  observations: 12584</span>
<span class="co">#&gt;  groups:       Study:ResearchID (484), ResearchID (195)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Estimates:</span>
<span class="co">#&gt;                                          mean     sd     5%    95% </span>
<span class="co">#&gt; (Intercept)                            -0.469  0.032 -0.523 -0.419</span>
<span class="co">#&gt; ot1                                     1.575  0.066  1.465  1.682</span>
<span class="co">#&gt; ot2                                     0.048  0.038 -0.014  0.110</span>
<span class="co">#&gt; ot3                                    -0.175  0.026 -0.218 -0.130</span>
<span class="co">#&gt; StudyTimePoint2                         0.410  0.035  0.355  0.468</span>
<span class="co">#&gt; StudyTimePoint3                         0.697  0.035  0.641  0.757</span>
<span class="co">#&gt; ot1:StudyTimePoint2                     0.565  0.079  0.437  0.695</span>
<span class="co">#&gt; ot1:StudyTimePoint3                     1.099  0.080  0.968  1.233</span>
<span class="co">#&gt; ot2:StudyTimePoint2                    -0.157  0.052 -0.242 -0.073</span>
<span class="co">#&gt; ot2:StudyTimePoint3                    -0.354  0.053 -0.443 -0.267</span>
<span class="co">#&gt; ot3:StudyTimePoint2                    -0.121  0.036 -0.181 -0.061</span>
<span class="co">#&gt; ot3:StudyTimePoint3                    -0.213  0.036 -0.275 -0.155</span>
<span class="co">#&gt; Sigma[Study:ResearchID:(Intercept),(Intercept)] </span>
<span class="co">#&gt;                                         0.093  0.008  0.081  0.107</span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot1,(Intercept)] 0.042  0.013  0.022  0.064</span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot2,(Intercept)]</span>
<span class="co">#&gt;                                        -0.015  0.008 -0.029 -0.001</span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot3,(Intercept)]</span>
<span class="co">#&gt;                                        -0.010  0.005 -0.019 -0.001</span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot1,ot1]         0.478  0.043  0.411  0.551</span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot2,ot1]         0.006  0.019 -0.026  0.036</span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot3,ot1]        -0.089  0.013 -0.111 -0.069</span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot2,ot2]         0.191  0.015  0.166  0.217</span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot3,ot2]        -0.007  0.008 -0.019  0.005</span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot3,ot3]         0.086  0.008  0.074  0.099</span>
<span class="co">#&gt; Sigma[ResearchID:(Intercept),(Intercept)]</span>
<span class="co">#&gt;                                         0.069  0.012  0.051  0.090</span>
<span class="co">#&gt; Sigma[ResearchID:ot1,(Intercept)]       0.087  0.018  0.060  0.117</span>
<span class="co">#&gt; Sigma[ResearchID:ot2,(Intercept)]      -0.025  0.009 -0.040 -0.011</span>
<span class="co">#&gt; Sigma[ResearchID:ot3,(Intercept)]      -0.004  0.004 -0.011  0.003</span>
<span class="co">#&gt; Sigma[ResearchID:ot1,ot1]               0.179  0.043  0.113  0.252</span>
<span class="co">#&gt; Sigma[ResearchID:ot2,ot1]              -0.030  0.015 -0.056 -0.006</span>
<span class="co">#&gt; Sigma[ResearchID:ot3,ot1]              -0.008  0.008 -0.022  0.004</span>
<span class="co">#&gt; Sigma[ResearchID:ot2,ot2]               0.016  0.008  0.005  0.030</span>
<span class="co">#&gt; Sigma[ResearchID:ot3,ot2]               0.001  0.002 -0.002  0.006</span>
<span class="co">#&gt; Sigma[ResearchID:ot3,ot3]               0.003  0.002  0.001  0.008</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Diagnostics:</span>
<span class="co">#&gt;                                                 mcse  Rhat  n_eff</span>
<span class="co">#&gt; (Intercept)                                     0.001 1.005 1086 </span>
<span class="co">#&gt; ot1                                             0.002 1.004  857 </span>
<span class="co">#&gt; ot2                                             0.001 1.006  842 </span>
<span class="co">#&gt; ot3                                             0.001 1.002 1156 </span>
<span class="co">#&gt; StudyTimePoint2                                 0.001 1.007 1034 </span>
<span class="co">#&gt; StudyTimePoint3                                 0.001 1.006  959 </span>
<span class="co">#&gt; ot1:StudyTimePoint2                             0.003 1.014  674 </span>
<span class="co">#&gt; ot1:StudyTimePoint3                             0.003 1.005  934 </span>
<span class="co">#&gt; ot2:StudyTimePoint2                             0.002 1.003  836 </span>
<span class="co">#&gt; ot2:StudyTimePoint3                             0.002 1.006  762 </span>
<span class="co">#&gt; ot3:StudyTimePoint2                             0.001 1.003 1183 </span>
<span class="co">#&gt; ot3:StudyTimePoint3                             0.001 1.001 1390 </span>
<span class="co">#&gt; Sigma[Study:ResearchID:(Intercept),(Intercept)] 0.000 1.002 1093 </span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot1,(Intercept)]         0.001 1.009  475 </span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot2,(Intercept)]         0.000 1.023  323 </span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot3,(Intercept)]         0.000 1.003  792 </span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot1,ot1]                 0.002 1.003  547 </span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot2,ot1]                 0.001 1.013  277 </span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot3,ot1]                 0.000 1.005  806 </span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot2,ot2]                 0.001 1.010  665 </span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot3,ot2]                 0.000 1.001 1131 </span>
<span class="co">#&gt; Sigma[Study:ResearchID:ot3,ot3]                 0.000 1.004 1220 </span>
<span class="co">#&gt; Sigma[ResearchID:(Intercept),(Intercept)]       0.000 1.004  913 </span>
<span class="co">#&gt; Sigma[ResearchID:ot1,(Intercept)]               0.001 1.008  636 </span>
<span class="co">#&gt; Sigma[ResearchID:ot2,(Intercept)]               0.001 1.026  307 </span>
<span class="co">#&gt; Sigma[ResearchID:ot3,(Intercept)]               0.000 1.006  711 </span>
<span class="co">#&gt; Sigma[ResearchID:ot1,ot1]                       0.003 1.010  261 </span>
<span class="co">#&gt; Sigma[ResearchID:ot2,ot1]                       0.001 1.021  242 </span>
<span class="co">#&gt; Sigma[ResearchID:ot3,ot1]                       0.000 1.006  331 </span>
<span class="co">#&gt; Sigma[ResearchID:ot2,ot2]                       0.000 1.037  257 </span>
<span class="co">#&gt; Sigma[ResearchID:ot3,ot2]                       0.000 1.009  439 </span>
<span class="co">#&gt; Sigma[ResearchID:ot3,ot3]                       0.000 1.007  340 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; For each parameter, mcse is Monte Carlo standard error, n_eff is a </span>
<span class="co">#&gt; crude measure of effective sample size, and Rhat is the potential </span>
<span class="co">#&gt; scale reduction factor on split chains (at convergence Rhat=1).</span></code></pre></div>
<div id="convergence-diagnostics-for-bayesian-models" class="section level3">
<h3><span class="header-section-number">B.1.1</span> Convergence diagnostics for Bayesian models</h3>
<p>For the Bayesian models estimated in Study 1 and Study 2, I assessed model convergence by checking software warnings and checking sampling diagnostics. Stan programs emit warnings when the Hamiltonian Monte Carlo sampler runs into problems like divergent transitions or a low Bayesian Fraction of Missing Information statistic. When I encountered these warnings, I handled them by adjusting the sampling controls, as documented in &lt;<a href="http://mc-stan.org/misc/warnings.html" class="uri">http://mc-stan.org/misc/warnings.html</a>&gt;, or incorporating more information into the model’s priors. In the above model, for example, the <code>adapt_delta</code> and <code>max_treedepth</code> controls were increased to help the model more carefully explore the posterior distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rstan<span class="op">::</span><span class="kw">check_hmc_diagnostics</span>(m<span class="op">$</span>stanfit)
<span class="co">#&gt; </span>
<span class="co">#&gt; Divergences:</span>
<span class="co">#&gt; 3 of 4000 iterations ended with a divergence (0.075%).</span>
<span class="co">#&gt; Try increasing &#39;adapt_delta&#39; to remove the divergences.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Tree depth:</span>
<span class="co">#&gt; 0 of 4000 iterations saturated the maximum tree depth of 15.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Energy:</span>
<span class="co">#&gt; E-BFMI indicated no pathological behavior.</span></code></pre></div>
<p>Additionally, I also checked for convergence by using Markov Chain Monte Carlo diagnostics. The models consisted of four sampling chains which explore the posterior distribution from random starting locations. The split <span class="math inline">\(\hat{R}\)</span> (“<em>R</em>-hat”) diagnostic checks how well the sampling chains mix together <span class="citation">(Gelman et al., <a href="#ref-BDA3">2014</a>, p. 285; Stan Development Team, <a href="#ref-StanManual">2017</a>, p. 371)</span>. If the chains are stuck in their own neighborhoods of the parameter space, then the values sampled in each chain will not mix very well. The <em>split</em> designation means that each chain is first split in half so that the also diagnostic checks for within-chain mixing. At convergence <span class="math inline">\(\hat{R}\)</span> equals 1, so a rule of thumb is that <span class="math inline">\(\hat{R}\)</span> should be less than 1.1 <span class="citation">(e.g., Gelman et al., <a href="#ref-BDA3">2014</a>, p. 285)</span>. In the bayesplot package <span class="citation">(Gabry &amp; Mahr, <a href="#ref-bayesplot">2018</a>)</span>, we use the convention that values below 1.05 are <em>good</em> and values above 1.05 but below than 1.1 are <em>okay</em>.</p>
<p>The other diagnostic I monitored was the number of effective samples. If we think of a sampling chain as exploring a parameter space, then the samples form a random walk with each step being a movement from an earlier location. This situation raises the risk of <em>autocorrelation</em> where neighboring sampling steps within a chain are correlated with each other. The number of effective samples (“<em>n</em> eff.”) diagnostic estimates the number of posterior samples, taking into account sampling autocorrelation <span class="citation">(Gelman et al., <a href="#ref-BDA3">2014</a>, p. 285; Stan Development Team, <a href="#ref-StanManual">2017</a>, p. 373)</span>. The square root of this number is used to calculate Monte Carlo standard error statistics <span class="citation">(e.g., Gelman et al., <a href="#ref-BDA3">2014</a>, p. 267)</span>, so I think of the number of effective samples as the amount of precision available for a parameter estimate. Interpreting this statistic depends on the quantity being estimated and the amount of precision desired. As a rule of thumb, <span class="citation">Gelman et al. (<a href="#ref-BDA3">2014</a>)</span> mentions 10 effective samples per chain as a baseline for diagnosing non-convergence: “Having an effective sample size of 10 per sequence should typically correspond to stability of all the simulated sequences. For some purposes, more precision will be desired, and then a higher effective sample size threshold can be used. [p. 287]”</p>
</div>
</div>
<div id="generalized-additive-models" class="section level2">
<h2><span class="header-section-number">B.2</span> Generalized additive models</h2>
<p>To model the looks to the competitor images, I used generalized additive (mixed) models. The models were fit in R (vers. 3.4.3) using the mgcv R package <span class="citation">(vers. 1.8.23; Wood, <a href="#ref-Wood2017">2017</a>)</span> with support from tools in the itsadug R package <span class="citation">(vers. 2.3; van Rij et al., <a href="#ref-itsadug">2017</a>)</span>.</p>
<p>I will briefly walk through the code used to fit one of these models in order to articulate the modeling decisions at play. I first convert the categorical variables into the right types, so that the model can fit difference smooths.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create a Study dummy variabe with Age 4 as the reference level</span>
phon_d<span class="op">$</span>S &lt;-<span class="st"> </span><span class="kw">factor</span>(
  phon_d<span class="op">$</span>Study, 
  <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;TimePoint2&quot;</span>, <span class="st">&quot;TimePoint1&quot;</span>, <span class="st">&quot;TimePoint3&quot;</span>))

<span class="co"># Convert the ResearchID into a factor</span>
phon_d<span class="op">$</span>R &lt;-<span class="st"> </span><span class="kw">as.factor</span>(phon_d<span class="op">$</span>ResearchID)

<span class="co"># Convert the Study factor (phon_d$S) into an ordered factor.</span>
<span class="co"># This step is needed for the ti model estimate difference smooths.</span>
phon_d<span class="op">$</span>S2 &lt;-<span class="st"> </span><span class="kw">as.ordered</span>(phon_d<span class="op">$</span>S)
<span class="kw">contrasts</span>(phon_d<span class="op">$</span>S2) &lt;-<span class="st"> &quot;contr.treatment&quot;</span>
<span class="kw">contrasts</span>(phon_d<span class="op">$</span>S2)</code></pre></div>
<p>I fit the generalized additive model with the code below. The outcome <code>elog</code> is the empirical log-odds of looking to the phonological competitor relative to the unrelated word.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(mgcv)

phon_gam &lt;-<span class="st"> </span><span class="kw">bam</span>(
  elog <span class="op">~</span><span class="st"> </span>S2 <span class="op">+</span>
<span class="st">    </span><span class="kw">s</span>(Time) <span class="op">+</span><span class="st"> </span><span class="kw">s</span>(Time, <span class="dt">by =</span> S2) <span class="op">+</span>
<span class="st">    </span><span class="kw">s</span>(Time, R, <span class="dt">bs =</span> <span class="st">&quot;fs&quot;</span>, <span class="dt">m =</span> <span class="dv">1</span>, <span class="dt">k =</span> <span class="dv">5</span>),
  <span class="dt">data =</span> phon_d)

<span class="co"># Save the output</span>
readr<span class="op">::</span><span class="kw">write_rds</span>(phon_gam, <span class="st">&quot;./data/aim1-phon-random-smooths.rds.gz&quot;</span>)</code></pre></div>
<p>There is just one parametric term: <code>S2</code>. The term computes the average effect of each study with Age 4 serving as the reference condition (and as the model intercept).</p>
<p>Next come the smooth terms. <code>s(Time)</code> fits the shape of Time for the reference condition (Age 4). <code>s(Time, by = S2)</code> fits the difference smooths for Age 3 versus Age 4 and Age 5 versus Age 4. <code>s(Time, R, bs = &quot;fs&quot;, m = 1, k = 5)</code> fits a smooth for each participant (<code>R</code>). <code>bs = &quot;fs&quot;</code> means that the model should use a factor smooth (<code>fs</code>) basis (<code>bs</code>)—that is, a “random effect” smooth for each participant. <code>m = 1</code> changes the smoothness penalty so that the random effects are pulled towards the group average; <span class="citation">Winter and Wieling (<a href="#ref-Winter2016">2016</a>)</span> and <span class="citation">Baayen, Rij, Cat, and Wood (<a href="#ref-Baayen2016">2016</a>)</span> suggest using this option. <code>k = 5</code> means to use 5 knots (<code>k</code>) for the basis function. The other smooths use the default number of knots (10). I used fewer knots for the by-child smooths because of limited data. As a result, these smooths capture by-child variation by making coarse adjustments to study-level growth curves.</p>
<p>Summary of the phonological model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m_p &lt;-<span class="st"> </span>readr<span class="op">::</span><span class="kw">read_rds</span>(<span class="st">&quot;./data/aim1-phon-random-smooths.rds.gz&quot;</span>)
<span class="kw">summary</span>(m_p)
<span class="co">#&gt; </span>
<span class="co">#&gt; Family: gaussian </span>
<span class="co">#&gt; Link function: identity </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Formula:</span>
<span class="co">#&gt; elog ~ S2 + s(Time) + s(Time, by = S2) + s(Time, R, bs = &quot;fs&quot;, </span>
<span class="co">#&gt;     m = 1, k = 5)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Parametric coefficients:</span>
<span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span>
<span class="co">#&gt; (Intercept)   0.159200   0.048807   3.262  0.00111 ** </span>
<span class="co">#&gt; S2TimePoint1 -0.002641   0.013840  -0.191  0.84864    </span>
<span class="co">#&gt; S2TimePoint3  0.151073   0.013601  11.107  &lt; 2e-16 ***</span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Approximate significance of smooth terms:</span>
<span class="co">#&gt;                          edf  Ref.df     F  p-value    </span>
<span class="co">#&gt; s(Time)                7.277   8.165 10.61 3.51e-15 ***</span>
<span class="co">#&gt; s(Time):S2TimePoint1   5.478   6.590 17.10  &lt; 2e-16 ***</span>
<span class="co">#&gt; s(Time):S2TimePoint3   1.001   1.002 17.86 2.37e-05 ***</span>
<span class="co">#&gt; s(Time,R)            852.928 974.000 12.97  &lt; 2e-16 ***</span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; R-sq.(adj) =  0.311   Deviance explained = 33.1%</span>
<span class="co">#&gt; fREML =  41726  Scale est. = 0.8629    n = 30008</span></code></pre></div>
<p>Summary of the semantic model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m_s &lt;-<span class="st"> </span>readr<span class="op">::</span><span class="kw">read_rds</span>(<span class="st">&quot;./data/aim1-semy-random-smooths.rds.gz&quot;</span>)
<span class="kw">summary</span>(m_s)
<span class="co">#&gt; </span>
<span class="co">#&gt; Family: gaussian </span>
<span class="co">#&gt; Link function: identity </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Formula:</span>
<span class="co">#&gt; elog ~ S2 + s(Time) + s(Time, by = S2) + s(Time, R, bs = &quot;fs&quot;, </span>
<span class="co">#&gt;     m = 1, k = 5)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Parametric coefficients:</span>
<span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span>
<span class="co">#&gt; (Intercept)   0.43878    0.04907   8.943  &lt; 2e-16 ***</span>
<span class="co">#&gt; S2TimePoint1 -0.13985    0.01352 -10.345  &lt; 2e-16 ***</span>
<span class="co">#&gt; S2TimePoint3  0.06486    0.01329   4.881 1.06e-06 ***</span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Approximate significance of smooth terms:</span>
<span class="co">#&gt;                          edf  Ref.df      F  p-value    </span>
<span class="co">#&gt; s(Time)                7.038   7.988 11.018 1.16e-15 ***</span>
<span class="co">#&gt; s(Time):S2TimePoint1   1.001   1.001  0.387 0.534636    </span>
<span class="co">#&gt; s(Time):S2TimePoint3   3.739   4.623  4.909 0.000323 ***</span>
<span class="co">#&gt; s(Time,R)            867.572 974.000 15.750  &lt; 2e-16 ***</span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; R-sq.(adj) =  0.379   Deviance explained = 39.7%</span>
<span class="co">#&gt; fREML =  42860  Scale est. = 0.85001   n = 30976</span></code></pre></div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-base">
<p>R Core Team. (2018). <em>R: A language and environment for statistical computing</em>. Vienna, Austria: R Foundation for Statistical Computing. Retrieved from <a href="https://www.R-project.org/" class="uri">https://www.R-project.org/</a></p>
</div>
<div id="ref-R-rstanarm">
<p>Gabry, J., &amp; Goodrich, B. (2018). <em>RStanARM: Bayesian applied regression modeling via Stan</em>. Retrieved from <a href="https://CRAN.R-project.org/package=rstanarm" class="uri">https://CRAN.R-project.org/package=rstanarm</a></p>
</div>
<div id="ref-Mirman2014">
<p>Mirman, D. (2014). <em>Growth curve analysis and visualization using R</em>. Boca Raton, FL: CRC Press/Taylor &amp; Francis Group.</p>
</div>
<div id="ref-Gelman2014">
<p>Gelman, A., &amp; Carlin, J. (2014). Beyond power calculations: Assessing Type S (sign) and Type M (magnitude) errors. <em>Perspectives on Psychological Science</em>, <em>9</em>(6), 641–651. doi:<a href="https://doi.org/10.1177/1745691614551642">10.1177/1745691614551642</a></p>
</div>
<div id="ref-BDA3">
<p>Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2014). <em>Bayesian data analysis</em> (3rd ed.). Boca Raton, FL: CRC Press/Taylor &amp; Francis Group.</p>
</div>
<div id="ref-StanManual">
<p>Stan Development Team. (2017). <em>Stan modeling language: User’s guide and reference manual</em>.</p>
</div>
<div id="ref-bayesplot">
<p>Gabry, J., &amp; Mahr, T. (2018). <em>bayesplot: Plotting for Bayesian models</em>. Retrieved from <a href="https://CRAN.R-project.org/package=bayesplot" class="uri">https://CRAN.R-project.org/package=bayesplot</a></p>
</div>
<div id="ref-Wood2017">
<p>Wood, S. N. (2017). <em>Generalized additive models: An introduction with R</em> (2nd ed.). Boca Raton, FL: CRC Press/Taylor &amp; Francis Group.</p>
</div>
<div id="ref-itsadug">
<p>van Rij, J., Wieling, M., Baayen, R. H., &amp; van Rijn, H. (2017). itsadug: Interpreting time series and autocorrelated data using GAMMs.</p>
</div>
<div id="ref-Winter2016">
<p>Winter, B., &amp; Wieling, M. (2016). How to analyze linguistic change using mixed models, growth curve analysis and generalized additive modeling. <em>Journal of Language Evolution</em>, <em>1</em>(1), 7–18. doi:<a href="https://doi.org/10.1093/jole/lzv003">10.1093/jole/lzv003</a></p>
</div>
<div id="ref-Baayen2016">
<p>Baayen, R. H., Rij, J. van, Cat, C. de, &amp; Wood, S. N. (2016). Autocorrelated errors in experimental data in the language sciences: Some solutions offered by generalized additive mixed models. Retrieved from <a href="http://arxiv.org/abs/1601.02043" class="uri">http://arxiv.org/abs/1601.02043</a></p>
</div>
</div>
<script type="text/javascript">
  $("div.page-wrapper > h3").appendTo(".page-inner > section");
  $("div.page-wrapper > #refs").appendTo(".page-inner > section");
  $("div.page-wrapper > .footnotes").appendTo(".page-inner > section");
  $("div.body-inner > h3").appendTo(".page-inner > section");
  $("div.body-inner > #refs").appendTo(".page-inner > section");
  $("div.body-inner > .footnotes").appendTo(".page-inner > section");
</script>
            </section>

          </div>
        </div>
      </div>
<a href="vw-experiment-items.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mp-experiment-items.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["dissertation.pdf"],
"toc": {
"collapse": "subsection"
},
"split_bib": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
