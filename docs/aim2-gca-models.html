<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Development of word recognition in preschoolers</title>
  <meta name="description" content="Development of word recognition in preschoolers">
  <meta name="generator" content="bookdown 0.7.12 and GitBook 2.6.7">

  <meta property="og:title" content="Development of word recognition in preschoolers" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="assets/cover.png" />
  
  <meta name="github-repo" content="tjmahr/dissertation" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Development of word recognition in preschoolers" />
  
  
  <meta name="twitter:image" content="assets/cover.png" />

<meta name="author" content="Tristan Mahr">


<meta name="date" content="2018-07-24">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="mp-experiment-items.html">
<link rel="next" href="aim2-mp-items.html">
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="assets\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="dedication.html"><a href="dedication.html"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="specific-aims.html"><a href="specific-aims.html"><i class="fa fa-check"></i><b>1</b> Specific aims</a><ul>
<li class="chapter" data-level="1.1" data-path="specific-aims.html"><a href="specific-aims.html#specific-aim-1-familiar-word-recognition-and-lexical-competition"><i class="fa fa-check"></i><b>1.1</b> Specific Aim 1 (Familiar Word Recognition and Lexical Competition)</a></li>
<li class="chapter" data-level="1.2" data-path="specific-aims.html"><a href="specific-aims.html#specific-aim-2-referent-selection-and-mispronunciations"><i class="fa fa-check"></i><b>1.2</b> Specific Aim 2 (Referent Selection and Mispronunciations)</a></li>
<li class="chapter" data-level="1.3" data-path="specific-aims.html"><a href="specific-aims.html#summary"><i class="fa fa-check"></i><b>1.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="research-hypotheses.html"><a href="research-hypotheses.html"><i class="fa fa-check"></i><b>2</b> Research hypotheses</a><ul>
<li class="chapter" data-level="2.1" data-path="research-hypotheses.html"><a href="research-hypotheses.html#specific-aim-1-familiar-word-recognition-and-lexical-competition-1"><i class="fa fa-check"></i><b>2.1</b> Specific Aim 1 (Familiar Word Recognition and Lexical Competition)</a></li>
<li class="chapter" data-level="2.2" data-path="research-hypotheses.html"><a href="research-hypotheses.html#specific-aim-2-referent-selection-and-mispronunciations-1"><i class="fa fa-check"></i><b>2.2</b> Specific Aim 2 (Referent Selection and Mispronunciations)</a></li>
</ul></li>
<li class="part"><span><b>Aim 1: Familiar Word Recognition and Lexical Competition</b></span></li>
<li class="chapter" data-level="3" data-path="aim1-introduction.html"><a href="aim1-introduction.html"><i class="fa fa-check"></i><b>3</b> Familiar word recognition</a><ul>
<li class="chapter" data-level="3.1" data-path="aim1-introduction.html"><a href="aim1-introduction.html#lexical-processing-dynamics"><i class="fa fa-check"></i><b>3.1</b> Lexical processing dynamics</a></li>
<li class="chapter" data-level="3.2" data-path="aim1-introduction.html"><a href="aim1-introduction.html#individual-differences-in-word-recognition"><i class="fa fa-check"></i><b>3.2</b> Individual differences in word recognition</a></li>
<li class="chapter" data-level="3.3" data-path="aim1-introduction.html"><a href="aim1-introduction.html#the-current-study"><i class="fa fa-check"></i><b>3.3</b> The current study</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="aim1-method.html"><a href="aim1-method.html"><i class="fa fa-check"></i><b>4</b> Method</a><ul>
<li class="chapter" data-level="4.1" data-path="aim1-method.html"><a href="aim1-method.html#aim1-participants"><i class="fa fa-check"></i><b>4.1</b> Participants</a></li>
<li class="chapter" data-level="4.2" data-path="aim1-method.html"><a href="aim1-method.html#aim1-procedure"><i class="fa fa-check"></i><b>4.2</b> Visual World Paradigm</a></li>
<li class="chapter" data-level="4.3" data-path="aim1-method.html"><a href="aim1-method.html#experiment-administration"><i class="fa fa-check"></i><b>4.3</b> Experiment administration</a></li>
<li class="chapter" data-level="4.4" data-path="aim1-method.html"><a href="aim1-method.html#stimuli"><i class="fa fa-check"></i><b>4.4</b> Stimuli</a></li>
<li class="chapter" data-level="4.5" data-path="aim1-method.html"><a href="aim1-method.html#data-screening"><i class="fa fa-check"></i><b>4.5</b> Data screening</a></li>
<li class="chapter" data-level="4.6" data-path="aim1-method.html"><a href="aim1-method.html#model-preparation"><i class="fa fa-check"></i><b>4.6</b> Model preparation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="fam-rec.html"><a href="fam-rec.html"><i class="fa fa-check"></i><b>5</b> Analysis of familiar word recognition</a><ul>
<li class="chapter" data-level="5.1" data-path="fam-rec.html"><a href="fam-rec.html#growth-curve-analysis"><i class="fa fa-check"></i><b>5.1</b> Growth curve analysis</a><ul>
<li class="chapter" data-level="5.1.1" data-path="fam-rec.html"><a href="fam-rec.html#growth-curve-features-as-measures-of-word-recognition-performance"><i class="fa fa-check"></i><b>5.1.1</b> Growth curve features as measures of word recognition performance</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="fam-rec.html"><a href="fam-rec.html#year-over-year-changes-in-word-recognition-performance"><i class="fa fa-check"></i><b>5.2</b> Year over year changes in word recognition performance</a></li>
<li class="chapter" data-level="5.3" data-path="fam-rec.html"><a href="fam-rec.html#exploring-plausible-ranges-of-performance-over-time"><i class="fa fa-check"></i><b>5.3</b> Exploring plausible ranges of performance over time</a></li>
<li class="chapter" data-level="5.4" data-path="fam-rec.html"><a href="fam-rec.html#are-individual-differences-stable-over-time"><i class="fa fa-check"></i><b>5.4</b> Are individual differences stable over time?</a></li>
<li class="chapter" data-level="5.5" data-path="fam-rec.html"><a href="fam-rec.html#predicting-future-vocabulary-size"><i class="fa fa-check"></i><b>5.5</b> Predicting future vocabulary size</a></li>
<li class="chapter" data-level="5.6" data-path="fam-rec.html"><a href="fam-rec.html#discussion"><i class="fa fa-check"></i><b>5.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lex-competitors.html"><a href="lex-competitors.html"><i class="fa fa-check"></i><b>6</b> Effects of phonological and semantic competitors</a><ul>
<li class="chapter" data-level="6.1" data-path="lex-competitors.html"><a href="lex-competitors.html#looks-to-the-phonological-competitor"><i class="fa fa-check"></i><b>6.1</b> Looks to the phonological competitor</a></li>
<li class="chapter" data-level="6.2" data-path="lex-competitors.html"><a href="lex-competitors.html#looks-to-the-semantic-competitor"><i class="fa fa-check"></i><b>6.2</b> Looks to the semantic competitor</a></li>
<li class="chapter" data-level="6.3" data-path="lex-competitors.html"><a href="lex-competitors.html#child-level-differences-in-competitor-sensitivity-at-age-3"><i class="fa fa-check"></i><b>6.3</b> Child-level differences in competitor sensitivity at age 3</a></li>
<li class="chapter" data-level="6.4" data-path="lex-competitors.html"><a href="lex-competitors.html#discussion-1"><i class="fa fa-check"></i><b>6.4</b> Discussion</a><ul>
<li class="chapter" data-level="6.4.1" data-path="lex-competitors.html"><a href="lex-competitors.html#immediate-activation-of-phonological-neighbors"><i class="fa fa-check"></i><b>6.4.1</b> Immediate activation of phonological neighbors</a></li>
<li class="chapter" data-level="6.4.2" data-path="lex-competitors.html"><a href="lex-competitors.html#late-activation-of-semantic-neighbors"><i class="fa fa-check"></i><b>6.4.2</b> Late activation of semantic neighbors</a></li>
<li class="chapter" data-level="6.4.3" data-path="lex-competitors.html"><a href="lex-competitors.html#lexical-competitors-and-child-level-predictors"><i class="fa fa-check"></i><b>6.4.3</b> Lexical competitors and child-level predictors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="aim1-discussion.html"><a href="aim1-discussion.html"><i class="fa fa-check"></i><b>7</b> General discussion</a><ul>
<li class="chapter" data-level="7.1" data-path="aim1-discussion.html"><a href="aim1-discussion.html#how-to-improve-word-recognition"><i class="fa fa-check"></i><b>7.1</b> How to improve word recognition</a></li>
<li class="chapter" data-level="7.2" data-path="aim1-discussion.html"><a href="aim1-discussion.html#learn-words-and-learn-connections-between-words"><i class="fa fa-check"></i><b>7.2</b> Learn words and learn connections between words</a></li>
<li class="chapter" data-level="7.3" data-path="aim1-discussion.html"><a href="aim1-discussion.html#individual-differences-are-most-important-at-younger-ages"><i class="fa fa-check"></i><b>7.3</b> Individual differences are most important at younger ages</a></li>
<li class="chapter" data-level="7.4" data-path="aim1-discussion.html"><a href="aim1-discussion.html#limitations-and-implications"><i class="fa fa-check"></i><b>7.4</b> Limitations and implications</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="aim1-h-check.html"><a href="aim1-h-check.html"><i class="fa fa-check"></i><b>8</b> Hypothesis check (Specific Aim 1)</a></li>
<li class="part"><span><b>Aim 2: Referent Selection and Mispronunciations</b></span></li>
<li class="chapter" data-level="9" data-path="aim2-method.html"><a href="aim2-method.html"><i class="fa fa-check"></i><b>9</b> Method</a><ul>
<li class="chapter" data-level="9.1" data-path="aim2-method.html"><a href="aim2-method.html#mispronunciation-task"><i class="fa fa-check"></i><b>9.1</b> Mispronunciation task</a></li>
<li class="chapter" data-level="9.2" data-path="aim2-method.html"><a href="aim2-method.html#visual-stimuli"><i class="fa fa-check"></i><b>9.2</b> Visual stimuli</a></li>
<li class="chapter" data-level="9.3" data-path="aim2-method.html"><a href="aim2-method.html#novel-word-retention-tests"><i class="fa fa-check"></i><b>9.3</b> Novel word retention tests</a></li>
<li class="chapter" data-level="9.4" data-path="aim2-method.html"><a href="aim2-method.html#aim2-screening"><i class="fa fa-check"></i><b>9.4</b> Data screening</a><ul>
<li class="chapter" data-level="9.4.1" data-path="aim2-method.html"><a href="aim2-method.html#classifying-trials-based-on-initial-fixation-location"><i class="fa fa-check"></i><b>9.4.1</b> Classifying trials based on initial fixation location</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="aim2-method.html"><a href="aim2-method.html#model-preparation-1"><i class="fa fa-check"></i><b>9.5</b> Model preparation</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="real-nonword-selection.html"><a href="real-nonword-selection.html"><i class="fa fa-check"></i><b>10</b> Development of referent selection</a><ul>
<li class="chapter" data-level="10.1" data-path="real-nonword-selection.html"><a href="real-nonword-selection.html#nonwords-versus-familiar-words"><i class="fa fa-check"></i><b>10.1</b> Nonwords versus familiar words</a></li>
<li class="chapter" data-level="10.2" data-path="real-nonword-selection.html"><a href="real-nonword-selection.html#does-age-3-referent-selection-better-predict-age-5-vocabulary"><i class="fa fa-check"></i><b>10.2</b> Does age 3 referent selection better predict age 5 vocabulary?</a></li>
<li class="chapter" data-level="10.3" data-path="real-nonword-selection.html"><a href="real-nonword-selection.html#discussion-2"><i class="fa fa-check"></i><b>10.3</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html"><i class="fa fa-check"></i><b>11</b> Sensitivity to mispronunciations</a><ul>
<li class="chapter" data-level="11.1" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#unfamiliar-initial-trials-move-along-now"><i class="fa fa-check"></i><b>11.1</b> Unfamiliar-initial trials: Move along now</a><ul>
<li class="chapter" data-level="11.1.1" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#child-level-predictors"><i class="fa fa-check"></i><b>11.1.1</b> Child-level predictors</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#familiar-initial-trials-should-i-stay-or-should-i-go"><i class="fa fa-check"></i><b>11.2</b> Familiar-initial trials: Should I stay or should I go?</a><ul>
<li class="chapter" data-level="11.2.1" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#child-level-predictors-and-different-listening-behaviors"><i class="fa fa-check"></i><b>11.2.1</b> Child-level predictors and different listening behaviors</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#looking-behaviors-and-word-learning"><i class="fa fa-check"></i><b>11.3</b> Looking behaviors and word learning</a></li>
<li class="chapter" data-level="11.4" data-path="sensitivity-to-mispronunciations.html"><a href="sensitivity-to-mispronunciations.html#discussion-3"><i class="fa fa-check"></i><b>11.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="aim2-discussion.html"><a href="aim2-discussion.html"><i class="fa fa-check"></i><b>12</b> General discussion</a><ul>
<li class="chapter" data-level="12.1" data-path="aim2-discussion.html"><a href="aim2-discussion.html#a-lexical-processing-account-of-the-findings"><i class="fa fa-check"></i><b>12.1</b> A lexical processing account of the findings</a></li>
<li class="chapter" data-level="12.2" data-path="aim2-discussion.html"><a href="aim2-discussion.html#limitations-and-implications-1"><i class="fa fa-check"></i><b>12.2</b> Limitations and implications</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="aim2-h-check.html"><a href="aim2-h-check.html"><i class="fa fa-check"></i><b>13</b> Hypothesis check (Specific Aim 2)</a></li>
<li class="chapter" data-level="14" data-path="part-afterword.html"><a href="part-afterword.html"><i class="fa fa-check"></i><b>14</b> (PART*) Afterword</a></li>
<li class="chapter" data-level="15" data-path="a-needle-in-a-self-organizing-haystack.html"><a href="a-needle-in-a-self-organizing-haystack.html"><i class="fa fa-check"></i><b>15</b> A needle in a self-organizing haystack</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="vw-experiment-items.html"><a href="vw-experiment-items.html"><i class="fa fa-check"></i><b>A</b> Items used in the visual world experiment</a></li>
<li class="chapter" data-level="B" data-path="aim1-gca-models.html"><a href="aim1-gca-models.html"><i class="fa fa-check"></i><b>B</b> Computational details for Specific Aim 1</a><ul>
<li class="chapter" data-level="B.1" data-path="aim1-gca-models.html"><a href="aim1-gca-models.html#growth-curve-analyses"><i class="fa fa-check"></i><b>B.1</b> Growth curve analyses</a></li>
<li class="chapter" data-level="B.2" data-path="aim1-gca-models.html"><a href="aim1-gca-models.html#generalized-additive-models"><i class="fa fa-check"></i><b>B.2</b> Generalized additive models</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="mp-experiment-items.html"><a href="mp-experiment-items.html"><i class="fa fa-check"></i><b>C</b> Items used in the mispronunciation experiment</a></li>
<li class="chapter" data-level="D" data-path="aim2-gca-models.html"><a href="aim2-gca-models.html"><i class="fa fa-check"></i><b>D</b> Computational details for Specific Aim 2</a><ul>
<li class="chapter" data-level="D.1" data-path="aim2-gca-models.html"><a href="aim2-gca-models.html#real-words-versus-nonwords-growth-curves"><i class="fa fa-check"></i><b>D.1</b> Real words versus nonwords growth curves</a></li>
<li class="chapter" data-level="D.2" data-path="aim2-gca-models.html"><a href="aim2-gca-models.html#mispronunication-growth-curves"><i class="fa fa-check"></i><b>D.2</b> Mispronunication growth curves</a></li>
<li class="chapter" data-level="D.3" data-path="aim2-gca-models.html"><a href="aim2-gca-models.html#item-response-analysis-for-novel-word-retention"><i class="fa fa-check"></i><b>D.3</b> Item-response analysis for novel word retention</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="aim2-mp-items.html"><a href="aim2-mp-items.html"><i class="fa fa-check"></i><b>E</b> Effects of specific mispronunciations</a></li>
<li class="chapter" data-level="F" data-path="related-work.html"><a href="related-work.html"><i class="fa fa-check"></i><b>F</b> Related work</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Created with bookdown</a></li>
<li><a href="https://tjmahr.github.io/" target="blank">Tristan Mahr</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Development of word recognition in preschoolers</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="aim2-gca-models" class="section level1">
<h1><span class="header-section-number">D</span> Computational details for Specific Aim 2</h1>
<div id="real-words-versus-nonwords-growth-curves" class="section level2">
<h2><span class="header-section-number">D.1</span> Real words versus nonwords growth curves</h2>
<p>These models were fit in R <span class="citation">(vers. 3.5.0; R Core Team, <a href="#ref-R-base">2018</a>)</span> with the brms package <span class="citation">(vers. 2.3.1; Bürkner, <a href="#ref-brms">2017</a>)</span>.</p>
<p>The orthogonal polynomial features for Time, they were scaled as in Specific Aim 1, so that the linear feature ranged from −.5 to .5. Under this scaling a unit change in Time<sup>1</sup> was equal to change from the start to the end of the analysis window.</p>
<p>The model formula used to specify the model with brms is printed below. The variables <code>ot1</code>, <code>ot2</code>, and <code>ot3</code> are the polynomial time features, <code>ResearchID</code> identifies children, and <code>Condition</code> identifies the experimental condition (either, the nonword or real word condition). <code>Target</code> counts the number of looks to the target image at each time bin; <code>trials()</code> is a flag that tells brms the number of trials for the binomial process. Here, it is the variable <code>Trials</code>, which is equal to the number of looks to target and distractor in each bin. The syntax <code>(1 + ot1 + ot2 + ot3) * Condition</code> specifies a Time x Condition interaction; it says to estimate an intercept and the three time feature effects for each condition. The line <code>(ot1 + ot2 + ot3 | ResearchID/Condition)</code> describes the random-effect structure of the model with the <code>/</code> indicating that data from each <code>Condition</code> is nested within each <code>ResearchID</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(brms)

<span class="co"># Fit a hierarchical logistic regression model</span>
formula &lt;-<span class="st"> </span><span class="kw">bf</span>(
  Target <span class="op">|</span><span class="st"> </span><span class="kw">trials</span>(Trials) <span class="op">~</span><span class="st"> </span>
<span class="st">    </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>ot1 <span class="op">+</span><span class="st"> </span>ot2 <span class="op">+</span><span class="st"> </span>ot3) <span class="op">*</span><span class="st"> </span>Condition <span class="op">+</span><span class="st"> </span>
<span class="st">    </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>ot1 <span class="op">+</span><span class="st"> </span>ot2 <span class="op">+</span><span class="st"> </span>ot3 <span class="op">|</span><span class="st"> </span>ResearchID<span class="op">/</span>Condition), 
  <span class="dt">family =</span> binomial)</code></pre></div>
<p>The priors for the model are described below. The regression effects (<code>class = &quot;b&quot;</code>) have a moderately informative prior of Normal(0, 1). Because most of the action in the growth curves is a sharp rise, the linear time effect <code>ot1</code> has a slightly wider prior of Normal(0, 2). A weakly informative LKJ(2) prior is put on the random-effect correlations. I review the role of the LKJ prior in<br />
<a href="aim1-gca-models.html#aim1-gca-models">Appendix <a href="aim1-gca-models.html#aim1-gca-models">B</a></a>. A weakly informative prior is put on the random-effect standard deviations Student-<em>t</em>([df] 7, [mean] 0, [sd] 3). The Student-<em>t</em> distribution is like the normal distribution but it provides slightly thicker tails which allow extreme or outlying values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">priors &lt;-<span class="st"> </span><span class="kw">c</span>(
  <span class="co"># Population-average intercept</span>
  <span class="kw">set_prior</span>(<span class="dt">class =</span> <span class="st">&quot;Intercept&quot;</span>, <span class="st">&quot;normal(0, 1)&quot;</span>),
  <span class="co"># Population-average slopes</span>
  <span class="kw">set_prior</span>(<span class="dt">class =</span> <span class="st">&quot;b&quot;</span>, <span class="st">&quot;normal(0, 1)&quot;</span>),
  <span class="co"># ... expect somewhat larger range of effects for linear time</span>
  <span class="kw">set_prior</span>(<span class="dt">class =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">coef =</span> <span class="st">&quot;ot1&quot;</span>, <span class="st">&quot;normal(0, 2)&quot;</span>),
  <span class="co"># Correlations for random effect terms</span>
  <span class="kw">set_prior</span>(<span class="dt">class =</span> <span class="st">&quot;cor&quot;</span>, <span class="st">&quot;lkj(2)&quot;</span>),
  <span class="co"># Standard deviation of the distribution from</span>
  <span class="co"># which random-intercepts are drawn</span>
  <span class="kw">set_prior</span>(<span class="dt">class =</span> <span class="st">&quot;sd&quot;</span>, <span class="st">&quot;student_t(7, 0, 3)&quot;</span>))</code></pre></div>
<p>I originally tried a single model containing all three years with corresponding year effects, year × time interactions, and year × condition × time interactions, but this model took 30 hours to run and did not converge. Therefore, I fit separate models for each year of the study using syntax like the following.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m_age3 &lt;-<span class="st"> </span><span class="kw">brm</span>(
  <span class="dt">formula =</span> formula,
  <span class="dt">data =</span> d_age3,
  <span class="dt">prior =</span> priors,
  <span class="dt">chains =</span> <span class="dv">4</span>,
  <span class="dt">iter =</span> <span class="dv">2000</span>,
  <span class="dt">cores =</span> <span class="dv">4</span>,
  <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> .<span class="dv">99</span>))

<span class="co"># Save the output</span>
readr<span class="op">::</span><span class="kw">write_rds</span>(m_age3, <span class="st">&quot;age3_mp.rds.gz&quot;</span>)</code></pre></div>
<p>This code fits the model using four sampling <code>chains</code> in parallel over four processing <code>cores</code>. Early attempts at the model produced warnings, so I increased the <code>adapt_delta</code> control option to make the sampling more robust and eliminate the warnings.</p>
<p>Model summary for real words versus nonwords at age 3:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span>readr<span class="op">::</span><span class="kw">read_rds</span>(<span class="st">&quot;./data/aim2-real-vs-nw-tp1.rds.gz&quot;</span>)
<span class="kw">summary</span>(m1, <span class="dt">priors =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> .<span class="dv">9</span>)
<span class="co">#&gt;  Family: binomial </span>
<span class="co">#&gt;   Links: mu = logit </span>
<span class="co">#&gt; Formula: Target | trials(Trials) ~ (ot1 + ot2 + ot3) * Condition + (ot1 + ot2 + ot3 | ResearchID/Condition) </span>
<span class="co">#&gt;    Data: test_data_1 (Number of observations: 7450) </span>
<span class="co">#&gt; Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;</span>
<span class="co">#&gt;          total post-warmup samples = 4000</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Priors: </span>
<span class="co">#&gt; b ~ normal(0, 1)</span>
<span class="co">#&gt; b_ot1 ~ normal(0, 2)</span>
<span class="co">#&gt; Intercept ~ normal(0, 1)</span>
<span class="co">#&gt; L ~ lkj_corr_cholesky(2)</span>
<span class="co">#&gt; sd ~ student_t(7, 0, 3)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Group-Level Effects: </span>
<span class="co">#&gt; ~ResearchID (Number of levels: 149) </span>
<span class="co">#&gt;                    Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; sd(Intercept)          0.73      0.13     0.51     0.93        290 1.00</span>
<span class="co">#&gt; sd(ot1)                0.64      0.40     0.05     1.31         68 1.03</span>
<span class="co">#&gt; sd(ot2)                0.34      0.19     0.04     0.65        130 1.02</span>
<span class="co">#&gt; sd(ot3)                0.14      0.10     0.01     0.33        233 1.01</span>
<span class="co">#&gt; cor(Intercept,ot1)     0.25      0.33    -0.36     0.72        176 1.02</span>
<span class="co">#&gt; cor(Intercept,ot2)    -0.26      0.32    -0.73     0.32        838 1.00</span>
<span class="co">#&gt; cor(ot1,ot2)          -0.08      0.37    -0.64     0.56        380 1.01</span>
<span class="co">#&gt; cor(Intercept,ot3)     0.12      0.34    -0.47     0.65       1338 1.00</span>
<span class="co">#&gt; cor(ot1,ot3)           0.06      0.38    -0.57     0.66        740 1.00</span>
<span class="co">#&gt; cor(ot2,ot3)          -0.01      0.37    -0.60     0.61       1226 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; ~ResearchID:Condition (Number of levels: 298) </span>
<span class="co">#&gt;                    Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; sd(Intercept)          1.24      0.08     1.12     1.37        611 1.00</span>
<span class="co">#&gt; sd(ot1)                2.66      0.16     2.40     2.93        320 1.01</span>
<span class="co">#&gt; sd(ot2)                1.40      0.09     1.25     1.55        572 1.00</span>
<span class="co">#&gt; sd(ot3)                0.83      0.05     0.74     0.92        923 1.00</span>
<span class="co">#&gt; cor(Intercept,ot1)     0.29      0.08     0.15     0.41        407 1.01</span>
<span class="co">#&gt; cor(Intercept,ot2)     0.06      0.10    -0.10     0.21        694 1.00</span>
<span class="co">#&gt; cor(ot1,ot2)           0.01      0.09    -0.14     0.15        631 1.00</span>
<span class="co">#&gt; cor(Intercept,ot3)    -0.10      0.09    -0.25     0.06        945 1.00</span>
<span class="co">#&gt; cor(ot1,ot3)          -0.06      0.09    -0.21     0.10       1202 1.00</span>
<span class="co">#&gt; cor(ot2,ot3)          -0.06      0.08    -0.20     0.07       1128 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Population-Level Effects: </span>
<span class="co">#&gt;                   Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; Intercept             0.41      0.12     0.21     0.61       1392 1.00</span>
<span class="co">#&gt; ot1                   4.59      0.24     4.20     4.99        744 1.01</span>
<span class="co">#&gt; ot2                  -1.36      0.13    -1.57    -1.15       1167 1.00</span>
<span class="co">#&gt; ot3                   0.39      0.08     0.25     0.52       1828 1.00</span>
<span class="co">#&gt; Conditionreal        -0.19      0.15    -0.43     0.05       1550 1.00</span>
<span class="co">#&gt; ot1:Conditionreal     0.45      0.31    -0.05     0.94        673 1.01</span>
<span class="co">#&gt; ot2:Conditionreal    -0.02      0.17    -0.30     0.26       1077 1.00</span>
<span class="co">#&gt; ot3:Conditionreal    -0.07      0.11    -0.25     0.12       1674 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample </span>
<span class="co">#&gt; is a crude measure of effective sample size, and Rhat is the potential </span>
<span class="co">#&gt; scale reduction factor on split chains (at convergence, Rhat = 1).</span></code></pre></div>
<p>Model summary for real words versus nonwords at age 4:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m2 &lt;-<span class="st"> </span>readr<span class="op">::</span><span class="kw">read_rds</span>(<span class="st">&quot;./data/aim2-real-vs-nw-tp2.rds.gz&quot;</span>)
<span class="kw">summary</span>(m2, <span class="dt">priors =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> .<span class="dv">9</span>)
<span class="co">#&gt;  Family: binomial </span>
<span class="co">#&gt;   Links: mu = logit </span>
<span class="co">#&gt; Formula: Target | trials(Trials) ~ (ot1 + ot2 + ot3) * Condition + (ot1 + ot2 + ot3 | ResearchID/Condition) </span>
<span class="co">#&gt;    Data: test_data_2 (Number of observations: 7750) </span>
<span class="co">#&gt; Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;</span>
<span class="co">#&gt;          total post-warmup samples = 4000</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Priors: </span>
<span class="co">#&gt; b ~ normal(0, 1)</span>
<span class="co">#&gt; b_ot1 ~ normal(0, 2)</span>
<span class="co">#&gt; Intercept ~ normal(0, 1)</span>
<span class="co">#&gt; L ~ lkj_corr_cholesky(2)</span>
<span class="co">#&gt; sd ~ student_t(7, 0, 3)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Group-Level Effects: </span>
<span class="co">#&gt; ~ResearchID (Number of levels: 155) </span>
<span class="co">#&gt;                    Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; sd(Intercept)          0.64      0.09     0.49     0.79        461 1.01</span>
<span class="co">#&gt; sd(ot1)                0.65      0.35     0.09     1.22         65 1.04</span>
<span class="co">#&gt; sd(ot2)                0.31      0.19     0.03     0.63        155 1.03</span>
<span class="co">#&gt; sd(ot3)                0.15      0.10     0.01     0.34        218 1.01</span>
<span class="co">#&gt; cor(Intercept,ot1)     0.03      0.30    -0.51     0.50        666 1.01</span>
<span class="co">#&gt; cor(Intercept,ot2)    -0.25      0.32    -0.72     0.35        518 1.00</span>
<span class="co">#&gt; cor(ot1,ot2)           0.04      0.36    -0.57     0.62        570 1.00</span>
<span class="co">#&gt; cor(Intercept,ot3)    -0.00      0.34    -0.55     0.57       1435 1.00</span>
<span class="co">#&gt; cor(ot1,ot3)           0.01      0.37    -0.61     0.61        779 1.00</span>
<span class="co">#&gt; cor(ot2,ot3)          -0.02      0.37    -0.61     0.60        599 1.01</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; ~ResearchID:Condition (Number of levels: 310) </span>
<span class="co">#&gt;                    Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; sd(Intercept)          1.02      0.06     0.92     1.12        900 1.00</span>
<span class="co">#&gt; sd(ot1)                2.54      0.16     2.28     2.81        296 1.01</span>
<span class="co">#&gt; sd(ot2)                1.51      0.09     1.37     1.66        538 1.01</span>
<span class="co">#&gt; sd(ot3)                0.87      0.05     0.78     0.96       1082 1.00</span>
<span class="co">#&gt; cor(Intercept,ot1)     0.53      0.06     0.42     0.62        589 1.00</span>
<span class="co">#&gt; cor(Intercept,ot2)    -0.17      0.08    -0.30    -0.03        841 1.01</span>
<span class="co">#&gt; cor(ot1,ot2)           0.10      0.08    -0.04     0.23       1010 1.00</span>
<span class="co">#&gt; cor(Intercept,ot3)    -0.22      0.09    -0.36    -0.08        922 1.00</span>
<span class="co">#&gt; cor(ot1,ot3)          -0.23      0.08    -0.36    -0.09       1372 1.00</span>
<span class="co">#&gt; cor(ot2,ot3)          -0.03      0.08    -0.16     0.10       1171 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Population-Level Effects: </span>
<span class="co">#&gt;                   Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; Intercept             1.32      0.10     1.16     1.49       1613 1.00</span>
<span class="co">#&gt; ot1                   4.57      0.22     4.21     4.94       1397 1.00</span>
<span class="co">#&gt; ot2                  -1.71      0.14    -1.94    -1.49       1201 1.00</span>
<span class="co">#&gt; ot3                   0.41      0.09     0.27     0.55       1662 1.00</span>
<span class="co">#&gt; Conditionreal        -0.82      0.12    -1.01    -0.62       1401 1.00</span>
<span class="co">#&gt; ot1:Conditionreal    -0.51      0.29    -0.96    -0.04       1313 1.00</span>
<span class="co">#&gt; ot2:Conditionreal     0.17      0.18    -0.13     0.47       1149 1.01</span>
<span class="co">#&gt; ot3:Conditionreal    -0.07      0.11    -0.25     0.11       1563 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample </span>
<span class="co">#&gt; is a crude measure of effective sample size, and Rhat is the potential </span>
<span class="co">#&gt; scale reduction factor on split chains (at convergence, Rhat = 1).</span></code></pre></div>
<p>Model summary for real words versus nonwords at age 5:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m3 &lt;-<span class="st"> </span>readr<span class="op">::</span><span class="kw">read_rds</span>(<span class="st">&quot;./data/aim2-real-vs-nw-tp3.rds.gz&quot;</span>)
<span class="kw">summary</span>(m3, <span class="dt">priors =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> .<span class="dv">9</span>)
<span class="co">#&gt;  Family: binomial </span>
<span class="co">#&gt;   Links: mu = logit </span>
<span class="co">#&gt; Formula: Target | trials(Trials) ~ (ot1 + ot2 + ot3) * Condition + (ot1 + ot2 + ot3 | ResearchID/Condition) </span>
<span class="co">#&gt;    Data: test_data_3 (Number of observations: 7550) </span>
<span class="co">#&gt; Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;</span>
<span class="co">#&gt;          total post-warmup samples = 4000</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Priors: </span>
<span class="co">#&gt; b ~ normal(0, 1)</span>
<span class="co">#&gt; b_ot1 ~ normal(0, 2)</span>
<span class="co">#&gt; Intercept ~ normal(0, 1)</span>
<span class="co">#&gt; L ~ lkj_corr_cholesky(2)</span>
<span class="co">#&gt; sd ~ student_t(7, 0, 3)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Group-Level Effects: </span>
<span class="co">#&gt; ~ResearchID (Number of levels: 151) </span>
<span class="co">#&gt;                    Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; sd(Intercept)          0.22      0.15     0.02     0.49         45 1.07</span>
<span class="co">#&gt; sd(ot1)                0.48      0.31     0.05     1.02         52 1.06</span>
<span class="co">#&gt; sd(ot2)                0.26      0.17     0.02     0.57        102 1.03</span>
<span class="co">#&gt; sd(ot3)                0.20      0.11     0.03     0.39        219 1.03</span>
<span class="co">#&gt; cor(Intercept,ot1)     0.21      0.38    -0.47     0.76        135 1.02</span>
<span class="co">#&gt; cor(Intercept,ot2)    -0.16      0.38    -0.72     0.52        241 1.01</span>
<span class="co">#&gt; cor(ot1,ot2)          -0.04      0.37    -0.65     0.59        443 1.01</span>
<span class="co">#&gt; cor(Intercept,ot3)     0.14      0.36    -0.48     0.69        478 1.00</span>
<span class="co">#&gt; cor(ot1,ot3)          -0.03      0.37    -0.63     0.60        739 1.01</span>
<span class="co">#&gt; cor(ot2,ot3)          -0.05      0.37    -0.64     0.57        626 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; ~ResearchID:Condition (Number of levels: 302) </span>
<span class="co">#&gt;                    Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; sd(Intercept)          1.18      0.07     1.07     1.28        210 1.02</span>
<span class="co">#&gt; sd(ot1)                2.78      0.16     2.51     3.05        429 1.01</span>
<span class="co">#&gt; sd(ot2)                1.54      0.09     1.41     1.69        920 1.00</span>
<span class="co">#&gt; sd(ot3)                0.88      0.06     0.79     0.98        771 1.01</span>
<span class="co">#&gt; cor(Intercept,ot1)     0.59      0.05     0.50     0.67        845 1.00</span>
<span class="co">#&gt; cor(Intercept,ot2)    -0.13      0.08    -0.26     0.01        753 1.02</span>
<span class="co">#&gt; cor(ot1,ot2)           0.09      0.08    -0.04     0.23        953 1.01</span>
<span class="co">#&gt; cor(Intercept,ot3)    -0.36      0.08    -0.48    -0.23        860 1.00</span>
<span class="co">#&gt; cor(ot1,ot3)          -0.39      0.08    -0.52    -0.27       1492 1.00</span>
<span class="co">#&gt; cor(ot2,ot3)           0.15      0.08     0.01     0.28       1570 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Population-Level Effects: </span>
<span class="co">#&gt;                   Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; Intercept             1.42      0.10     1.26     1.57        646 1.01</span>
<span class="co">#&gt; ot1                   4.67      0.23     4.30     5.07        602 1.01</span>
<span class="co">#&gt; ot2                  -1.70      0.14    -1.94    -1.47       1228 1.00</span>
<span class="co">#&gt; ot3                   0.28      0.09     0.13     0.42       1598 1.00</span>
<span class="co">#&gt; Conditionreal        -0.48      0.13    -0.70    -0.27        556 1.01</span>
<span class="co">#&gt; ot1:Conditionreal    -0.13      0.30    -0.64     0.37        702 1.00</span>
<span class="co">#&gt; ot2:Conditionreal     0.19      0.19    -0.13     0.49        846 1.01</span>
<span class="co">#&gt; ot3:Conditionreal     0.03      0.12    -0.16     0.22       1242 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample </span>
<span class="co">#&gt; is a crude measure of effective sample size, and Rhat is the potential </span>
<span class="co">#&gt; scale reduction factor on split chains (at convergence, Rhat = 1).</span></code></pre></div>
</div>
<div id="mispronunication-growth-curves" class="section level2">
<h2><span class="header-section-number">D.2</span> Mispronunication growth curves</h2>
<p>Like the models above, these ones are Bayesian mixed-effects logistic regression growth curve models fit with brms. I used two separate models, one for unfamilar-initial trials and familiar-initial trials. Each model included data from all three years of the study. The code is essentially the same syntax with a <code>Study</code> variable replacing the <code>Condition</code> variable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(brms)

<span class="co"># Fit a hierarchical logistic regression model</span>
formula &lt;-<span class="st"> </span><span class="kw">bf</span>(
  Target <span class="op">|</span><span class="st"> </span><span class="kw">trials</span>(Trials) <span class="op">~</span><span class="st"> </span>
<span class="st">    </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>ot1 <span class="op">+</span><span class="st"> </span>ot2 <span class="op">+</span><span class="st"> </span>ot3) <span class="op">*</span><span class="st"> </span>Study <span class="op">+</span><span class="st"> </span>
<span class="st">    </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>ot1 <span class="op">+</span><span class="st"> </span>ot2 <span class="op">+</span><span class="st"> </span>ot3 <span class="op">|</span><span class="st"> </span>ResearchID<span class="op">/</span>Study), 
  <span class="dt">family =</span> binomial)

priors &lt;-<span class="st"> </span><span class="kw">c</span>(
  <span class="kw">set_prior</span>(<span class="dt">class =</span> <span class="st">&quot;Intercept&quot;</span>, <span class="st">&quot;normal(0, 1)&quot;</span>),
  <span class="kw">set_prior</span>(<span class="dt">class =</span> <span class="st">&quot;b&quot;</span>, <span class="st">&quot;normal(0, 1)&quot;</span>),
  <span class="kw">set_prior</span>(<span class="dt">class =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">coef =</span> <span class="st">&quot;ot1&quot;</span>, <span class="st">&quot;normal(0, 2)&quot;</span>),
  <span class="kw">set_prior</span>(<span class="dt">class =</span> <span class="st">&quot;cor&quot;</span>, <span class="st">&quot;lkj(2)&quot;</span>),
  <span class="kw">set_prior</span>(<span class="dt">class =</span> <span class="st">&quot;sd&quot;</span>, <span class="st">&quot;student_t(7, 0, 2)&quot;</span>))

mp_unfam &lt;-<span class="st"> </span><span class="kw">brm</span>(
  <span class="dt">formula =</span> formula,
  <span class="dt">data =</span> d_u,
  <span class="dt">prior =</span> priors,
  <span class="dt">chains =</span> <span class="dv">4</span>,
  <span class="dt">cores =</span> <span class="dv">4</span>,
  <span class="co"># Run extra iterations to get a higher effective sample size</span>
  <span class="dt">iter =</span> <span class="dv">3000</span>,
  <span class="dt">control =</span> <span class="kw">list</span>(
    <span class="dt">adapt_delta =</span> .<span class="dv">995</span>, 
    <span class="dt">max_treedepth =</span> <span class="dv">15</span>))

<span class="co"># Save the output</span>
readr<span class="op">::</span><span class="kw">write_rds</span>(mp_unfam, <span class="st">&quot;./data/aim2-mp-unfam.rds.gz&quot;</span>)

mp_fam &lt;-<span class="st"> </span><span class="kw">brm</span>(
  <span class="dt">formula =</span> formula,
  <span class="dt">data =</span> d_f,
  <span class="dt">prior =</span> priors,
  <span class="dt">chains =</span> <span class="dv">4</span>,
  <span class="dt">cores =</span> <span class="dv">4</span>,
  <span class="dt">control =</span> <span class="kw">list</span>(
    <span class="dt">adapt_delta =</span> .<span class="dv">99</span>, 
    <span class="dt">max_treedepth =</span> <span class="dv">15</span>))

<span class="co"># Save the output</span>
readr<span class="op">::</span><span class="kw">write_rds</span>(mp_fam, <span class="st">&quot;./data/aim2-mp-fam.rds.gz&quot;</span>)</code></pre></div>
<p>The priors for this model are the same, except for a tighter prior on scale/standard deviations for the random effect distributions. The model had difficulty obtaining an effective number of samples for these parameters. Initially, I tried to tell the model to do more work on each sampling step (<code>adapt_delta = .995</code> and <code>max_treedepth = 15</code>) and run the chains for 50% longer (<code>iter = 3000</code>). These changes did not solve the problem. By using a tighter prior, the model had a smaller search space meaning it could obtain samples more efficiently.</p>
<p>The revised prior was still weakly informative. Figure <a href="aim2-gca-models.html#fig:student-t-priors">D.1</a> llustrates the differences in the prior densities—that is, which values are plausible before seeing the data. It also shows posterior densities from the model and how those values are easily enclosed by the prior densities.</p>

<div class="figure" style="text-align: center"><span id="fig:student-t-priors"></span>
<img src="96-app-aim2-models_files/figure-html/student-t-priors-1.png" alt="Prior densities (left) versus posterior densities (right) for the random-effect standard deviations. I changed the prior to be tighter, so that it favor values up to 7.5. This prior still turned out to be very conservative, given that the posterior samples for these values are all less than 3." width="100%" />
<p class="caption">
Figure D.1: Prior densities (left) versus posterior densities (right) for the random-effect standard deviations. I changed the prior to be tighter, so that it favor values up to 7.5. This prior still turned out to be very conservative, given that the posterior samples for these values are all less than 3.
</p>
</div>
<p>Model summary for unfamiliar-initial mispronunciation trials:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mp_unfam, <span class="dt">priors =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> .<span class="dv">9</span>)
<span class="co">#&gt;  Family: binomial </span>
<span class="co">#&gt;   Links: mu = logit </span>
<span class="co">#&gt; Formula: Target | trials(Trials) ~ (1 + ot1 + ot2 + ot3) * Study + (1 + ot1 + ot2 + ot3 | ResearchID/Study) </span>
<span class="co">#&gt;    Data: d_u (Number of observations: 11875) </span>
<span class="co">#&gt; Samples: 4 chains, each with iter = 3000; warmup = 1500; thin = 1;</span>
<span class="co">#&gt;          total post-warmup samples = 6000</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Priors: </span>
<span class="co">#&gt; b ~ normal(0, 1)</span>
<span class="co">#&gt; b_ot1 ~ normal(0, 2)</span>
<span class="co">#&gt; Intercept ~ normal(0, 1)</span>
<span class="co">#&gt; L ~ lkj_corr_cholesky(2)</span>
<span class="co">#&gt; sd ~ student_t(7, 0, 2)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Group-Level Effects: </span>
<span class="co">#&gt; ~ResearchID (Number of levels: 193) </span>
<span class="co">#&gt;                    Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; sd(Intercept)          0.21      0.11     0.03     0.40         66 1.04</span>
<span class="co">#&gt; sd(ot1)                0.57      0.22     0.16     0.90        111 1.05</span>
<span class="co">#&gt; sd(ot2)                0.21      0.13     0.02     0.44         77 1.05</span>
<span class="co">#&gt; sd(ot3)                0.12      0.08     0.01     0.26        198 1.03</span>
<span class="co">#&gt; cor(Intercept,ot1)    -0.00      0.34    -0.55     0.55        284 1.00</span>
<span class="co">#&gt; cor(Intercept,ot2)     0.04      0.37    -0.59     0.64        432 1.01</span>
<span class="co">#&gt; cor(ot1,ot2)           0.11      0.35    -0.49     0.67        616 1.01</span>
<span class="co">#&gt; cor(Intercept,ot3)    -0.08      0.36    -0.66     0.52        792 1.00</span>
<span class="co">#&gt; cor(ot1,ot3)           0.02      0.35    -0.55     0.60       1213 1.00</span>
<span class="co">#&gt; cor(ot2,ot3)          -0.10      0.36    -0.66     0.53        658 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; ~ResearchID:Study (Number of levels: 475) </span>
<span class="co">#&gt;                    Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; sd(Intercept)          0.96      0.04     0.89     1.02        185 1.01</span>
<span class="co">#&gt; sd(ot1)                2.03      0.09     1.88     2.19        324 1.02</span>
<span class="co">#&gt; sd(ot2)                1.14      0.05     1.05     1.23        388 1.01</span>
<span class="co">#&gt; sd(ot3)                0.65      0.03     0.60     0.71        865 1.00</span>
<span class="co">#&gt; cor(Intercept,ot1)     0.01      0.06    -0.09     0.10        763 1.00</span>
<span class="co">#&gt; cor(Intercept,ot2)     0.10      0.06    -0.00     0.19       1309 1.00</span>
<span class="co">#&gt; cor(ot1,ot2)          -0.12      0.06    -0.22    -0.02        665 1.01</span>
<span class="co">#&gt; cor(Intercept,ot3)     0.01      0.07    -0.10     0.12       2333 1.00</span>
<span class="co">#&gt; cor(ot1,ot3)          -0.16      0.06    -0.27    -0.06       1707 1.00</span>
<span class="co">#&gt; cor(ot2,ot3)          -0.29      0.06    -0.39    -0.19       2053 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Population-Level Effects: </span>
<span class="co">#&gt;                     Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; Intercept              -0.54      0.08    -0.67    -0.41        704 1.00</span>
<span class="co">#&gt; ot1                     3.36      0.17     3.07     3.63       1520 1.00</span>
<span class="co">#&gt; ot2                    -1.02      0.10    -1.20    -0.85       1360 1.00</span>
<span class="co">#&gt; ot3                     0.39      0.06     0.28     0.49       2244 1.00</span>
<span class="co">#&gt; StudyTimePoint2         0.15      0.11    -0.02     0.33        492 1.00</span>
<span class="co">#&gt; StudyTimePoint3         0.45      0.11     0.27     0.63        712 1.00</span>
<span class="co">#&gt; ot1:StudyTimePoint2    -0.28      0.22    -0.65     0.09       1455 1.00</span>
<span class="co">#&gt; ot1:StudyTimePoint3    -0.29      0.23    -0.67     0.09       1408 1.00</span>
<span class="co">#&gt; ot2:StudyTimePoint2     0.03      0.14    -0.20     0.26       1202 1.00</span>
<span class="co">#&gt; ot2:StudyTimePoint3     0.01      0.14    -0.21     0.24       1232 1.01</span>
<span class="co">#&gt; ot3:StudyTimePoint2    -0.11      0.08    -0.25     0.02       2310 1.00</span>
<span class="co">#&gt; ot3:StudyTimePoint3     0.01      0.09    -0.13     0.15       2622 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample </span>
<span class="co">#&gt; is a crude measure of effective sample size, and Rhat is the potential </span>
<span class="co">#&gt; scale reduction factor on split chains (at convergence, Rhat = 1).</span></code></pre></div>
<p>Model summary for familiar-initial mispronunciation trials:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mp_fam, <span class="dt">priors =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> .<span class="dv">9</span>)
<span class="co">#&gt;  Family: binomial </span>
<span class="co">#&gt;   Links: mu = logit </span>
<span class="co">#&gt; Formula: Target | trials(Trials) ~ (1 + ot1 + ot2 + ot3) * Study + (1 + ot1 + ot2 + ot3 | ResearchID/Study) </span>
<span class="co">#&gt;    Data: d_f (Number of observations: 12100) </span>
<span class="co">#&gt; Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;</span>
<span class="co">#&gt;          total post-warmup samples = 4000</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Priors: </span>
<span class="co">#&gt; b ~ normal(0, 1)</span>
<span class="co">#&gt; b_ot1 ~ normal(0, 2)</span>
<span class="co">#&gt; Intercept ~ normal(0, 1)</span>
<span class="co">#&gt; L ~ lkj_corr_cholesky(2)</span>
<span class="co">#&gt; sd ~ student_t(7, 0, 2)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Group-Level Effects: </span>
<span class="co">#&gt; ~ResearchID (Number of levels: 195) </span>
<span class="co">#&gt;                    Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; sd(Intercept)          0.44      0.08     0.31     0.55        171 1.04</span>
<span class="co">#&gt; sd(ot1)                0.73      0.24     0.25     1.06         70 1.05</span>
<span class="co">#&gt; sd(ot2)                0.21      0.10     0.04     0.37         97 1.04</span>
<span class="co">#&gt; sd(ot3)                0.08      0.05     0.01     0.18        240 1.02</span>
<span class="co">#&gt; cor(Intercept,ot1)    -0.01      0.25    -0.42     0.39        403 1.01</span>
<span class="co">#&gt; cor(Intercept,ot2)     0.40      0.30    -0.15     0.81        376 1.01</span>
<span class="co">#&gt; cor(ot1,ot2)          -0.29      0.33    -0.74     0.36        252 1.02</span>
<span class="co">#&gt; cor(Intercept,ot3)     0.13      0.35    -0.48     0.66       1555 1.00</span>
<span class="co">#&gt; cor(ot1,ot3)          -0.18      0.35    -0.71     0.45       1159 1.00</span>
<span class="co">#&gt; cor(ot2,ot3)           0.08      0.36    -0.53     0.66       1358 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; ~ResearchID:Study (Number of levels: 484) </span>
<span class="co">#&gt;                    Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; sd(Intercept)          0.84      0.04     0.79     0.91        434 1.01</span>
<span class="co">#&gt; sd(ot1)                1.95      0.10     1.79     2.11        195 1.01</span>
<span class="co">#&gt; sd(ot2)                1.11      0.05     1.04     1.19        727 1.00</span>
<span class="co">#&gt; sd(ot3)                0.62      0.03     0.57     0.66       1578 1.00</span>
<span class="co">#&gt; cor(Intercept,ot1)    -0.05      0.06    -0.16     0.05        678 1.00</span>
<span class="co">#&gt; cor(Intercept,ot2)    -0.02      0.06    -0.12     0.08        544 1.01</span>
<span class="co">#&gt; cor(ot1,ot2)          -0.21      0.06    -0.30    -0.11        705 1.00</span>
<span class="co">#&gt; cor(Intercept,ot3)    -0.06      0.07    -0.16     0.05       1189 1.00</span>
<span class="co">#&gt; cor(ot1,ot3)          -0.05      0.07    -0.16     0.06       1254 1.00</span>
<span class="co">#&gt; cor(ot2,ot3)          -0.49      0.05    -0.57    -0.40       1595 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Population-Level Effects: </span>
<span class="co">#&gt;                     Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; Intercept               0.76      0.07     0.64     0.88       1660 1.00</span>
<span class="co">#&gt; ot1                    -3.01      0.17    -3.29    -2.74       1502 1.00</span>
<span class="co">#&gt; ot2                     2.00      0.09     1.85     2.15       1286 1.00</span>
<span class="co">#&gt; ot3                    -0.52      0.06    -0.61    -0.42       2008 1.00</span>
<span class="co">#&gt; StudyTimePoint2        -0.23      0.09    -0.39    -0.08       1394 1.00</span>
<span class="co">#&gt; StudyTimePoint3        -0.01      0.09    -0.17     0.14       1651 1.00</span>
<span class="co">#&gt; ot1:StudyTimePoint2     0.56      0.21     0.21     0.92       1489 1.00</span>
<span class="co">#&gt; ot1:StudyTimePoint3     1.30      0.21     0.94     1.64       1508 1.00</span>
<span class="co">#&gt; ot2:StudyTimePoint2    -0.01      0.13    -0.23     0.20       1198 1.00</span>
<span class="co">#&gt; ot2:StudyTimePoint3    -0.26      0.13    -0.48    -0.03       1179 1.00</span>
<span class="co">#&gt; ot3:StudyTimePoint2    -0.11      0.08    -0.24     0.02       2008 1.00</span>
<span class="co">#&gt; ot3:StudyTimePoint3    -0.10      0.08    -0.23     0.03       1939 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample </span>
<span class="co">#&gt; is a crude measure of effective sample size, and Rhat is the potential </span>
<span class="co">#&gt; scale reduction factor on split chains (at convergence, Rhat = 1).</span></code></pre></div>
</div>
<div id="item-response-analysis-for-novel-word-retention" class="section level2">
<h2><span class="header-section-number">D.3</span> Item-response analysis for novel word retention</h2>
<p>This is an item-response analysis carried out using a Bayesian mixed-effects logistic regression model. These models were fit in R <span class="citation">(vers. 3.5.0; R Core Team, <a href="#ref-R-base">2018</a>)</span> with the brms package <span class="citation">(vers. 2.3.1; Bürkner, <a href="#ref-brms">2017</a>)</span>. I used weakly informative priors.</p>
<p>The linear model <code>Correct ~ ItemType * c_peak_10</code> says to estimate the log-odds of answering correctly using on mispronunciations (intercept) using the growth curve peaks (<code>c_peak_10</code>), adjust it for nonwords (<code>ItemType</code>) and for the nonword × peak interaction. The peaks were mean-centered within each type and multiplied by 10 so that the slope represents the effect of change of .1 from the mean peak value. The model includes four random intercepts: general child ability and child × condition ability adjustments (simultaneously requested using <code>1 | ResearchID/ItemType</code>, i.e., <code>ResearchID</code> levels have <code>ItemType</code> levels nested under <code>/</code> them), plus specific item-level difficulties (<code>1 | Item</code>) and item-pair level difficulties (<code>1 | WordGroup</code>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span>readr<span class="op">::</span><span class="kw">read_csv</span>(<span class="st">&quot;./data/mp-norming-data.csv.gz&quot;</span>)

priors &lt;-<span class="st"> </span><span class="kw">c</span>(
  <span class="co"># Population-average intercept</span>
  <span class="kw">set_prior</span>(<span class="dt">class =</span> <span class="st">&quot;Intercept&quot;</span>, <span class="st">&quot;normal(0, 1)&quot;</span>),
  <span class="co"># Population-average slopes</span>
  <span class="kw">set_prior</span>(<span class="dt">class =</span> <span class="st">&quot;b&quot;</span>, <span class="st">&quot;normal(0, 1)&quot;</span>),
  <span class="co"># Standard deviation of the distribution from</span>
  <span class="co"># which random-intercepts are drawn</span>
  <span class="kw">set_prior</span>(<span class="dt">class =</span> <span class="st">&quot;sd&quot;</span>, <span class="st">&quot;student_t(7, 0, 2)&quot;</span>))

m_norm &lt;-<span class="st"> </span><span class="kw">brm</span>(
  Correct <span class="op">~</span><span class="st"> </span>ItemType <span class="op">*</span><span class="st"> </span>c_peak_<span class="dv">10</span> <span class="op">+</span>
<span class="st">    </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>ResearchID<span class="op">/</span>ItemType) <span class="op">+</span>
<span class="st">    </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>WordGroup) <span class="op">+</span>
<span class="st">    </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Item),
  <span class="dt">prior =</span> priors,
  <span class="dt">family =</span> bernoulli,
  <span class="dt">chains =</span> <span class="dv">4</span>,
  <span class="dt">iter =</span> <span class="dv">2000</span>,
  <span class="dt">cores =</span> <span class="dv">4</span>,
  <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> .<span class="dv">99</span>),
  <span class="dt">data =</span> d)

readr<span class="op">::</span><span class="kw">write_rds</span>(m_norm, <span class="st">&quot;./data/mp-norming-m2.rds.gz&quot;</span>) </code></pre></div>
<p>Model summary for retention trials at age 5:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m_norm, <span class="dt">priors =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> .<span class="dv">9</span>)
<span class="co">#&gt;  Family: bernoulli </span>
<span class="co">#&gt;   Links: mu = logit </span>
<span class="co">#&gt; Formula: Correct ~ ItemType * c_peak_10 + (1 | ResearchID/ItemType) + (1 | WordGroup) + (1 | Item) </span>
<span class="co">#&gt;    Data: d (Number of observations: 1200) </span>
<span class="co">#&gt; Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;</span>
<span class="co">#&gt;          total post-warmup samples = 4000</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Priors: </span>
<span class="co">#&gt; b ~ normal(0, 1)</span>
<span class="co">#&gt; Intercept ~ normal(0, 1)</span>
<span class="co">#&gt; sd ~ student_t(7, 0, 2)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Group-Level Effects: </span>
<span class="co">#&gt; ~Item (Number of levels: 12) </span>
<span class="co">#&gt;               Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; sd(Intercept)     0.67      0.22     0.37     1.08       1468 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; ~ResearchID (Number of levels: 101) </span>
<span class="co">#&gt;               Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; sd(Intercept)     0.34      0.17     0.05     0.62        454 1.01</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; ~ResearchID:ItemType (Number of levels: 200) </span>
<span class="co">#&gt;               Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; sd(Intercept)     0.47      0.19     0.11     0.76        419 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; ~WordGroup (Number of levels: 6) </span>
<span class="co">#&gt;               Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; sd(Intercept)     0.46      0.36     0.05     1.11       1152 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Population-Level Effects: </span>
<span class="co">#&gt;                           Estimate Est.Error l-90% CI u-90% CI Eff.Sample Rhat</span>
<span class="co">#&gt; Intercept                     0.57      0.37    -0.03     1.16       1992 1.00</span>
<span class="co">#&gt; ItemTypenonword               0.90      0.40     0.22     1.54       2180 1.00</span>
<span class="co">#&gt; c_peak_10                    -0.12      0.06    -0.22    -0.02       4000 1.00</span>
<span class="co">#&gt; ItemTypenonword:c_peak_10     0.04      0.14    -0.20     0.28       4000 1.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample </span>
<span class="co">#&gt; is a crude measure of effective sample size, and Rhat is the potential </span>
<span class="co">#&gt; scale reduction factor on split chains (at convergence, Rhat = 1).</span></code></pre></div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-base">
<p>R Core Team. (2018). <em>R: A language and environment for statistical computing</em>. Vienna, Austria: R Foundation for Statistical Computing. Retrieved from <a href="https://www.R-project.org/" class="uri">https://www.R-project.org/</a></p>
</div>
<div id="ref-brms">
<p>Bürkner, P.-C. (2017). brms: An R package for Bayesian multilevel models using Stan. <em>Journal of Statistical Software</em>, <em>80</em>(1), 1–28. doi:<a href="https://doi.org/10.18637/jss.v080.i01">10.18637/jss.v080.i01</a></p>
</div>
</div>
<script type="text/javascript">
  $("div.page-wrapper > h3").appendTo(".page-inner > section");
  $("div.page-wrapper > #refs").appendTo(".page-inner > section");
  $("div.page-wrapper > .footnotes").appendTo(".page-inner > section");
  $("div.body-inner > h3").appendTo(".page-inner > section");
  $("div.body-inner > #refs").appendTo(".page-inner > section");
  $("div.body-inner > .footnotes").appendTo(".page-inner > section");
</script>
            </section>

          </div>
        </div>
      </div>
<a href="mp-experiment-items.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="aim2-mp-items.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"split_bib": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
