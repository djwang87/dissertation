
Method {#aim1-method}
=======================================================================









Participants
-----------------------------------------------------------------------

The participants were 28–39 months-old at Time 1, 39–52 at Time 2,
and 51–65 at Time 3. Approximately, 180 children participated at
Time 1, 170 at Time 2, and 160 at Time 3. Of these children,
approximately 20 were identified by their parents as late talkers.
Prospective families were interviewed over telephone before
participating in the study. Children were not scheduled for testing if a
parent reported language problems, vision problems, developmental
delays, or an individualized education program for the child.
Recruitment and data collection occurred at two Learning to Talk lab
sites—one at the University of Wisconsin–Madison and the other at the
University of Minnesota.

[^1]: [Appendix \@ref(related-work)](#related-work) describes how this 
      dissertation relates to other work from our lab.

Table \@ref(tab:participant-info) summarizes the cohort of children in
each year of testing. The numbers and summary statistics here are
general, describing children who participated at each year, but whose
data may have been excluded from the analyses. Some potential reasons
for exclusion include: excessive missing data during eyetracking,
experiment or technology error, developmental concerns not identified
until later in study, or a failed hearing screening. Final sample sizes
will depend on the measures needed for an analysis and the results from
data screening checks. For each project aim, I will disclose all
measurements and data exclusions following guidelines by the Center for
Open Science [@OSF_Statement].

                                          Year 1 (Age 3)     Year 2 (Age 4)     Year 3 (Age 5)
----------------------------------------- ------------------ ------------------ ------------------
N                                         184                175                160
Boys, Girls                               94, 90             89, 86             82, 78
Maternal education: Low, Middle, High     15, 98, 71         12, 92, 71         6, 90, 64
Dialect: MAE, AAE                         171, 13            163, 12            153, 7
Parent-identified late talkers            20                 19                 16
Age (months): Mean (SD) [Range]           33 (3) [28–39]     45 (4) [39–52]     57 (4) [51–66]
EVT-2 standard score: Mean (SD)           115 (18)           118 (16)           118 (14)
PPVT-4 standard score: Mean (SD)          113 (17)           120 (16)           —
GFTA-2 standard score: Mean (SD)          92 (13)            —                  91 (13)

Table: (\#tab:participant-info) Participant characteristics. Education
levels: *Low*: less than high school, or high school; *Middle*: trade
school, technical or associates degree, some college, or college degree;
and *High*: graduate degree.

### Special case data screening

(_Skip for now._ This is where I review the participant notes and will remove 
children who have to be excluded for other reasons, like being diagnosed with a 
language disorder at TimePoint 3.)



Procedure
-----------------------------------------------------------------------

This experiment used a version of the Visual World Paradigm for word recognition
experiments [@RWLPaper]. In eyetracking studies with toddlers, two familiar
images are usually presented: a target and a distractor. This experiment is a
four-image eyetracking task that was designed to provide a more demanding word
recognition task for preschoolers. In this procedure, four familiar images are
presented onscreen followed by a prompt to view one of the images (e.g., 
*find the bell!*). The four images include the target word (e.g., *bell*), a
semantically related word (*drum*), a phonologically similar word (*bee*), and
an unrelated word (*swing*). Figure \@ref(fig:sample-vw-screen) shows an example
of a trial's items. This procedure measures a child’s real-time comprehension of
words by capturing how the child’s gaze location changes over time in response
to speech.

(ref:sample-vw-screen-cap2) Example display for the target *bell* with
the semantic foil *drum*, the phonological foil *bee*, and the unrelated
*swing*.

<div class="figure">
<img src="./misc/rwl-screens/TimePoint1/actual/Block2_17_swing2_bell2_bee2_drum2_UpperRightImage_bell.png" alt="(ref:sample-vw-screen-cap2)" width="100%" />
<p class="caption">(\#fig:sample-vw-screen)(ref:sample-vw-screen-cap2)</p>
</div>



Experiment Administration
-----------------------------------------------------------------------

Children participating in the study were tested over two lab visits
(i.e., on different dates). The first portion of each visit involved
“watching movies”—that is, performing two blocks of eyetracking
experiments. A play break or hearing screening occurred between the two
eyetracking blocks, depending on the visit.

Each eyetracking experiment was administered as a block of trials (24 for this
experiment and 38 for a two-image task---see chapter X). Children received two
different blocks of each experiment. The blocks for an experiment differed in
trial ordering and other features. Experiment order and block selection were
counterbalanced over children and visits. For example, a child might have
received Exp. 1 Block A and Exp. 2 Block B on Visit 1 and next received Exp. 2
Block A and Exp. 1 Block B on Visit 2. The purpose of this presentation was to
control possible ordering effects where a particular experiment or block
benefited from consistently occurring first or second.

Experiments were administered using E-Prime 2.0 and a Tobii T60XL
eyetracker which recorded gaze location at a rate of 60 Hz. The
experiments were conducted by two examiners, one “behind the scenes” who
controlled the computer running the experiment and another “onstage” who
guided the child through the experiment. At the beginning of each block,
the child was positioned so the child’s eyes were approximately 60 cm
from the screen. The examiners calibrated the eyetracker to the child’s
eyes using a five-point calibration procedure (center of screen and
centers of four screen quadrants). The examiners repeated this
calibration procedure if one of the five calibration points for one of
the eyes did not calibrate successfully. During the experiment, the
behind-the-scenes examiner monitored the child’s distance from the
screen and whether the eyetracker was capturing the child’s gaze. The
onstage examiner coached the child to stay fixated on the screen and
repositioned the child as needed to ensure the child’s eyes were being
tracked. Every six or seven trials in a block of an experiment, the
experiment briefly paused with a reinforcing animation or activity.
During these breaks, the onstage examiner could reposition the child if
necessary before resuming the experiment.

We used a gaze-contingent stimulus presentation. First, the images appeared in
silence on screen for 2 s as a familiarization period. The experiment then
checked whether the child's gaze was being recorded. If the experiment could
continuously track the child's gaze for 300 ms, the child's gaze was verified
and the trial continued. If the experiment could not verify the gaze after 10 s,
the trial continued. This procedure guaranteed that for most trials, the child
was looking to the display before presenting the carrier phrase and that the
experiment was ready to record the child's response to the carrier. During
Year 1 and Year 2, an attention-getter (e.g., *check it out*!) played 1 s
following the end of the target noun. These reinforcers were dropped in Year 3
to streamline the experiment for older listeners.



Stimuli
-----------------------------------------------------------------------

*A few sentences to reiterate what the four kinds of images represented*. The
four images on each trial consisted of a target noun, a phonological foil, a
semantic foil, and an unrelated word. A complete list of the items used in the
experiment in [Appendix \@ref(vw-experiment-items)](#vw-experiment-items).

The stimuli were recorded in both Mainstream American English (MAE) and African
American English (AAE), so that the experiment could accommodate the child's
home dialect. Prior to the lab visit, we made a preliminary guess about the
child's home dialect, based on the recruitment channel, address, among other
factors. If we expected the dialect to be AAE, then the lab visit was led by an
examiner who natively spoke AAE and could fluently dialect-shift between AAE and
MAE. At the beginning of the lab visit, the examiner listened to the
interactions between the child and caregiver in order to confirm the child's
home dialect. Prompts to view the target image of a trial (e.g., *find the
girl*) used the carrier phrases “find the” and “see the”. These carriers were
recording in the frame “find/see the egg” and cross-spliced with the target
nouns to minimize coarticulatory cues on the determiner “the”.

The images used in the experiment consisted of color photographs on gray
backgrounds. These images were piloted with 30 children from two preschool
classrooms to ensure that children consistently used the same label for familiar
objects. The two preschool classrooms differed in their students' SES
demographics: One classroom (13 piloting students) was part of a university
research center which predominantly serves higher-SES families, and the other
classroom (17 piloting students) was part of Head Start center which
predominantly serves lower-SES families. The images were tested by presenting
four images (a target, a phonological foil, a semantic foil and an unrelated
word) and having the student point to the named image. The pictures had to be
recognized by at least 80% of students in each classroom.

Data screening
-----------------------------------------------------------------------





To process the eyetracking data, we first mapped gaze *x*-*y* coordinates onto
the onscreen images. We next performed *deblinking*. We interpolated short runs
of missing gaze data (up to 150 ms) if the same image was fixated before and
after the missing data run. Put differently, we classified a window of missing
data as a blink if the window was brief and the gaze remained on the same image
before and after the blink. We interpolated missing data from blinks using the
fixated image.

After mapping the gaze coordinates onto the onscreen images, we performed data
screening. We considered the time window from 0 to
2000 ms after target noun onset. We identified a trial
as _unreliable_ if at least 50% of the looks
were missing during the time window. We excluded an entire block of trials if it
had fewer than 12 reliable trials. 





Table \@ref(tab:screening-counts) shows the numbers of participants and trials
at each timepoint before and after data screening. There were more children in
the second timepoint than the first timepoint due to a timing error in the
initial version of this experiment, leading to the exclusion of
27 participants from the first timepoint.


Table: (\#tab:screening-counts)Eyetracking data before and after data screening. For convenience, the number of exclusions are included as Raw - Screened.

Dataset    Study         N Children   N Blocks   N Trials
---------  -----------  -----------  ---------  ---------
Raw        TimePoint1           178        332       7967
           TimePoint2           180        347       8327
           TimePoint3           163        322       7724
Screened   TimePoint1           163        291       5951
           TimePoint2           165        305       6421
           TimePoint3           156        295       6483
NA         TimePoint1            15         41       2016
           TimePoint2            15         42       1906
           TimePoint3             7         27       1241





Prepare the dataset for modeling
-----------------------------------------------------------------------



To prepare the data for modeling, we downsampled the data into
50-ms (3-frame) bins, reducing the
eyetracker's effective sampling rate to 20 Hz. Eye
movements have durations on the order of 100 or 200 ms, so capturing data
every 16.67 ms oversamples eye movements and can introduce high-frequency noise
into the signal. Binning together data from neighboring frames can smooth out
this noise. We modeled the looks from 250 to
1500 ms. Lastly, we aggregated looks by child, study and
time, and created orthogonal polynomials to use as time features for the model.





<img src="11-aim1-notebook_files/figure-html/spaghetti-elogit-1.png" width="100%" />









