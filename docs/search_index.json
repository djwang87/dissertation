[
["index.html", "My dissertation Development of word recognition in preschoolers Updates", " My dissertation Tristan Mahr 2018-01-26 Development of word recognition in preschoolers This book, when finished, will contain my dissertation research. Updates 2017-08-18: I migrated what I wrote for my dissertation proposal. In doing so, I worked all the kinks required to generate a nice web version, a lovely pdf version, and a… uh… functional Word document version. Last compiled: 2018-01-26 15:22:18 "],
["scratch-paper.html", "Chapter 1 Scratch paper 1.1 Bookdown cheatsheet 1.2 Debug info", " Chapter 1 Scratch paper This book is made with bookdown, an R package/tool-chain for creating a books in multiple formats. This chapter is just a placeholder section and some scratch-paper so that I have some examples on-hand of how to use bookdown’s syntax and features. This is a book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). For now, you have to install the development versions of bookdown from Github: devtools::install_github(&quot;rstudio/bookdown&quot;) Code settings: library(methods) knitr::opts_chunk$set( tidy = FALSE, collapse = TRUE, comment = &quot;#&gt;&quot;, out.width = 80 ) options(width = 80) 1.1 Bookdown cheatsheet 1.1.1 Cross-references to sections The headings above were written with the following markdown: ## Bookdown cheatsheet ### Cross-references to sections {#manual-section-label-demo} The first one gets an implicit label. The second one has an explicit section label. I can refer to Section \\@ref(bookdown-cheatsheet) and [link to it](#bookdown-cheatsheet) with its implicit label. I can refer to Section \\@ref(manual-section-label-demo) and [link to it](#manual-section-label-demo) with its explicit label. I can refer to Section 1.1 and link to it with its implicit label. I can refer to Section 1.1.1 and link to it with its explicit label. 1.1.2 Cross-references to appendices The sample principles apply to appendices. This is a reference to [an appendix](#mp-experiment-items) The number of that appendix \\@ref(mp-experiment-items). I hope. Both: See [Appendix \\@ref(mp-experiment-items)](#mp-experiment-items) This is a reference to an appendix The number of that appendix B. I hope. Both: See Appendix B 1.1.3 Cross-references to tables The chunk label `table-single` provides an implicit label for Table \\@ref(tab:table-single). ```{r table-single, echo = FALSE} knitr::kable( head(mtcars[, 1:5], 5), booktabs = TRUE, caption = &#39;A table of the first 5 rows of the mtcars data.&#39; ) ``` The chunk label table-single provides an implicit label for Table 1.1. Table 1.1: A table of the first 5 rows of the mtcars data. mpg cyl disp hp drat Mazda RX4 21.0 6 160 110 3.90 Mazda RX4 Wag 21.0 6 160 110 3.90 Datsun 710 22.8 4 108 93 3.85 Hornet 4 Drive 21.4 6 258 110 3.08 Hornet Sportabout 18.7 8 360 175 3.15 1.1.4 Figure references and using text references as captions The caption for Figure \\@ref(fig:cat) is defined as a _text reference_ below and passed to the `fig.cap` chunk option. (ref:cat-cap) This is a happy cat. ```{r cat, fig.cap = &quot;(ref:cat-cap)&quot;, out.width = &quot;30%&quot;, fig.show = &quot;hold&quot;} knitr::include_graphics( rep(&quot;./misc/happy-cat-grooming-itself-vector-file.png&quot;, 2) ) ``` The caption for Figure 1.1 is defined as a text reference below and passed to the fig.cap chunk option. knitr::include_graphics( rep(&quot;./misc/happy-cat-grooming-itself-vector-file.png&quot;, 2) ) Figure 1.1: This is a happy cat. 1.2 Debug info str(list(html = is_html_output(), latex = is_latex_output(), word = is_word_output(), width = options(&quot;width&quot;)[[1]])) #&gt; List of 4 #&gt; $ html : logi TRUE #&gt; $ latex: logi FALSE #&gt; $ word : logi FALSE #&gt; $ width: int 80 devtools::session_info() #&gt; Session info ------------------------------------------------------------------ #&gt; setting value #&gt; version R version 3.4.3 (2017-11-30) #&gt; system x86_64, mingw32 #&gt; ui RTerm #&gt; language (EN) #&gt; collate English_United States.1252 #&gt; tz America/Chicago #&gt; date 2018-01-26 #&gt; Packages ---------------------------------------------------------------------- #&gt; package * version date source #&gt; backports 1.1.2 2017-12-13 CRAN (R 3.4.3) #&gt; base * 3.4.3 2017-11-30 local #&gt; bookdown 0.6 2018-01-25 CRAN (R 3.4.3) #&gt; compiler 3.4.3 2017-11-30 local #&gt; datasets * 3.4.3 2017-11-30 local #&gt; devtools 1.13.4 2017-11-09 CRAN (R 3.4.1) #&gt; digest 0.6.14 2018-01-14 CRAN (R 3.4.3) #&gt; evaluate 0.10.1 2017-06-24 CRAN (R 3.4.1) #&gt; graphics * 3.4.3 2017-11-30 local #&gt; grDevices * 3.4.3 2017-11-30 local #&gt; highr 0.6 2016-05-09 CRAN (R 3.2.3) #&gt; htmltools 0.3.6 2017-04-28 CRAN (R 3.4.0) #&gt; knitr 1.18 2017-12-27 CRAN (R 3.4.3) #&gt; magrittr 1.5 2014-11-22 CRAN (R 3.1.2) #&gt; memoise 1.1.0 2017-04-21 CRAN (R 3.3.2) #&gt; methods * 3.4.3 2017-11-30 local #&gt; parallel 3.4.3 2017-11-30 local #&gt; Rcpp 0.12.15 2018-01-20 CRAN (R 3.4.3) #&gt; rmarkdown 1.8 2017-11-17 CRAN (R 3.4.1) #&gt; rprojroot 1.3-2 2018-01-03 CRAN (R 3.4.3) #&gt; stats * 3.4.3 2017-11-30 local #&gt; stringi 1.1.6 2017-11-17 CRAN (R 3.4.2) #&gt; stringr 1.2.0 2017-02-18 CRAN (R 3.3.2) #&gt; tools 3.4.3 2017-11-30 local #&gt; utils * 3.4.3 2017-11-30 local #&gt; withr 2.1.1 2017-12-19 CRAN (R 3.4.3) #&gt; xfun 0.1 2018-01-22 CRAN (R 3.4.3) #&gt; yaml 2.1.16 2017-12-12 CRAN (R 3.4.3) last_four_commits &lt;- git2r::commits(git2r::repository(&quot;.&quot;), n = 4) msgs &lt;- lapply(last_four_commits, methods::show) #&gt; [6e3937f] 2018-01-26: move another specification part #&gt; [17614ff] 2018-01-26: clean up synthetic participants section #&gt; [07e7838] 2018-01-26: flesh out curve averages #&gt; [cf6bd64] 2018-01-26: add peaks to correlations section Built with love using R (Version 3.4.3; R Core Team, 2017) and the R-packages bayesplot (Version 1.4.0.9000; Gabry &amp; Mahr, 2017), bookdown (Version 0.6; Xie, 2018), dplyr (Version 0.7.4; Wickham, Francois, Henry, &amp; Müller, 2017), ggplot2 (Version 2.2.1; Wickham &amp; Chang, 2016), knitr (Version 1.18; Xie, 2017), littlelisteners (Version 0.0.0.9000; Mahr, 2017), lme4 (Version 1.1.15; Bates, Maechler, Bolker, &amp; Walker, 2017), rlang (Version 0.1.6; Henry &amp; Wickham, 2017), rmarkdown (Version 1.8; Allaire et al., 2017), rstanarm (Version 2.17.2; Gabry &amp; Goodrich, 2017), tjmisc (Version 0.0.0.9000; T. Mahr, 2017a), and tristan (Version 0.0.0.9000; T. Mahr, 2017b). References "],
["front-matter.html", "Chapter 2 Front Matter", " Chapter 2 Front Matter About This Document This document outlines the research questions, data, and methods for my dissertation. This proposal started out as a grant-writing project, so it has some of the touchstones of NIH F31 grant (Specific Aims, Significance, Approach), but these sections have been expanded considerably. Date of oral presentation of dissertation proposal: April 3, 2017. Dissertation Committee Members Jan Edwards, primary mentor and chair, Department of Hearing and Speech Sciences, University of Maryland Susan Ellis Weismer, official advisor at UW-Madison, Department of Communication Sciences and Disorders Margarita Kaushanskaya, Department of Communication Sciences and Disorders Audra Sterling, Department of Communication Sciences and Disorders David Kaplan, Department of Educational Psychology Bob McMurray, Department of Psychological and Brain Sciences, University of Iowa Planned Dissertation Format Three thematically related manuscripts, one for each specific aim, to be completed by Summer 2018. "],
["specific-aims.html", "Chapter 3 Specific Aims 3.1 Specific Aim 1 (Familiar Word Recognition and Lexical Competition) 3.2 Specific Aim 2 (Referent Selection and Mispronunciations) 3.3 Specific Aim 3 (Computational Modeling) 3.4 Summary", " Chapter 3 Specific Aims Individual differences in language ability are apparent as soon as children start talking, but it is difficult to identify children at risk for language delay or disorder. Recent work suggests word recognition efficiency—that is, how well children map incoming speech to words—may help identify early differences in children’s language trajectories. Children learn spoken language by listening to caregivers, so children who are faster at recognizing words have an advantage for word learning. This view is borne out by some studies suggesting that children who are faster at processing words show greater vocabulary gains months later (e.g., Weisleder &amp; Fernald, 2013). We do not know, however, how word recognition itself develops over time within a child. This is an important open question because word recognition may provide a key mechanism for understanding how individual differences emerge in word learning and persist into early language development. Without a developmental account of word recognition, we lack the context for understanding individual differences in lexical processing. Thus, even the big-picture questions are unclear: Do early differences persist over time so that faster processors remain relatively fast later in childhood? Or, is such a question ill-posed because the magnitude of the differences among children shrink with age? I plan to address this gap in knowledge by analyzing three years of word recognition data collected in recently completed longitudinal study of 180 children. In particular, I will examine the development of familiar word recognition, lexical competition, and fast referent selection (the ability to map novel words to novel objects in the moment). Through these analyses, I will develop a fine-grained description of how the dynamics of word recognition change year over year, and I will study how differences in word recognition performance relate to child-level measures (such as vocabulary and speech perception). I will complement these empirical analyses with computational cognitive models. With these models, I will simulate the word recognition data from each year and study how the models need to change to adapt to children’s developing word recognition abilities. These simulations can identify plausible psychological mechanisms that underlie changes in word recognition behavior. 3.1 Specific Aim 1 (Familiar Word Recognition and Lexical Competition) To characterize the development of familiar word recognition and lexical competition, I will analyze data from a visual world paradigm experiment, conducted at age 3, age 4, and age 5. In these eyetracking experiments, children were presented with four images of familiar objects and heard a prompt to view one of the images. The four images included a target word (e.g., bell), a semantically related word (drum), a phonologically similar word (bee), and an unrelated word (swing). I will use a series of growth curve analyses to describe how children’s familiar word recognition develop year over year. Of interest is how individual differences at Year 1 persist into Year 3. I will also analyze how expressive vocabulary and lexical processing develop together over time. Lastly, I will examine the children’s looks to the distractors to study the developmental course of lexical competition from similar sounding and similar meaning words. Changes in sensitivity to competing words can reveal how lexical competition emerges as a byproduct of learning new words. 3.2 Specific Aim 2 (Referent Selection and Mispronunciations) To characterize how fast referent selection develops longitudinally, I will analyze data from a looking-while-listening mispronunciation experiment, conducted at age 3, age 4, and age 5. In these eyetracking experiments (Law &amp; Edwards, 2015; based on White &amp; Morgan, 2008), children saw an image of a familiar object and an unfamiliar object, and they heard either a correct production of the familiar object (e.g., soup), a one-feature mispronunciation of the familiar object (shoop), or a novel word unrelated to either image (cheem). The correct productions test familiar word recognition and the nonwords test fast referent selection. The mispronunciations test a child’s phonological categories (whether the child permits, rejects, or equivocates about mispronunciations). I will use growth curve analyses to study how children’s responses to the three word types change over time. I will examine familiar word recognition and fast referent selection to determine which feature of lexical processing better predicts vocabulary growth. I plan to examine dissociations or asymmetries in these forms of processing within children as a way to empirically assess the claim that “novel word processing (referent selection) is not distinct from familiar word recognition” (McMurray, Horst, &amp; Samuelson, 2012). Finally, I will examine how individual differences in vocabulary and speech perception predict responses to mispronunciations and novel words. 3.3 Specific Aim 3 (Computational Modeling) To identify plausible psychological mechanisms underlying the development of word recognition, I will simulate the word recognition data using cognitive computation models. The TRACE model of word recognition (McClelland &amp; Elman, 1986) has been used to simulate word recognition data from adults (Allopenna, Magnuson, &amp; Tanenhaus, 1998), adults with aphasia (Mirman, Yee, Blumstein, &amp; Magnuson, 2011), toddlers (Mayor &amp; Plunkett, 2014), and adolescents with language impairments (McMurray, Samelson, Lee, &amp; Tomblin, 2010). In this model, incoming acoustic information activates perceptual units which in turn activate phoneme units which in turn activate word units. Connections are interactive, so the model can accommodate top-down processing effects and competition among units through inhibition. The model is controlled by psychological parameters like inhibition strength, activation decay rates, and lexicon size and composition. The advantage of using this model is how one can map different behavioral patterns onto changes in model parameterizations to develop a plausible psychological account of developmental changes. I will simulate the Year 1 word recognition data in TRACE, and I will study how the model’s parameters need to change in order to accommodate data from Year 2 and Year 3. I will examine how different model parameters map onto individual differences in word recognition. For instance, under what modeling conditions are mispronunciations of familiar words accepted and do these conditions correspond to child-level differences in vocabulary or speech perception? These simulations will provide a psychological account for the developmental trends and individual variation in word recognition. 3.4 Summary This project investigates how word recognition develops during the preschool years. There has been no research studying word recognition longitudinally after age two. Findings will show how individual differences in lexical processing change over time and can reveal how low-level mechanisms underlying word recognition mature longitudinally in children. These findings will have translational value by studying processing abilities that subserve word learning and by assessing the predictive relationships between early word recognition ability and later language outcomes. References "],
["significance.html", "Chapter 4 Significance 4.1 Public Health Significance 4.2 Scientific Significance", " Chapter 4 Significance 4.1 Public Health Significance Vocabulary size in preschool is a robust predictor of later language development, and early language skills predict early literacy skills at school entry (P. L. Morgan, Farkas, Hillemeier, Hammer, &amp; Maczuga, 2015). By studying the mechanisms that shape word learning, we can understand how individual differences in language ability arise and identify strategies for closing language gaps between children. Word recognition—the process of mapping incoming speech sounds to known or novel words—has been shown to predict later language outcomes. We do not know how this ability develops over time, and we do not know when word recognition is most predictive of future outcomes. This project will provide an integrated account of how word recognition and its relationship with vocabulary size change from age 3 to age 5. 4.2 Scientific Significance 4.2.1 Lexical Processing Dynamics Mature listeners recognize words by continuously evaluating incoming speech input for possible word matches through lexical competition. The first part of a word activates multiple candidate words in parallel, and these candidates compete so that the best-fitting word is recognized. For example, the onset “bee” might activate the candidates bee, beam, beetle, beak, beaker, beginning, and so on, but an additional “m” would narrow the candidates to just beam. Semantic relationships also influence lexical processing, and cascading phonological-semantic effects—e.g., where castle activates the phonologically similar candy which in turn activates the semantically related sweet—have been demonstrated (Marslen-Wilson &amp; Zwitserlood, 1989). Both low-level phonetic cues and high-level grammatical, semantic and pragmatic information can influence this process, but the continuous processing of multiple competing candidates is the essential dynamic underlying word recognition (Magnuson, Mirman, &amp; Myers, 2013). What about young children who know considerably fewer words? Eyetracking studies with toddlers have suggested a developmental continuity between toddlers and adult listeners. Children recognize words incrementally (Swingley, Pinto, &amp; Fernald, 1999), match truncated words to their intended referents (Fernald, Swingley, &amp; Pinto, 2001), and use information from neighboring words in a sentence to facilitate word recognition. This information can be grammatical: Lew-Williams and Fernald (2007) found that Spanish-acquiring preschoolers can use grammatical gender on determiners (el or la) to anticipate the word named in a two-object word recognition task. The information can also be subcategorical phonetic variation: We found that English-acquiring toddlers look earlier to a named image when the coarticulatory formant cues on word the predict the noun of the sentence, compared to tokens with neutral coarticulation (Mahr, McMillan, Saffran, Ellis Weismer, &amp; Edwards, 2015). There is some evidence for lexical competition where children are sensitive to phonological and semantic similarities among words. Ellis Weismer, Haebig, Edwards, Saffran, and Venker (2016) showed that toddlers (14–29 months old) looked less reliably to a named image when the onscreen competitor was a semantically related word or perceptually similar image. In Law, Mahr, Schneeberg, and Edwards (2016), preschoolers (28–60 months old) demonstrated sensitivity to semantic and phonological competitors in a four-image eyetracking task. Huang and Snedeker (2011) presented evidence of cascading semantic-phonological activation in five-year-olds such that for a target word like log, children looked more to an indirect phonological competitor like key (competing through its activation of lock) than they looked to an unrelated image like carrot. In contrast to these studies which all demonstrate interference from similar words, Mani and Plunkett (2010) demonstrated cross-modal phonological priming effects in 18-month-olds. In this study, a picture of prime word (e.g., cat or teeth) was presented in silence; then two images (e.g., cup and shoe) were presented, one of which was named (cup). Children on average looked more to the target word (like cup) when it was primed by an image of a phonological neighbor (like cat), and the children performed at chance when the prime was not related to the named word. Mani, Durrant, and Floccia (2012) found a similar result for cascading phonological-semantic priming with 24-month-olds: Children looked more to a target shoe compared to a distractor door when primed by an image of clock, assumed to activate sock which primed shoe. The above studies involved young children of different ages tested under different procedures, sometimes in different dialects and languages. Averaging these results together, so to speak, the studies suggest that early word recognition demonstrates some hallmarks of adult behavior: Continuous processing of words, integration of information from different levels of representation, and the influence of similar, unspoken words on recognition of a word. Nevertheless, we only have a fragmented view of how familiar word recognition and lexical competition develop within children. One open question is how lexical competition develops within children. For example, do phonological similar words exert more interference during word recognition as children grow older? As a guiding hypothesis, we can think of word learning as a gradual process where familiarity with a word moves from shallow receptive knowledge to deeper expressive knowledge. In adult listeners, words compete and inhibit one another, so that a word is truly “learned” and integrated into the lexicon when it can influence the processing of other words (a line of reasoning reviewed by Kapnoula, Packard, Gupta, &amp; McMurray, 2015). Increasing sensitivity to similar sounding words over time would reveal that children improve their ability to consider multiple candidates in parallel. By studying how sensitivity to similar-sounding and similar-meaning words develop over time and within ever-growing vocabularies, this project can reveal how children come to process words efficiently. Another avenue for studying word recognition is to examine how listeners respond to unfamiliar or novel stimuli. A productive line of research has found that children are sensitive to mispronunciations during word recognition (e.g., Swingley &amp; Aslin, 2000, 2002). White and Morgan (2008) presented toddlers with images of a familiar and novel object, and children heard a correct production of the familiar object, mispronunciations of the familiar object of varying severity, or an unrelated nonword. Toddlers looked less to a familiar word when the first segment was mispronounced. Moreover, they demonstrated graded sensitivity such that a 1-feature mispronunciation yielded more looks to an image than a 2-feature mispronunciation, and a 2-feature mispronunciation yielded more looks than a 3-feature one. Finally, in the nonword condition, the children looked more to the novel object than the familiar one, demonstrating fast referent selection as they associated novel words to novel objects in the moment. A similar pattern of effects was observed in the mispronunciation study by Law and Edwards (2015) with preschoolers mapping real words to familiar objects, nonwords to novel objects, and equivocating about mispronunciations of familiar words. As with lexical competition, it is unclear how children’s responses to mispronunciations and novel words change over time or how individual differences among children change over time. For example, do children become more forgiving of mispronunciations as they mature and learn more words? We might expect so, as children become more experienced at listening to noisy, degraded, or misspoken speech. Another open question involves the development of fast referent selection. At face value, we might expect a child’s ability to associate new words with unfamiliar objects to be more direct measure of word-learning capacity than a child’s ability to process known words. Under this assumption, we would expect individual differences in fast referent selection to be highly correlated with vocabulary growth. But McMurray et al. (2012) propose that the same basic process is at play in both recognition of familiar words and fast association of nonwords. In experiments, the observed behaviors are the same: Children hear a word and direct their attention to an appropriate referent. This project can tackle these questions by describing how mispronunciations are processed as children grow older and by examining whether familiar word recognition and fast referent selection dissociate and which one is a better predictor of vocabulary growth. 4.2.2 Individual Differences in Word Recognition We have a rough understanding of the development of word recognition, and these gaps in knowledge matter because young children differ in their word recognition abilities. These differences are usually measured using accuracy (a probability of recognizing to a word) or efficiency (a reaction time or some measure of how quickly accuracy changes over time). These differences are consequential too, as word recognition differences correlate with other language measures concurrently, retrospectively, and prospectively. The best predictor of lexical processing efficiency is concurrent vocabulary size: Children who know more words look more quickly and reliably to a named word (e.g., Law &amp; Edwards, 2015). This fact deserves a brief reflection: Suppose the information processing mechanism behind word recognition were just a naïve table search. Then this finding is somewhat puzzling: Children with larger lexicons have to find a needle in a larger haystack—yet this apparent liability is an advantage. That is why the search analogy is naïve. One explanation follows from the earlier described idea about graded word learning: Children become better at recognizing words as they learn more words because they extract regularities and discover similarities among words and develop more efficient lexical representations—the haystack develops regularity and becomes easier to search. Although it is a robust predictor of word recognition, vocabulary size is nonspecific. For lexical processing dynamics, vocabulary size can be considered an indicator for the organization and efficiency of a child’s lexicon, but it also correlates with other (meaningful) differences. Vocabulary is related to differences in speech perception (Cristia, Seidl, Junge, Soderstrom, &amp; Hagoort, 2014) and environmental factors like language input (e.g., Hart &amp; Risley, 1995; Hoff, 2003). For instance, measures of speech perception at 6–8 months predict vocabulary size at 24 months (Kuhl et al., 2008; e.g., Tsao, Liu, &amp; Kuhl, 2004), so processing predicts future vocabulary predicts concurrent processing. A related complication is the apparent predictive validity of word recognition measures. Marchman and Fernald (2008) found that vocabulary size and lexical processing efficiency at age 2 jointly predicted working memory scores and expressive language scores at age 8. This result would suggest domain-general processing advantages influence word learning. Fernald and Marchman (2012) also found that late talkers who looked more quickly to a named word at 18 months showed larger gains in vocabulary by 30 months compared to late-talkers who looked more slowly at 18 months. Weisleder and Fernald (2013) found that lexical processing and language input at 19 months predict vocabulary size at 25 months and that lexical processing mediated the effect of language input. Word recognition efficiency and vocabulary size are interconnected measures with concurrent and predictive associations. This project can clarify this relationship by examining the co-development of word recognition, vocabulary size, and speech perception. In particular, I will ask how individual differences in word recognition change over time alongside differences in vocabulary. I can also which features of word recognition (fast referent selection, lexical competition, etc.) are most predictive of vocabulary outcomes at age 5. The additional measures of speech perception can also help clarify the specific effects of vocabulary size on word recognition. 4.2.3 Computational Modeling One way to bolster a theory of word recognition is to implement the theory in a computational model and simulate human behavior with the model. If the model can produce responses like those of human listeners, then the behavioral data support the model and the model’s underlying theory. Models have adjustable parameters, and these generally correspond to plausible psychological constructs like the inhibition strength among competing units or a learning rate for modifying connections. Part of the simulation process therefore is to ask under what conditions a model simulates human behavior, and then interpret the simulations in terms of psychological mechanisms. Here is a hypothetical example of this modeling strategy: Suppose we want to investigate the finding that a larger vocabulary size predicts more efficient word recognition. We would ask under which parameters (i.e., model settings) a model with a large lexicon outperforms one with a smaller lexicon at word recognition. It might be the case that these results occur only when parameters are set so that the representations of speech sounds are comparatively noisier in a model with fewer words. In this scenario, the models provide a plausible psychological interpretation for the empirical findings: Children develop better representations of speech sounds as they learn words, and these better representations enable more efficient word recognition. Granted, this example is just a hypothetical case, but it illustrates how I intend to use computational models as a way to describe word recognition trends and variation in terms of psychological mechanisms and processing parameters. The TRACE model of speech perception and word recognition (McClelland &amp; Elman, 1986) is well suited for this kind of simulation work. TRACE can simulate a dozen or so empirical results from speech perception literature (Strauss, Harris, &amp; Magnuson, 2007, Table 1), and it has been used to simulate word-recognition data from adults (Allopenna et al., 1998), adults with aphasia (Mirman et al., 2011), and toddlers (Mayor &amp; Plunkett, 2014). McMurray et al. (2010) used TRACE to simulate word recognition results from adolescents with specific language impairment (SLI). They mapped certain theories about SLI onto different model parameters. To test the theory that listeners with SLI have impairments in acoustic perception, for example, they varied three of the model’s parameters: amount of noise added to the model’s mock-speech input (acoustic resolution), temporal spread of acoustic features in the input (temporal resolution), and rate of decay in the model’s acoustic feature detectors (perceptual memory). Other theories of SLI (and other model parameters) provided a closer fit to the observed data than the perceptual deficit theory. Specifically, lexical decay—“the ability to maintain words in memory” (p. 23)—was the most important model parameter, implying that individual differences in word recognition for listeners with SLI are rooted in lexical processes, as opposed to perceptual or phonological ones. This example shows how simulations with TRACE recast word recognition performance in terms of psychological processes. For this project, I will use TRACE to describe how cognitive processes and representations need to change to simulate the development of word recognition in preschoolers. 4.2.4 Summary This project studies word recognition in children over three years, so it will provide the first longitudinal study of word recognition in preschoolers. Children in this cohort cover a range of vocabulary scores at Time 1, and this variability allows one to investigate individual differences in vocabulary and word recognition over time and assess the predictive value of these measures. Furthermore, this project studies word recognition in two experimental tasks that can tap into different aspects of word recognition. Specifically, a four-image experiment with semantic and phonological foils allows me to study how lexical competition develops, and a two-image experiment with nonwords and mispronunciations enables me to study how fast referent selection develops over time as well. I will use mixed effects modeling to study not just gross measures of accuracy or interference from distractors, but the time course and lexical dynamics of word recognition using growth curve analyses. These empirical models of the longitudinal word recognition data will be supported by computational models, so that the developmental changes can be described by plausible psychological mechanisms of word recognition. References "],
["research-hypotheses.html", "Chapter 5 Research Hypotheses 5.1 Specific Aim 1 (Familiar Word Recognition and Lexical Competition) 5.2 Specific Aim 2 (Referent Selection and Mispronunciations) 5.3 Specific Aim 3 (Computational Modeling)", " Chapter 5 Research Hypotheses In this section, I outline the main hypotheses I plan to study for each specific aim. This section is intended to preregister the main analyses for this project, so I can cleanly separate confirmatory and exploratory results. 5.1 Specific Aim 1 (Familiar Word Recognition and Lexical Competition) Children’s accuracy and efficiency of recognizing words will improve each year. There are stable individual differences in lexical processing of familiar words such that children who are relatively fast at Year 1 remain relatively fast at Year 2 and Year 3. However, the magnitude of these individual differences diminishes over time, as children converge on a mature level of performance for this paradigm. As a consequence, individual differences in word recognition at age 3, for example, will be more discriminating and predictive of age 5 language outcomes than differences at age 4. Vocabulary size and lexical processing will be tightly correlated such that large year-over-year gains in one measure will predict large year-over-years gains in the other measure. Children will become more sensitive to lexical competitors as they age, based on the hypothesis that children discover similarities among words as a consequence of learning more and more words. Children will differ in their sensitivity to lexical competitors, and these individual differences will correlate with other child-level measures. 5.2 Specific Aim 2 (Referent Selection and Mispronunciations) Children’s accuracy and efficiency of recognizing real words and fast-associating nonwords will improve each year. Performance in real word recognition and fast association of nonwords will be highly correlated, based on the hypothesis that the same process (referent selection) operates in both situations. Under the alternative hypothesis, real word recognition and fast referent selection reflect different skills with different developmental trajectories. Thus, if there is any dissociation between recognition of real words and nonwords, it will be observed in younger children. Although these two measures will be correlated, I predict performance in the nonword condition will be a better predict of future vocabulary growth than performance in the real word condition. This hypothesis is based on the idea that fast referent selection is a more relevant skill for learning new words than recognition of known words. For the mispronunciations, I predict children with larger vocabularies (that is, older children) will be more likely to tolerate a mispronunciation as a production of familiar word compared to children with smaller vocabularies. Mispronunciations that feature later-mastered sounds (e.g., rice/wice) will be more likely to be associated to novel objects than earlier-mastered sounds (duck/guck). 5.3 Specific Aim 3 (Computational Modeling) The research questions for this aim are more exploratory than confirmatory. I plan to use the well-studied TRACE model of spoken word recognition (McClelland &amp; Elman, 1986). The TRACE model has no built-in semantic representations, so it will be used to model data from the two-image mispronunciation experiment (Aim 2) and a “semantic-less” subset of the data from the four-image experiment (Aim 1). Research questions include: What parameterizations of the TRACE model of word recognition account Year 1 word recognition data? If the default model of TRACE represents “default” adult listeners, how the children are Year 1 different than adults. How do these parameterizations have to change year over year to accommodate the data from older children at Year 2 and again at Year 3? What is the developmental story for these changing parameterizations? How does the model handle mispronunciations? What changes are required to accommodate mispronunciations and novel words? Mayor and Plunkett (2014) have simulated mispronunciation experiments like White and Morgan (2008), although it is not clear how the model can accommodate the current mispronunciation experiment which presents mispronunciations and nonwords. How does the model’s vocabulary parameters (namely the size and composition of the lexicon used for the simulations) correlate with the vocabulary size of participants? Can the model simulations incorporate other child-level measures? For example, do children who demonstrate poorer speech perception in a non-eyetracking speech discrimination task require lower levels of phoneme inhibition in their word recognition models? References "],
["methods.html", "Chapter 6 Methods 6.1 General Research Design: Participants 6.2 General Eyetracking Procedure 6.3 Specific Procedure: Aim 1 (Familiar Word Recognition and Lexical Competition) 6.4 Specific Procedure: Aim 2 (Referent Selection and Mispronunciations)", " Chapter 6 Methods 6.1 General Research Design: Participants Word recognition data and vocabulary data, among other measures, were collected over a three-year longitudinal study (R01DC002932; the Learning to Talk project). Children were 28–39 months-old at Time 1, 39–52 at Time 2, and 51–65 at Time 3. Approximately, 180 children participated at Time 1, 170 at Time 2, and 160 at Time 3. Of these children, approximately 20 were identified by their parents as late talkers. Prospective families were interviewed over telephone before participating in the study, and “children with an individualized education program or any parent-reported visual problems, language problems, or developmental delays were not scheduled for testing” (Law et al., 2016).1 Recruitment and data collection occurred at two Learning to Talk lab sites—one at the University of Wisconsin–Madison and the other at the University of Minnesota. Table 6.1 summarizes the cohort of children in each year of testing. The numbers and summary statistics here are approximate, describing children who participated at each year, but whose data may still be excluded from the analyses. Some potential reasons for exclusion include: excessive missing data during eyetracking, experiment or technology error, developmental concerns not identified until later in study, or a failed hearing screening. Final sample sizes will depend on the measures needed for an analysis and the results from data screening checks. For each project aim, I will disclose all measurements and data exclusions following guidelines by the Center for Open Science (Nosek et al., 2014). Table 6.1: Participant characteristics. Education levels: Low: less than high school, or high school; Middle: trade school, technical or associates degree, some college, or college degree; and High: graduate degree. Year 1 Year 2 Year 3 N 184 175 160 Boys, Girls 94, 90 89, 86 82, 78 Maternal education: Low, Middle, High 15, 98, 71 12, 92, 71 6, 90, 64 Dialect: MAE, AAE 171, 13 163, 12 153, 7 Parent-identified late talkers 20 19 16 Age (months): Mean (SD) [Range] 33 (3) [28–39] 45 (4) [39–52] 57 (4) [51–66] EVT-2 standard score: Mean (SD) 115 (18) 118 (16) 118 (14) PPVT-4 standard score: Mean (SD) 113 (17) 120 (16) — GFTA-2 standard score: Mean (SD) 92 (13) — 91 (13) 6.2 General Eyetracking Procedure Two eyetracking experiments were performed each year of the longitudinal study. These experiments followed the same essential procedure: During each trial, photographs of images appeared on a computer screen for a few seconds followed by a prompt to view one of the images (e.g., find the dog). This procedure measures a child’s real-time comprehension of words by capturing how the child’s gaze location changes over time in response to speech. 6.2.1 Experiment Administration Children participating in the study were tested over two lab visits (i.e., on different dates). The first portion of each visit involved “watching movies”—that is, performing two blocks of the eyetracking experiments. A play break or hearing screening occurred between the two eyetracking blocks, depending on the visit. Each eyetracking experiment was administered as a block of trials (24 for the four-image task and 38 for the two-image task). Children received two different blocks of each experiment. The blocks for an experiment differed in trial ordering and other features (see Specific Procedures). Experiment order and block selection were counterbalanced over children and visits. For example, a child might have received Exp. 1 Block A and Exp. 2 Block B on Visit 1 and next received Exp. 2 Block A and Exp. 1 Block B on Visit 2. The purpose of this presentation was to control possible ordering effects where a particular experiment or block benefited from consistently occurring first or second. Experiments were administered using E-Prime 2.0 and a Tobii T60XL eyetracker which recorded gaze location at a rate of 60 Hz. The experiments were conducted by two examiners, one “behind the scenes” who controlled the computer running the experiment and another “onstage” who guided the child through the experiment. At the beginning of each block, the child was positioned so the child’s eyes were approximately 60 cm from the screen. The examiners calibrated the eyetracker to the child’s eyes using a five-point calibration procedure (center of screen and centers of four screen quadrants). The examiners repeated this calibration procedure if one of the five calibration points for one of the eyes did not calibrate successfully. During the experiment, the behind-the-scenes examiner monitored the child’s distance from the screen and whether the eyetracker was capturing the child’s gaze. The onstage examiner coached the child to stay fixated on the screen and repositioned the child as needed to ensure the child’s eyes were being tracked. Every six or seven trials in a block of an experiment, the experiment briefly paused with a reinforcing animation or activity. During these breaks, the onstage examiner could reposition the child if necessary before resuming the experiment. We used a gaze-contingent stimulus presentation. “After 2 s of familiarization time with the images in silence, the experiment paused to verify that the child’s gaze was being tracked. After 300 ms of continuous gaze tracking, the trial advanced. Otherwise, if the gaze could not be verified after 10 s, the trial advanced. This step ensured that for nearly every trial, the gaze was being tracked before playing the carrier phrase, or in other words, that the child was ready to hear the carrier stimuli” (Mahr &amp; Edwards, in revision). During Year 1 and Year 2, an attention-getter (e.g., check it out!) played 1 s following the end of the target noun. These reinforcers were dropped in Year 3 to streamline the experiment for older listeners. 6.2.2 Stimuli For both experiments, “stimuli were presented in children’s home dialect, either Mainstream American English (MAE) or African American English (AAE). We made an initial guess about what the home dialect was likely to be based on a number of factors, including the recruitment source and the child’s address. For most children, the home dialect was MAE. If we thought the home dialect might be AAE, a native AAE speaker who was a fluent dialect-shifter between AAE and MAE was scheduled for the lab visit, and she confirmed the home dialect by listening to the caregiver interact with the child during the consent procedure at the beginning of the visit” (Mahr &amp; Edwards, in revision). Prompts to view the target image of a trial (e.g., find the girl) used the carrier phrases “find the” and “see the”. These carriers were recording in the frame “find/see the egg” and cross-spliced with the target nouns to minimize coarticulatory cues on the determiner “the”. The images used in each experiment consisted of color photographs on gray backgrounds. These images were piloted in a preschool classroom to ensure that children consistently used the same label for familiar objects and did not consistently use the same label for novel/unfamiliar objects. 6.2.3 Data Preparation Data from both experiments were prepared using the same procedure. “We mapped the gaze x-y coordinates onto the images onscreen. We performed deblinking by interpolating short windows of missing data (up to 150 ms) if the child fixated on the same image before and after a missing data window. In other words, if the gaze did not shift to another image, and if the missing data window was short enough, that window was classified as a blink and interpolated, using the fixated image as the imputed value” (Mahr &amp; Edwards, in revision). Next, we performed trial-level cleaning. “We examined eyetracking data in the 2-s window following the onset of the target word. A trial was considered unreliable if at least 50% of the eyetracking data during the 2-s window was missing. These trials were not reliable because the child did not look at the display for the majority of the analysis window” (Mahr &amp; Edwards, in revision). If more than half of a child’s trials, combined across blocks, were unreliable, that child was excluded from analysis. “Finally, we downsampled our data into 50-ms bins, reducing the eyetracking sampling rate from 60 Hz to 20 Hz. This procedure smoothed out high-frequency noise in the data by pooling together data from adjacent frames” (Mahr &amp; Edwards, in revision). 6.3 Specific Procedure: Aim 1 (Familiar Word Recognition and Lexical Competition) Visual World Paradigm Task. In eyetracking studies with toddlers, two familiar images are usually presented: a target and a distractor. This experiment is a four-image eyetracking task that was designed to provide a more demanding word recognition task for preschoolers. In this procedure, four familiar images are presented onscreen followed by a prompt to view one of the images (e.g., find the bell!). The four images include the target word (e.g., bell), a semantically related word (drum), a phonologically similar word (bee), and an unrelated word (swing). Figure 6.1: Example display for the target bell with the semantic foil drum, the phonological foil bee, and the unrelated swing. 6.4 Specific Procedure: Aim 2 (Referent Selection and Mispronunciations) Mispronunciation Task. This experiment is an adaptation of the mispronunciation detection task by White and Morgan (2008) and Law and Edwards (2015). In this experiment, two images are presented onscreen—a familiar object and an unfamiliar object—and the child hears a prompt to view one of the images. In the correct pronunciation (or real word) and mispronunciation conditions, the child hears either the familiar word (e.g., soup) or a one-feature mispronunciation of the first consonant of the target word ([sh]oup). Importantly, within a block of trials, the child never hears both the correct and mispronounced forms of the word. These conditions are designed to test whether children map mispronunciations to novel words. To encourage fast referent selection, there were also trials in a nonword condition where the label was an unambiguous novel word (e.g., cheem presented with images of a bed and a novel-looking pastry mixer). Each nonword was constructed to match the phonotactic probability of one of the mispronunciations. Figure 6.2: Example display for a trial in which duck is mispronounced as “guck”. In a block of trials, there were 12 trials each from the nonword condition, correct production condition, and mispronunciation conditions, and children received two blocks of the experiment. A complete list of the items used in the experiment over the three years of the study is included in Appendix B. References "],
["outcome-measures.html", "Chapter 7 Outcome Measures 7.1 Sample Data", " Chapter 7 Outcome Measures The primary outcome measure for this project is word recognition performance, and I will study how it changes over time, under different experimental conditions, across children, and so on. I will measure word recognition performance using eyetracking growth curves. In this technique, I aggregate looking locations across trials to compute the proportion of looks to the target object (versus the distractor images) at each time point after the onset of the target word. This growth curve measures how the probability of fixating on a named object changes over time. Thus, children who have steeper accuracy growth curves demonstrate faster word recognition because the probability of looking at a named object increases more quickly over time. The other primary child-level measurements of interest for this project are vocabulary and speech perception. Expressive vocabulary was measured at all three years using the Expressive Vocabulary Test (EVT-2, Williams, 2007). Receptive vocabulary was measured at Year 1 and Year 2 using the Peabody Picture Vocabulary Test (PPVT-4, L. M. Dunn &amp; Dunn, 2007). These tests will provide child-level measures of vocabulary and vocabulary growth. At Year 1 and Year 2, we measured speech perception using a minimal-pair discrimination task (as in Baylis, Munson, &amp; Moller, 2008), and Year 2 and Year 3, we measured speech perception with a speech sound judgment task (SAILS as in Rvachew, 2006). Performance on these tasks will quantify individual differences in speech perception. Two other relevant measures include an articulation test administered at Year 1 and Year 3 (GFTA-2, Goldman &amp; Fristoe, 2000) and phonological awareness given at Year 2 and Year 3 (CTOPP-2, Wagner, Torgesen, Rashotte, &amp; Pearson, 2013). They are relevant because they draw on a child’s phonological knowledge of words, but I limit their consideration for only post-hoc, exploratory analyses because they measure a child’s speech production performance. 7.1 Sample Data Figure 7.1 shows the development of word recognition on the four-image task over the three years of the longitudinal study for two participants in this data set. To limit data-peeking, I only considered at data from the first 20 participants and then selected two patterns of development with clean individual differences. Figure 7.1: Data for two children on the four-image task over the three years of the study. The participant on the left showed substantial gains in processing ability from year to year, whereas the child on the right showed approximately the same level of performance each year, with some small gains in peak accuracy and rate of change. The child with substantial changes in word recognition also showed gains in expressive vocabulary with improved standard scores on the EVT-2: 83 at Year 1, 95 at Year 2, and 103 at Year 3. In contrast, the child with consistent processing had an above-average standard expressive vocabulary score during each year of the study: 133 at Year 1, 138 at Year 2, and 130 at Year 3. These two children give a glimpse of the many possible developmental trajectories for this task and how they relate to expressive vocabulary. The main work of this project will be describing and characterizing the trajectories of many children on different aspects of lexical processing. References "],
["analysis.html", "Chapter 8 Analysis 8.1 Growth Curve Analysis 8.2 Aim 1 (Familiar Word Recognition and Lexical Competition) 8.3 Aim 2 (Referent Selection and Mispronunciations) 8.4 Aim 3 (Computational Modeling)", " Chapter 8 Analysis 8.1 Growth Curve Analysis Eyetracking growth curves will be analyzed using Bayesian mixed effects logistic regression. I will use logistic regression because the outcome measurement is a probability (the log-odds of looking to the target image versus a distractor). I will use mixed-effects models because I want to estimate a separate growth curve for each child (to measure individual differences in word recognition) but also treat each child’s individual growth curve as a draw from a distribution of related curves. I plan to use Bayesian techniques to study a generative model of the data. Instead of reporting and describing a single, best-fitting model of some data, Bayesian techniques consider an entire distribution of plausible models that are consistent with the data and any prior information we have about the models. By using this approach, I can explicitly quantify uncertainty about statistical effects and draw inferences using estimates of uncertainty (instead of using statistical significance—which is not a straightforward matter for mixed-effects models).2 The eyetracking growth curves will be fit using an orthogonal cubic polynomial function of time (a now-conventional approach; see Mirman, 2014). Put differently, I will model the probability of looking to the target during an eyetracking task as: \\[ \\text{log-odds}(\\mathit{looking}) = \\beta_0 + \\beta_1 * \\textit{Time}^1 + \\beta_2 * \\textit{Time}^2 + \\beta_3 * \\textit{Time}^3 \\] That the time terms are orthogonal means that \\(\\textit{Time}^1\\), \\(\\textit{Time}^2\\) and \\(\\textit{Time}^3\\) are transformed so that they are uncorrelated. Under this formulation, the parameters \\(\\beta_0\\) and \\(\\beta_1\\) have a clear interpretation in terms of lexical processing performance. The intercept, \\(\\beta_0\\), measures the area under the growth curve—or the probability of fixating on the target word averaged over the whole window. We can think of \\(\\beta_0\\) as a measure of average accuracy or of word recognition reliability. The linear time parameter, \\(\\beta_1\\), estimates the steepness of the growth curve—or how the probability of fixating changes from frame to frame. We can think of \\(\\beta_1\\) as a measure of processing efficiency, because growth curves with stronger linear features exhibit steeper frame-by-frame increases in looking probability.3 For each experimental task, I will study how word recognition changes over time by modeling how growth curves change over developmental time. This amounts to study how the growth curve parameters changes year over year in the study. I can model the data for an eyetracking task by including dummy-coded indicators for Year 1, Year 2, and Year 3 and having these indicators interact with the growth curve parameters. In such a model, Year 2 would be the reference year, so the Year 1 parameters would estimate how the word-recognition-curves change from Year 2 to Year 1, and Year 3 parameters would be interpreted similarly. \\[ \\begin{align*} \\text{Year 2 Growth Curve:}\\\\ \\text{log-odds}(\\mathit{looking}) &amp;= \\beta_0 + \\beta_1 * \\textit{Time}^1 + \\beta_2 * \\textit{Time}^2 + \\beta_3 * \\textit{Time}^3 \\\\ \\text{Adjustments to Year 2:} \\\\ \\beta_i &amp;= \\gamma_{i:2} + \\gamma_{i:1} * \\text{Year1} + \\gamma_{i:3} * \\text{Year3} \\\\ \\end{align*} \\] Thus, the interaction effects for the intercept term (\\(\\gamma_{0:1}\\), \\(\\gamma_{0:3}\\)) describe how overall accuracy changed between years, and interaction effects for the linear-time terms (\\(\\gamma_{1:1}\\), \\(\\gamma_{1:3}\\)) describe changes in overall processing efficiency between years. Lastly, a brief comment about priors. Bayesian models require prior information (“priors”). For these models, I will use weakly to moderately informative priors. For example, suppose x and y are scaled to mean 0 and standard deviation 1. A weakly informative prior for the effect of x on y might be Normal(0, 5)—a normal distribution with mean 0 and standard deviation 5. If we fit a regression model and observed an effect size of 12 SD units, our first assumption would be that something went wrong with our software. The weakly informative prior captures this level of prior information. A moderately informative prior would be Normal(0, 1). This prior information captures our disciplinary experience that effect sizes greater than ±1 relatively uncommon in child language research. A strongly informative prior for this effect might be something like Normal(.4, .1) which says that our model should be very skeptical of negative effects and of effects larger than .8. For this project, I will default to the first two levels of prior information. 8.2 Aim 1 (Familiar Word Recognition and Lexical Competition) In the four-image task, I will model the development of familiar word recognition by studying how looks to the target image change year over year, as described in Growth Curve Analysis. I predict that children will be more sensitive to the phonological foil and semantic foils in this task as they age and learn more words. This hypothesis is based on the idea that children discover similarities among words as they learn word and integrate them into their lexicon. To test this hypothesis, I will study how the probability of fixating on the foils changes over trial-time and how these growth curves change from year to year. In the conventional model of eyetracking data, the outcome is a binomial choice—Target versus Distractor—and we can estimate the log-odds of fixating on the target image relative to the distractors. To study the specific effect of the Phonological foil in this task, Law et al. (2016) treated the Unrelated foil as a reference distractor and compared two separate binomial growth curves (see Figure 8.1): Target versus Phonological, and Target versus Unrelated. The same technique was used on the Semantic foil as well. With this approach, shown below, we observed an early negative effect of the phonological foil and a late negative effect of the semantic foil. Figure 8.1: Reprint of Figure 4 from Law et al. (2016) to illustrate the strategy of examining lexical competition effects where the semantic foil and phonological foil are compared to the unrelated image. I plan to employ this technique for studying lexical competition effects and their development from Year 1 to Year 2 to Year 3.4 Increased lexical completion would be reflected in greater interference from the foils compared to the unrelated image. A more comprehensive statistical model for this experiment would capture the fact that the data are multinomial: Target versus Phonological versus Semantic versus Unrelated. However, multinomial mixed effects growth curves have not been used for eyetracking data. They are not estimable with the standard classical modeling software (lme4). I plan to examine whether such a model is feasible with Bayesian techniques, but it may prove to be too unstable. In that case, I will fall back to the above described strategy. I will also examine whether individual differences in lexical processing are stable over time. For an initial analytic step, I will identify how frequently children change quartiles or deciles. The idea here is that stable individual differences in processing should preserve some relative rankings—fast-processing children should remain relatively fast compared to their peers. If children make large swings in their rankings, e.g., changing from the bottom 55th to the 30th percentile, then we have evidence that these rankings are unstable. I am also interested in how the magnitude of individual differences change over time. The differences among children could diminish over time so that the rankings are unstable and reflect small variations among children. Another question concerns the relationship between the development of lexical processing and the development of vocabulary size. Specifically, I will ask how age-adjusted lexical processing measures (accuracy and efficiency) correlate with age-adjusted vocabulary size each year. Moreover, I will also examine whether children who make large gains in vocabulary size also show large concurrent changes in their lexical processing measures. 8.3 Aim 2 (Referent Selection and Mispronunciations) For this task, I will model how the looks to the familiar image differ in each condition (real words, mispronunciations, nonwords) and how the growth curves for each condition change year over year. This model will use growth curve model described in Growth Curve Analysis but augmented with Condition effects. I will examine whether and when any dissociation is observed for word recognition in the real word and nonword conditions. McMurray et al. (2012) argue that familiar word recognition and fast association for novel words reflect the same cognitive process: referent selection. Data from this task would support with this hypothesis when the growth curves for looks to the familiar image are symmetrical for the real word and nonword conditions. Figure 8.2, showing data from Law and Edwards (2015, n = 34 children, 30-46 months old), shows some symmetry for the real word and nonword conditions. Figure 8.2: Condition averages for data described by Law and Edwards (2015). Compare to Figure 2 in the original manuscript. I will test whether the two measures ever dissociate by computing the posterior predicted difference between the growth curves. This approach is similar to the bootstrap-based divergence analyses used in some word recognition experiments (Dink &amp; Ferguson, 2016; e.g., Oleson, Cavanaugh, McMurray, &amp; Brown, 2015). The essential question is when—at which specific time points—do two growth curves differ significantly from one another. The bootstrap approach uses resampling to get an estimate, whereas I will use posterior predicted samples to estimate these differences. (I have not seen my approach used yet in the literature, so it is a small innovation.) Specifically, I will compute the posterior-predicted looks to the familiar object in the real word condition, P(Familiar | Real Word, Time t, Child i) and the analogous looks to the unfamiliar object in the nonword condition, P(Unfamiliar | Nonword, Time t, Child i). The difference between these two probabilities estimates how the time course of word recognition differs between these two conditions, and I can use 50% and 90% uncertainty intervals to determine during which time points the curves credibly differ from each other. Figure 8.3 shows this calculation performed on data from Law and Edwards (2015). If feasible, I will also examine whether these measures dissociate within children and examine which child-level factors are associated with these kinds of listeners. Figure 8.3: Demonstration of posterior difference technique on data from Law and Edwards (2015). Even though performance on the real word and nonword conditions might be highly correlated, one might intuitively hypothesize that that performance on the nonword condition to be a better predictor of concurrent or future vocabulary size. The rationale would be that referent selection for novel words is a more transparent test of the word learner’s basic task of associating new labels with objects. Therefore, I will examine how each of these measures relates to vocabulary growth. I will describe how looking behavior in the mispronunciation condition changes over time and changes for specific mispronunciation patterns. Overall, I predict that children will be more tolerant of mispronunciations as they age, because older children know more words and have more implicit knowledge about the similarities among words. As for specific mispronunciation items, let us (safely) suppose that speech perception improves with age, especially for later mastered sounds. Then we should expect that looking patterns for the rice-wice trials change significantly between Year 2 and Year 3, at least compared to looking patterns on trials with mispronunciations of earlier acquired sounds (e.g., girl-dirl or duck-guck). Therefore, I will examine individual mispronunciation effects and how they are associated with child-level measures, including speech perception. 8.4 Aim 3 (Computational Modeling) 8.4.1 TRACE Model Architecture TRACE (McClelland &amp; Elman, 1986) is an interactive activation model, and it interprets an input pattern by spreading energy (activation) through a network of processing units. The pattern of activation over the network is its interpretation of the input signal, so that more active units represent more likely interpretations. Over many processing cycles, the network propagates energy among its connections until it settles into a stable pattern of activation. The input for TRACE is a mock-speech signal that activates perceptual feature-detectors. These units respond to phonetic features like voicing or vocalic resonance. The perceptual units activate phoneme units, and the phoneme units activate lexical word units, as shown in Figure 8.4. Figure 8.4: TRACE model architecture. Thick arrows indicate excitatory connections between layers, including a top-down connections from words onto phoneme units. Lines with points at the end reflect inhibitory connections among competing units within a layer. This image is a lightly modified public-domain version of Figure 1 in Strauss et al. (2007): https://en.wikipedia.org/wiki/File:TRACE_architecture.jpg Units within a level (phonetic, phonemic, lexical) compete through lateral inhibition, so that more active units can suppress less active units. This inhibition allows the network to rule out possible units and narrow its interpretations over time. There are also top-down connections so that word units in the lexical layer can reinforce the sound units that make up those words. One consequence of this feature is that the network can resolve ambiguous phonemes, as in Xift where X is a sound between /k/ and /g/ and top-down influence supports gift rather than kift. Lastly, activation in units gradually decays over time, and the network will eventually “forget” it input pattern to return to a resting state. The model parameters that govern how activation propagates through the network map onto psychological processing constructs. For instance, the phoneme inhibition strength parameter controls how decisively (or categorically) the network interprets speech sounds. Phoneme-to-word activation strength reflects how quickly speech sounds begin to activate words. Decay parameters control the model’s temporary memory for different kinds of representations. 8.4.2 Modeling Looking Data To simulate behavioral data, we need a linking hypothesis for translating between human behavior and network behavior (Magnuson, Mirman, &amp; Harris, 2012). During each network cycle, some words are activated, some more activated than others, so that the unit with the highest activation is the preferred interpretation of the input. We can convert these activations into probabilities using the softmax function: \\[P(\\textit{word}_w) = \\mathrm{softmax}(\\textit{activation}_w) = \\frac{\\exp(k * \\textit{activation}_w)} {\\sum{\\exp(k * \\textit{activation}_{[\\text{words to choose from}]})}}\\] In other words, scale the activation using some parameter k, exponentiate the scaled activation values of all relevant choices, and the proportion of the total exponentiated activation belonging to word w is the probability of fixating on word w.5 The scaling value k is manipulated to help the model-simulated probabilities match the observed looking probabilities. The next step, and this process will be rather open-ended and iterative, is to simulate the behavioral data with the model. As a first step, I will need to create a developmentally appropriate lexicon. I will use developmental norms and databases to determine an appropriate set of items. I will first try to simulate the mispronunciation experiment. Mayor and Plunkett (2014) successfully simulated some mispronunciation data from experiments with data, so I will borrow some of their strategies for modeling these kinds of experiments. After I get the model to simulate Year 1 data, I will explore which parameters need to change to account for Year 2 and then Year 3 data. There are many degrees of freedom (model parameters) for how to replicate these development changes, so I will plan to systematically report the model fitting and the development consequences of the model fit. The four-image experiment is slightly trickier, because it incorporates semantically related information, and TRACE has no built-in semantic representations. Two options for modeling the experiment are 1) to ignore looks to the semantic foil, following the Luce choice rule (1959, 2008) and 2) to try to incorporate semantic information into the model by modifying connections between semantically related words—although it is unclear whether either is tractable. At any rate, part of this project will be to explore how to reconcile these data with this apparent limitation with the TRACE model. Finally, I would like to account for individual differences among children and incorporate child-level information into the simulations. Specifically, I would like to test how vocabulary differences are associated with large lexicons for model simulations. Moreover, I would like to whether children’s speech perception abilities systematically relate to the model’s phonological activation parameters. Such a correspondence would further validate the models. These simulations would further validate these child-level measures by describing how they specifically affect word recognition. For these research questions, I will following the modeling heuristics described by Magnuson et al. (2012). For example, for any modeling failures, I will assess the failure in terms of the computational consequences: Is the failure a problem with the theory, implementation, parameters or link between human data and model activity? Similarly, can the model fit specific item effects or just condition effects? The authors provide other heuristics, and these guidelines heuristics will help formalize the assessment and comparison of computational models. References "],
["aim1-method.html", "Chapter 9 Method 9.1 Participants 9.2 Procedure 9.3 Experiment Administration 9.4 Stimuli 9.5 Data screening 9.6 Prepare the dataset for modeling", " Chapter 9 Method 9.1 Participants 9.1.1 Special case data screening (Skip for now. This is where I review the participant notes and will remove children who have to be excluded for other reasons, like being diagnosed with a language disorder at TimePoint 3.) 9.2 Procedure This experiment used a version of the Visual World Paradigm for word recognition experiments. In eyetracking studies with toddlers, two familiar images are usually presented: a target and a distractor. This experiment is a four-image eyetracking task that was designed to provide a more demanding word recognition task for preschoolers. In this procedure, four familiar images are presented onscreen followed by a prompt to view one of the images (e.g., find the bell!). The four images include the target word (e.g., bell), a semantically related word (drum), a phonologically similar word (bee), and an unrelated word (swing). Figure 6.1 shows an example of a trial’s items. This procedure measures a child’s real-time comprehension of words by capturing how the child’s gaze location changes over time in response to speech. Figure 6.1: Example display for the target bell with the semantic foil drum, the phonological foil bee, and the unrelated swing. 9.3 Experiment Administration Children participating in the study were tested over two lab visits (i.e., on different dates). The first portion of each visit involved “watching movies”—that is, performing two blocks of eyetracking experiments. A play break or hearing screening occurred between the two eyetracking blocks, depending on the visit. Each eyetracking experiment was administered as a block of trials (24 for this experiment and 38 for a two-image task—see chapter X). Children received two different blocks of each experiment. The blocks for an experiment differed in trial ordering and other features. Experiment order and block selection were counterbalanced over children and visits. For example, a child might have received Exp. 1 Block A and Exp. 2 Block B on Visit 1 and next received Exp. 2 Block A and Exp. 1 Block B on Visit 2. The purpose of this presentation was to control possible ordering effects where a particular experiment or block benefited from consistently occurring first or second. Experiments were administered using E-Prime 2.0 and a Tobii T60XL eyetracker which recorded gaze location at a rate of 60 Hz. The experiments were conducted by two examiners, one “behind the scenes” who controlled the computer running the experiment and another “onstage” who guided the child through the experiment. At the beginning of each block, the child was positioned so the child’s eyes were approximately 60 cm from the screen. The examiners calibrated the eyetracker to the child’s eyes using a five-point calibration procedure (center of screen and centers of four screen quadrants). The examiners repeated this calibration procedure if one of the five calibration points for one of the eyes did not calibrate successfully. During the experiment, the behind-the-scenes examiner monitored the child’s distance from the screen and whether the eyetracker was capturing the child’s gaze. The onstage examiner coached the child to stay fixated on the screen and repositioned the child as needed to ensure the child’s eyes were being tracked. Every six or seven trials in a block of an experiment, the experiment briefly paused with a reinforcing animation or activity. During these breaks, the onstage examiner could reposition the child if necessary before resuming the experiment. We used a gaze-contingent stimulus presentation. “After 2 s of familiarization time with the images in silence, the experiment paused to verify that the child’s gaze was being tracked. After 300 ms of continuous gaze tracking, the trial advanced. Otherwise, if the gaze could not be verified after 10 s, the trial advanced. This step ensured that for nearly every trial, the gaze was being tracked before playing the carrier phrase, or in other words, that the child was ready to hear the carrier stimuli” (Mahr &amp; Edwards, in revision). During Year 1 and Year 2, an attention-getter (e.g., check it out!) played 1 s following the end of the target noun. These reinforcers were dropped in Year 3 to streamline the experiment for older listeners. 9.4 Stimuli A few sentences to reiterate what the four kinds of images represented. A complete list of the items used in the experiment in Appendix A. Recorded “stimuli were presented in children’s home dialect, either Mainstream American English (MAE) or African American English (AAE). We made an initial guess about what the home dialect was likely to be based on a number of factors, including the recruitment source and the child’s address. For most children, the home dialect was MAE. If we thought the home dialect might be AAE, a native AAE speaker who was a fluent dialect-shifter between AAE and MAE was scheduled for the lab visit, and she confirmed the home dialect by listening to the caregiver interact with the child during the consent procedure at the beginning of the visit” (Mahr &amp; Edwards, in revision). Prompts to view the target image of a trial (e.g., find the girl) used the carrier phrases “find the” and “see the”. These carriers were recording in the frame “find/see the egg” and cross-spliced with the target nouns to minimize coarticulatory cues on the determiner “the”. The images used in the experiment consisted of color photographs on gray backgrounds. These images were piloted in a preschool classroom to ensure that children consistently used the same label for familiar objects. 9.5 Data screening “We mapped the gaze x-y coordinates onto the images onscreen. We performed deblinking by interpolating short windows of missing data (up to 150 ms) if the child fixated on the same image before and after a missing data window. In other words, if the gaze did not shift to another image, and if the missing data window was short enough, that window was classified as a blink and interpolated, using the fixated image as the imputed value” (Mahr &amp; Edwards, in revision). After mapping the gaze coordinates onto the onscreen images, we performed data screening. We considered the time window from 0 to 2000 ms after target noun onset. We identified a trial as unreliable if at least 50% of the looks were missing during the time window. We excluded an entire block of trials if it had fewer than 12 reliable trials. Table 9.1 shows the numbers of participants and trials excluded at each time point due to unreliable data. There were more children in the second timepoint than the first timepoint due to a timing error in the initial version of this experiment, leading to the exclusion of 27 participants from the first timepoint. Table 9.1: Eyetracking data before and after data screening. Dataset Study N Children N Blocks N Trials Raw TimePoint1 178 332 7967 TimePoint2 180 347 8327 TimePoint3 163 322 7724 Screened TimePoint1 163 291 5951 TimePoint2 165 305 6421 TimePoint3 156 295 6483 Raw − Screened TimePoint1 15 41 2016 TimePoint2 15 42 1906 TimePoint3 7 27 1241 9.6 Prepare the dataset for modeling To prepare the data for modeling, we downsampled the data into 50-ms (3-frame) bins, “reducing the eyetracking sampling rate from 60 Hz to 20 Hz. This procedure smoothed out high-frequency noise in the data by pooling together data from adjacent frames” (Mahr &amp; Edwards, in revision). We modeled the looks from 250 to 1500 ms. Lastly, we aggregated looks by child, study and time, and created orthogonal polynomials to use as time features for the model. "],
["analysis-of-familiar-word-recognition.html", "Chapter 10 Analysis of familiar word recognition 10.1 Growth curve analysis 10.2 Bayesian model results", " Chapter 10 Analysis of familiar word recognition General Todos: Determine and use consistent terminology for the studies and growth curve features. Make sure axis labels and legend labels are consistent. 10.1 Growth curve analysis TJM: I’ll have to clean this section up once the order of the main results is pinned down. That way this can flow neatly in to the findings. Looks to the familiar image were analyzed using Bayesian mixed effects logistic regression. We used logistic regression because the outcome measurement is a probability (the log-odds of looking to the target image versus a distractor). We used mixed-effects models to estimate a separate growth curve for each child (to measure individual differences in word recognition) but also treat each child’s individual growth curve as a draw from a distribution of related curves. We used Bayesian techniques to study a generative model of the data. Instead of reporting and describing a single, best-fitting model of some data, Bayesian methods consider an entire distribution of plausible models that are consistent with the data and any prior information we have about the models. By using this approach, one can explicitly quantify uncertainty about statistical effects and draw inferences using estimates of uncertainty (instead of using statistical significance—which is not a straightforward matter for mixed-effects models).6 The eyetracking growth curves were fit using an orthogonal cubic polynomial function of time (a now-conventional approach; see Mirman, 2014). Put differently, we modeled the probability of looking to the target during an eyetracking task as: \\[ \\text{log-odds}(\\mathit{looking}) = \\beta_0 + \\beta_1 * \\textit{Time}^1 + \\beta_2 * \\textit{Time}^2 + \\beta_3 * \\textit{Time}^3 \\] That the time terms are orthogonal means that \\(\\textit{Time}^1\\), \\(\\textit{Time}^2\\) and \\(\\textit{Time}^3\\) are transformed so that they are uncorrelated. Under this formulation, the parameters \\(\\beta_0\\) and \\(\\beta_1\\) have a direct interpretation in terms of lexical processing performance. The intercept, \\(\\beta_0\\), measures the area under the growth curve—or the probability of fixating on the target word averaged over the whole window. We can think of \\(\\beta_0\\) as a measure of word recognition reliability. The linear time parameter, \\(\\beta_1\\), estimates the steepness of the growth curve—or how the probability of fixating changes from frame to frame. We can think of \\(\\beta_1\\) as a measure of processing efficiency, because growth curves with stronger linear features exhibit steeper frame-by-frame increases in looking probability.7 We studied how word recognition changes over time by modeling how growth curves change over developmental time. This amounted to studying how the growth curve parameters changes year over year. We included dummy-coded indicators for Year 1, Year 2, and Year 3 and having these indicators interact with the growth curve parameters. We also included random effects to represent child-by-study effects. 10.1.1 Growth curve features as measures of word recognition performance As noted above, two of the model’s growth curve features have direct interpretations in terms of lexical processing performance: The model’s intercept parameter corresponds to the average proportion/probability of looking to the named image over the trial window and the linear time parameter corresponds to slope of the growth curve or lexical processing efficiency. We also were interested in peak proportion of looks to the target. We derived this value by computing the growth curves from the model and taking the median of the five highest points on the curve. Figure TODO shows three simulated growth curves and how each of these growth curve features relate to word recognition performance. TODO: Figure number and caption 10.1.2 Year over year changes in word recognition TJM: This is a bit rough still. As a mixed-effects model, the model estimated a population-average growth curve (the “fixed” effects) and how individual children deviated from average (the “random” effects). Figure TODO shows 200 posterior samples of those average growth curves for each study. Clearly, on average, the growth curves become steeper and achieve higher looking probabilities with each year of the study. We now consider how the curvature of the average growth curves change each year. Figure 10.1 depicts uncertainty intervals with the model’s average effects of each timepoint on the growth curve features. The intercept and linear time effects increased each year, confirming that children become more reliable and faster at recognizing words as they grow older. The peak accuracy also increased each year. For each effect, the change from year 1 to year 2 is approximately the same as the change from year 2 to year 3, as visible in figure 10.2. Figure 10.1: Uncertainty intervals for the effects of study timepoints on growth curve features. The intercept and peak features were converted from log-odds to proportions to ease interpretation. Figure 10.2: Uncertainty intervals for the differences between study timepoints. Again, the intercept and peak features were converted to proportions. The average looking probability (intercept feature) was 0.385 [90% UI: 0.372–0.397] for timepoint 1, 0.485 [0.473–0.498] for timepoint 2, and 0.557 [0.544–0.569] for timepoint 3. The averages increased by 0.1 [0.087–0.114] from timepoint 1 to timepoint 2 and by 0.072 [0.058–0.085] from timepoint 2 to timepoint 3. The peak looking probability was 0.548 [0.528–0.567] for timepoint 1, 0.683 [0.666–0.7] for timepoint 2, and 0.769 [0.755–0.782] for timepoint 3. The peak values increased by 0.135 [0.114–0.156] from timepoint 1 to timepoint 2 and by 0.086 [0.069–0.103] from timepoint 2 to timepoint 3. These results numerically confirm the hypothesis that children would improve in their word recognition reliability, both in terms of average looking and in terms of peak accuracy, each year. Summary. The average growth curve features increased year over year, so that children’s looked to the target more quickly and more reliable. 10.1.3 Exploring plausible ranges of performance over time TJM: I like this analysis, but I’m realizing I should describe how variable the real participants are first. Bayesian models are generative; they describe how the data could have been generated. Our model assumed that each child’s growth curve was drawn from a population of related growth curves, and it tried to infer the parameters over that distribution. These two features—a generative model and learning about the population of growth curves—allow the model to simulate new samples from that distribution of growth curves. That is, we can predict a set of growth curves for a hypothetical, unobserved child drawn from the same distribution as the 195 observed children. This procedure lets us explore the plausible degrees of variability in performance at each age. Figure 10.3 shows the posterior predictions for 1,000 simulated participants, which demonstrates how the model expects new participants to improve longitudinally but also exhibit stable individual differences over time. Figure 10.4 shows uncertainty intervals for these simulations. The model learned to predict less accurate and more variable performance at age 3 with improving accuracy and narrowing variability at age 4 and age 5. Figure 10.3: Posterior predictions for hypothetical unobserved participants. Each line represents the predicted performance for a new participant. The three dark lines highlight predictions from one single simulated participant. The simulated participant shows both longitudinal improvement in word recognition and similar relative performance compared to other simulations each year, indicating that the model would predict new children to improve year over year and show stable individual differences over time. Figure 10.4: Uncertainty intervals for the simulated participants. Variability is widest at age 3 and narrowest at age 5, consistent with the prediction that children become less variable as they grow older. We hypothesized that children would become less variable as they grew older and converged on a mature level of performance. We can address this question by inspecting the ranges of predictions for the simulated participants. The claim that children become less variable would imply that the range of predictions should be narrower age 5 than for age 4 than age 3. Figure 10.5 depicts the range of the predictions, both in terms of the 90 percentile range (i.e., the range of the middle 90% of the data) and in terms of the 50 percentile (interquartile) range. The ranges of performance decrease from age 3 to age 4 to age 5, consistent with the hypothesized reduction in variability. Figure 10.5: Ranges of predictions for simulated participants over the course of a trial. The ranges are most similar during the first half of the trial when participants are at chance performance, and the ranges are most different at the end of the trial as children reliably fixate on the target image. The ranges of performance decreases with each year of the study as children show less variability. The developmental pattern of increasing reliability and decreasing variability was also observed for the growth curve peaks. For the synthetic participants, the model predicted that individual peak probabilities will increase each year, peakTP1 = 0.552 [90% UI: 0.347–0.774], peakTP2 = 0.692 [0.477–0.861], peakTP3 = 0.776 [0.586–0.909]. Moreover, the range of plausible values for the individual peaks narrowed each for the simulated data. For instance, the difference between the 95th and 5th percentiles was 0.43 for timepoint 1, 0.38 for timepoint 2, and 0.32 for timepoint 3. Summary. We used the model’s random effects estimates to simulate growth curves from 1,000 hypothetical, unobserved participants. The simulated dataset showed increasing looking probability and decreasing variability with each year of the study. These simulations confirm the hypothesis that variability would be diminish as children converge on a mature level of performance on this task. 10.1.4 Are individual differences stable over time? TJM: This section is in really good shape. We predicted that children would show stable individual differences such that children who are faster and more reliable at recognizing words at age 3 remain relatively faster and more reliable at age 5. To evaluate this hypothesis, we used Kendall’s W (the coefficient of correspondence or concordance). This nonparametric statistic measures the degree of agreement among J judges who are rating I items. For our purposes, the items are the 123 children who provided reliable eyetracking for all three years of the study. (That is, we excluded children who only had reliable eyetracking data for one or two years.) The judges are the sets of growth curve parameters from each year of study. For example, the intercept term provides three sets of ratings: The participants’ intercept terms from year 1 are one set of ratings and the terms from years 2 and 3 provide two more sets of ratings. These three ratings are the “judges” used to compute the intercept’s W. Thus, we compute five groups of W coefficients, one for each set of growth curve features: Intercept, Time1, Time2, Time3, and Peak looking probability. Because we used a Bayesian model, we have a distribution of ratings and thus a distribution of concordance statistics. Each sample of the posterior distribution fits a growth curve for each child in each study, so each posterior sample provides a set of ratings for concordance coefficients. The distribution of W’s lets us quantify our uncertainty because we can compute W’s for each of the 4000 samples from the posterior distribution. One final matter is how do we assess whether a concordance statistic is meaningful. To tackle this question, we also included a “null rater”, a fake parameter that assigned each child in each year a random number. We can use the distribution of W’s generated by randomly rating children as a benchmark for assessing whether the other concordance statistics differ meaningfully from chance. Figure 10.6: Uncertainty intervals for the Kendall’s coefficient of concordance. Random ratings provide a baseline of null W statistics. The intercept and linear time features are decisively non-null, indicating a significant degree of correspondence in children’s relative word recognition reliability and efficiency over three years of study. We used the kendall() function in the irr package (vers. 0.84, CITATION) to compute concordance statistics. Figure 10.6 depicts uncertainty intervals for the Kendall W’s for these growth curve features. The 90% uncertainty interval of W statistics from random ratings [0.275–0.391] subsumes the intervals for the Time2 effect [0.295–0.351] and the Time3 effect [0.276–0.348], indicating that these values do not differentiate children in a longitudinally stable way. That is, the Time2 and Time3 features differentiate children across studies as well as random numbers. Earlier, we stated that only the intercept, linear time, and peak features have psychologically meaningful interpretations and that the higher-order features of these models serve to capture the shape of the growth curve data. These statistics support that assertion. Concordance is strongest for the peak feature, W = 0.587 [0.572–0.601] and the intercept term, W = 0.585 [0.573–0.596], followed by the linear time term, W = 0.501 [0.483–0.518]. Because these values are removed from the statistics for random ratings, we conclude that there is a credible degree of correspondence across studies when we rank children using their peak looking probability, average look probability (the intercept) or their growth curve slope (linear time). Summary. Growth curve features reflect individual differences in word recognition reliability and efficiency. By using Kendall’s W to measure the degree of concordance among growth curve features over developmental time, we tested whether individual differences in lexical processing persisted over development. We found that the peak looking probability, average looking probability and linear time features were stable over time. rm(ws, reduced_data, posterior_w) 10.1.5 Predicting future vocabulary size TJM: This section is in good shape. We hypothesized that individual differences in word recognition at age 3 will be more discriminating and predictive future language outcomes than differences at age 4 or age 5. To test this hypothesis, we calculated the correlations of growth curve features with year 3 expressive vocabulary size and year 2 receptive vocabulary. (The receptive test was not administered during year 3 for logistical reasons). As with the concordance analysis, we computed each the correlations for each sample of the posterior distribution to obtain a distribution of correlations. Figure 10.7 shows the correlations of the peak looking probability, average looking probability and linear time features with expressive vocabulary size at year 3, and Figure 10.8 shows analagous correlations for the receptive vocabulary at year 2. For all cases, the strongest correlations were found between the growth curve features at year 1. Growth curve peaks from year 1 correlated with year 3 vocabulary with r = .519, 90% UI [.498–.54], but the concurrent peaks from year 3 showed a correlation of just r = .311, [.293–.328], a difference between year 1 and year 3 of rTP1−TP3 = .209, [.18–.236]. A similar pattern held for lexical processing efficiency values. Linear time features from year 1 correlated with year 3 vocabulary with r = .413, 90% UI [.389–.438], whereas the concurrent lexical processing values from year 3 only showed a correlation of r = .283, [.259–.306], a difference of rTP1−TP3 = .13, [.097–.164]. For the average looking probabilities, the correlation for year 1, r = .391, [.389–.438], was probably only slightly greater than the correlation for year 2, rTP1−TP2 = .018, [−.005–.042] but considerably greater than the concurrent correlation at year 3, rTP1−TP3 = .077, [.054–.099]. Figure 10.7: Uncertainty intervals for the correlations of growth curve features at each time point with expressive vocabulary (EVT2 standard scores) at year 3. The bottom rows provide intervals for the pairwise differences in correlations between time points. Peak looking probabilities from year 1 were strongly correlated with year 2 receptive vocabulary, r = .625, [.606–.643], and this correlation was much greater than the correlation observed for the year 2 growth curve peaks, rTP1−TP2 = .258, [.258]. The correlation of year 1 average looking probabilities, r = .454, [.437–.47], was greater than the year 2 correlation, rTP1−TP2 = .084, [.084], and the correlation for year 1 linear time features, r = .514, [.492–.538], was likewise greater than the year 2 correlation, rTP1−TP2 = .225, [.192–.257]. Figure 10.8: Uncertainty intervals for the correlations of growth curve features at each time point with expressive vocabulary (PPVT4 standard scores) at year 2. The bottom row shows pairwise differences between the correlations at year 1 and year 2. Summary. Although individual differences in word recognition are stable over time, early differences are more significant than later ones. The strongest predictors of future vocabulary size were the growth curve features from age 3. That is, word recognition performance from age 3 was more strongly correlated with age 5 expressive vocabulary than word recognition performance at age 5. A similar pattern of results held for predicting receoptive vocabulary at age 4. 10.1.6 Relationships with other child-level predictors TJM: This is where I would analyze the other test scores as we have discussed. 10.2 Bayesian model results Here is the code used to fit the model with Stan. It took about 24 hours to run the model. The regression terms have the prior Normal(0, 1) library(rstanarm) options(mc.cores = parallel::detectCores()) m &lt;- stan_glmer( cbind(Primary, Others) ~ (ot1 + ot2 + ot3) * Study + (ot1 + ot2 + ot3 | ResearchID/Study), family = binomial, prior = normal(0, 1, autoscale = FALSE), prior_intercept = normal(0, 2), prior_covariance = decov(2, 1, 1), data = d_m) readr::write_rds(m, &quot;./data/stan_aim1_cubic_model.rds.gz&quot;) The output below contains the model quick view, a summary of the fixed effect terms, and a summary of the priors used. b #&gt; stan_glmer #&gt; family: binomial [logit] #&gt; formula: cbind(Primary, Others) ~ (ot1 + ot2 + ot3) * Study + (ot1 + ot2 + #&gt; ot3 | ResearchID/Study) #&gt; observations: 12584 #&gt; ------ #&gt; Median MAD_SD #&gt; (Intercept) -0.5 0.0 #&gt; ot1 1.6 0.1 #&gt; ot2 0.0 0.0 #&gt; ot3 -0.2 0.0 #&gt; StudyTimePoint2 0.4 0.0 #&gt; StudyTimePoint3 0.7 0.0 #&gt; ot1:StudyTimePoint2 0.6 0.1 #&gt; ot1:StudyTimePoint3 1.1 0.1 #&gt; ot2:StudyTimePoint2 -0.2 0.0 #&gt; ot2:StudyTimePoint3 -0.4 0.1 #&gt; ot3:StudyTimePoint2 -0.1 0.0 #&gt; ot3:StudyTimePoint3 -0.2 0.0 #&gt; #&gt; Error terms: #&gt; Groups Name Std.Dev. Corr #&gt; Study:ResearchID (Intercept) 0.305 #&gt; ot1 0.691 0.20 #&gt; ot2 0.437 -0.11 0.02 #&gt; ot3 0.294 -0.11 -0.44 -0.06 #&gt; ResearchID (Intercept) 0.264 #&gt; ot1 0.423 0.78 #&gt; ot2 0.125 -0.75 -0.56 #&gt; ot3 0.058 -0.23 -0.31 0.19 #&gt; Num. levels: Study:ResearchID 484, ResearchID 195 #&gt; #&gt; Sample avg. posterior predictive distribution of y: #&gt; Median MAD_SD #&gt; mean_PPD 49.9 0.1 #&gt; #&gt; ------ #&gt; For info on the priors used see help(&#39;prior_summary.stanreg&#39;). summary(b, pars = names(fixef(b))) #&gt; #&gt; Model Info: #&gt; #&gt; function: stan_glmer #&gt; family: binomial [logit] #&gt; formula: cbind(Primary, Others) ~ (ot1 + ot2 + ot3) * Study + (ot1 + ot2 + #&gt; ot3 | ResearchID/Study) #&gt; algorithm: sampling #&gt; priors: see help(&#39;prior_summary&#39;) #&gt; sample: 4000 (posterior sample size) #&gt; observations: 12584 #&gt; groups: Study:ResearchID (484), ResearchID (195) #&gt; #&gt; Estimates: #&gt; mean sd 2.5% 25% 50% 75% 97.5% #&gt; (Intercept) -0.5 0.0 -0.5 -0.5 -0.5 -0.4 -0.4 #&gt; ot1 1.6 0.1 1.4 1.5 1.6 1.6 1.7 #&gt; ot2 0.0 0.0 0.0 0.0 0.0 0.1 0.1 #&gt; ot3 -0.2 0.0 -0.2 -0.2 -0.2 -0.2 -0.1 #&gt; StudyTimePoint2 0.4 0.0 0.3 0.4 0.4 0.4 0.5 #&gt; StudyTimePoint3 0.7 0.0 0.6 0.7 0.7 0.7 0.8 #&gt; ot1:StudyTimePoint2 0.6 0.1 0.4 0.5 0.6 0.6 0.7 #&gt; ot1:StudyTimePoint3 1.1 0.1 0.9 1.0 1.1 1.2 1.3 #&gt; ot2:StudyTimePoint2 -0.2 0.1 -0.3 -0.2 -0.2 -0.1 -0.1 #&gt; ot2:StudyTimePoint3 -0.4 0.1 -0.5 -0.4 -0.4 -0.3 -0.3 #&gt; ot3:StudyTimePoint2 -0.1 0.0 -0.2 -0.1 -0.1 -0.1 0.0 #&gt; ot3:StudyTimePoint3 -0.2 0.0 -0.3 -0.2 -0.2 -0.2 -0.1 #&gt; #&gt; Diagnostics: #&gt; mcse Rhat n_eff #&gt; (Intercept) 0.0 1.0 1086 #&gt; ot1 0.0 1.0 857 #&gt; ot2 0.0 1.0 842 #&gt; ot3 0.0 1.0 1156 #&gt; StudyTimePoint2 0.0 1.0 1034 #&gt; StudyTimePoint3 0.0 1.0 959 #&gt; ot1:StudyTimePoint2 0.0 1.0 674 #&gt; ot1:StudyTimePoint3 0.0 1.0 934 #&gt; ot2:StudyTimePoint2 0.0 1.0 836 #&gt; ot2:StudyTimePoint3 0.0 1.0 762 #&gt; ot3:StudyTimePoint2 0.0 1.0 1183 #&gt; ot3:StudyTimePoint3 0.0 1.0 1390 #&gt; #&gt; For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1). prior_summary(b) #&gt; Priors for model &#39;b&#39; #&gt; ------ #&gt; Intercept (after predictors centered) #&gt; ~ normal(location = 0, scale = 5) #&gt; #&gt; Coefficients #&gt; ~ normal(location = [0,0,0,...], scale = [1,1,1,...]) #&gt; **adjusted scale = [3.33,3.33,3.33,...] #&gt; #&gt; Covariance #&gt; ~ decov(reg. = 2, conc. = 1, shape = 1, scale = 1) #&gt; ------ #&gt; See help(&#39;prior_summary.stanreg&#39;) for more details Let’s try to understand our model by making some plots. 10.2.1 Plot the intervals for the random effect parameters These are the parameters governing the random effect distributions. First, we plot the standard deviations. Recall that in our hierarchical model we suppose that each growth curve is drawn from a population of related curves. The model’s fixed effects estimate the means of the distribution. These terms estimate the variability around that mean. We did not have any a priori hypotheses about the values of these scales, so do not discuss them any further. Then the correlations. 10.2.2 Posterior predictive checks Bayesian models are generative; they describe how the data could have been generated. One way to evaluate the model is to have it simulate new observations. If the simulated data closely resembles the observed data, then we have some confidence that our model has learned an approximation of how the data could have been generated. Figure 10.9 depicts the density of the observed data from each year of the study versus 200 posterior simulations. Because the simulations closely track the density of the observed data, we can infer that the model has learned how to generate data from each year of the study. Figure 10.9: Posterior predictive density for the observed data from each year of the study. The x-axis represents the outcome measure—the proportion of looks to the target image—and the y-axis is the density of those values at year. At age 3, there is a large density of looks around chance performance (.25) with a rightward skew (above-chance looks are common). At age 4 and age 5, a bimodal distribution emerges, reflecting how looks start at chance and reliably increase to above-chance performance. Each light line is a simulation of the observed data from the model, and the thick lines are the observed data. Because the thick line is surrounded by light lines, we visually infer that the the model faithfully approximates the observed data. We can ask the model make even more specific posterior predictions. Below we plot the posterior predictions for random participants. This is the model simulating new data for these participants. set.seed(09272017) ppred &lt;- d_m %&gt;% sample_n_of(8, ResearchID) %&gt;% tristan::augment_posterior_predict(b, newdata = ., nsamples = 100) %&gt;% mutate(trials = Primary + Others) ggplot(ppred) + aes(x = Time, y = Prop, color = Study, group = Study) + geom_line(aes(y = .posterior_value / trials, group = interaction(.draw, Study)), alpha = .20) + geom_line(size = 1, color = &quot;grey50&quot;) + facet_wrap(&quot;ResearchID&quot;) + theme( legend.position = c(.95, 0), legend.justification = c(1, 0), legend.margin = margin(0)) + guides(color = guide_legend(title = NULL, override.aes = list(alpha = 1))) + labs( title = &quot;Observed means and 100 simulations of new data&quot;, x = &quot;Time after target onset [ms]&quot;, y = &quot;Proportion looks to target&quot;) Or we can plot the linear predictions. These are posterior predictions of the log-odds of looking to target before adding binomial noise. lpred &lt;- d_m %&gt;% sample_n_of(8, ResearchID) %&gt;% tristan::augment_posterior_linpred(b, newdata = ., nsamples = 100) ggplot(lpred) + aes(x = Time, y = .posterior_value, color = Study) + geom_line(aes(group = interaction(Study, ResearchID, .draw)), alpha = .1) + facet_wrap(&quot;ResearchID&quot;) + geom_point(aes(y = qlogis(Prop)), shape = 1) + theme( legend.position = c(.95, 0), legend.justification = c(1, 0), legend.margin = margin(0)) + guides(color = guide_legend(title = NULL, override.aes = list(alpha = 1))) + labs( title = &quot;Observed data and 100 posterior predictions&quot;, x = &quot;Time after target onset [ms]&quot;, y = &quot;Posterior log-odds&quot;) 10.2.3 Formal model specification TJM: Ignore this section for now. It might go to an appendix. Basically, I think I need to briefly summarize and justify the priors used in the analysis. We used moderately informative priors for the main regression effects. b ~ Normal(mean = 0, sd = 1) When we computed the orthogonal polynomial features for Time, they were rescaled so that the linear feature ranged from −.5 to .5. Under this scaling a unit change in Time1 was equal to change from the start to the end of the analysis window. The polynomial features for the Time had the following ranges: d_m %&gt;% distinct(ot1, ot2, ot3) %&gt;% tidyr::gather(&quot;Feature&quot;, &quot;Value&quot;) %&gt;% group_by(Feature) %&gt;% summarise(Min = min(Value), Max = max(Value), Range = Max - Min) %&gt;% mutate_if(is.numeric, round, 2) %&gt;% mutate(Feature = stringr::str_replace(Feature, &quot;ot(\\\\d)&quot;, &quot;Time^\\\\1^&quot;)) %&gt;% knitr::kable() Feature Min Max Range Time1 -0.50 0.50 1.00 Time2 -0.33 0.60 0.93 Time3 -0.63 0.63 1.26 Under the Normal(0, 1) prior, before seeing any data, we expect 95% of plausible effects to fall in the range ±1.96, which is an adequate range for these growth curve models. For example, consider just the effect of Time1. If a listener starts at chance performance, 25% or -1.1 logits, and increases to, say, 65% or 0.62, the effect of a unit change in Time1 would be a change of 1.72 logits. This magnitude of effect is accommodated by our Normal(0, 1) prior. _Here I would have to also describe the random effects structure. For the hierarchical part of the model, I used RstanARM’s decov() prior which simultaneously sets a prior of the variances and correlations of the model’s random effect terms. For these terms, I used the default prior for the variance terms and used a weakly informative LKJ(2) prior on the random effect correlations. Under LKJ(1) supports all correlations in the range ±1, but under LKJ(2) extreme correlations are less plausible. In the figure below, we see that the LKJ(2) prior nudges some of the probability mass away from ±1 towards the center. The motivation for this kind of prior was regularization: We give the model a small amount of information to nudge it away from extreme, degenerate values. References "],
["visualize-looks-to-each-image-type.html", "Chapter 11 Visualize looks to each image type 11.1 Comparing strong versus weak foils 11.2 Preparing data for the model 11.3 Look for individual differences in competitor sensitivity 11.4 Interim summary", " Chapter 11 Visualize looks to each image type We continue our exploration of the raw data by aggregating looks to each image type. Earlier we cleaned the data to remove trials with excessive missing data and blocks of trials with too few trials. Read in that data. Plot growth curves to each AOI. The looks to target increase year over year which decreases the remaining proportion of looks for the other three images each year. To study the relative propensity of looking to each image, we instead can use the log-odds of looking to each AOI versus the unrelated image. #&gt; Warning: Removed 10559 rows containing non-finite values (stat_smooth). Each curve is the log odds of looking to the target, phonological foil, and semantic foil versus the unrelated word. Positive values mean more looks to an image type than the unrelated. If you think of the y axis as the image’s relatedness to the target, you can see a time course of relatedness in each panel: Here early phonological effects meaning early relatedness and later, flatter semantic effects meaning late relatedness. (These effects make even more sense sense if phonological representations affect processing before semantic ones.) This plot suggests an important finding: Children becoming more sensitive to the phonological and semantic foils as they grow older. (I use the verb suggest because this is still a preliminary, unmodeled finding.) Jan and I had made opposite predictions about whether this would happen. Her argument, I think, was that children become better at word recognition by becoming better able to inhibit interference from competing words. This plot would suggest that they show increased sensitive to the target and foils words by looking less to the unrelated word as they age and reapportioning those looks to the other three lexically relevant words. 11.1 Comparing strong versus weak foils In Law et al. (2016), we ignored trials for certain items where we didn’t think the phonological or semantic similarity was strong enough. The two sets of phonological foils are shown below. Table 11.1: Trials with strong phonological foils. Target PhonologicalFoil SemanticFoil Unrelated bear bell horse ring bee bear fly heart bell bee drum swing dress drum shirt swing drum dress bell sword flag fly kite pear fly flag bee pen heart horse ring bread heart horse ring pan horse heart bear pan pan pear spoon vase pan pear spoon bell pear pen cheese ring pear pen cheese vase pen pear sword van vase van gift swan Table 11.1: Trials with weak phonological foils. Target PhonologicalFoil SemanticFoil Unrelated bread bear cheese vase cheese shirt bread van gift kite vase bread kite gift flag shirt ring swing dress flag shirt cheese dress fly spoon swan pan drum swan spoon bee bell swan spoon bee ring swing spoon kite heart sword swan pen gift van pan horse sword The stronger phonological foils are pairs where the syllable onsets are the same. The weaker foils include rime pairs, pairs where the words have different syllable onsets, and pairs where the onsets differ by a phonetic feature. Table 11.2: Trials with strong semantic foils. Target PhonologicalFoil SemanticFoil Unrelated bear bell horse ring bee bear fly heart bell bee drum swing bread bear cheese vase cheese shirt bread van dress drum shirt swing drum dress bell sword fly flag bee pen horse heart bear pan pan pear spoon vase pan pear spoon bell pear pen cheese ring pear pen cheese vase shirt cheese dress fly spoon swan pan drum Table 11.2: Trials with weak semantic foils. Target PhonologicalFoil SemanticFoil Unrelated flag fly kite pear gift kite vase bread heart horse ring bread heart horse ring pan kite gift flag shirt pen pear sword van ring swing dress flag swan spoon bee bell swan spoon bee ring swing spoon kite heart sword swan pen gift van pan horse sword vase van gift swan The strong semantic foils belong to the same category and the weaker ones have a less obvious relationship (ring and dress). We visually confirm that the strong versus weak foils behave differently. #&gt; Warning: Removed 48304 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 43071 rows containing non-finite values (stat_smooth). What’s going on here: The weak phonological foils are indeed weaker than the strong foils. The strong semantic foils appear stronger than the weak ones. The strong foils show a growth curve pattern of increasing looks away from baseline and there a developmental difference among the growth curves for each time point. Children have a lower advantage for the target (vs unrelated) in weak foil trials because… why? My reading is that if the semantic or phonological foil is effective, children will look at it instead of the unrelated image. Conversely, if the semantic or phonological foil are less effective, children will look more to the unrelated image, which pulls down the ratio of looks to target versus the unrelated image. In the above plots, we fixed the denominator to be the number of looks to the unrelated image and varied the numerator. In the plots below, we fix the numerator to be the looks to the target and vary the denominator to looks to target versus looks to each foil. #&gt; Warning: Removed 37498 rows containing non-finite values (stat_smooth). Curves in both panels attain the same height, so phonological and unrelated foils affect processing equally later in the trial. The strong phonological foils curves in the Target vs Phonological comparison rise later than the weak foils, reflecting early looks to the phonological foils. #&gt; Warning: Removed 29775 rows containing non-finite values (stat_smooth). Curves in the two panels do not attain the same height, so the semantic foil reduces odds of fixating to the target later on in a trial. There appears to be no difference in strong and weak foils in Year 2 and Year 3, so I might be able to collapse to remove this distinction and include more items in the analysis. 11.2 Preparing data for the model #&gt; Modelling options: #&gt; List of 4 #&gt; $ bin_width : num 3 #&gt; $ start_time: num 250 #&gt; $ end_time : num 1500 #&gt; $ bin_length: num 50 #&gt; As in the earlier models, we downsampled the data into 50-ms (3-frame) bins in order to smooth the data. We modeled the looks from 250 to 1500 ms. Lastly, we aggregated looks by child, study and time, and created orthogonal polynomials to use as time features for the model # library(lme4) # m &lt;- glmer( # cbind(Primary, Unrelated) ~ # Study * (ot1 + ot2 + ot3) + # (ot1 + ot2 + ot3 | ResearchID) + # (ot1 + ot2 + ot3 | Study:ResearchID), # family = binomial, # data = phon_d %&gt;% filter(Focus == &quot;PhonologicalFoil&quot;)) The final model should looks like this. I’ll have to run this for a few days. phon_d &lt;- phon_d %&gt;% filter(Focus == &quot;PhonologicalFoil&quot;) library(rstanarm) options(mc.cores = parallel::detectCores()) m &lt;- stan_glmer( cbind(Primary, Unrelated) ~ Study * (ot1 + ot2 + ot3) + (ot1 + ot2 + ot3 | ResearchID) + (ot1 + ot2 + ot3 | Study:ResearchID), family = binomial, prior = normal(0, 1, autoscale = FALSE), prior_intercept = normal(0, 2), prior_covariance = decov(2, 1, 1), control = list(adapt_delta = .99), data = phon_d) readr::write_rds(m, &quot;./data/stan_aim1_phon_model2.rds.gz&quot;) For this model structure, we estimate two growth curves simultaneously for each year of the study. Each growth curve is the log-odds of fixating on a certain image relative to the unrelated image. We use an indicator variable focus to record whether the image is the target or the phonological foil. Actually, I am not sure I need to estimate two growth curves simulataneously yet. I could just estimate log-odds of looking to Phonological vs Unrelated directly. This would let me estimate year over year changes, the time course of the effects and maybe some individual differences in looks to the phonological foil. I could posterior-predict looks at each time bin and see how the intervals compare to zero. Hmmm, this is not working quite right yet. 11.3 Look for individual differences in competitor sensitivity […put this on hold for a while…] 11.4 Interim summary Visual evidence that the semantic foil and phonological foil become more relevant (compared to unrelated foil) each year. Our previous distinction between strong and weak foils still applies, although it might be better to exclude only the (a priori) weakest foils, like the rime phonological foils. References "],
["aim-2-method.html", "Chapter 12 Aim 2: Method", " Chapter 12 Aim 2: Method 12.0.1 Visual stimuli The images used in the experiment consisted of color photographs on gray backgrounds. These images were piloted in a preschool classroom to ensure that children consistently used the same label for familiar objects and did not consistently use the same label for novel/unfamiliar objects. "],
["prepare-and-explore-the-data.html", "Chapter 13 Prepare and explore the data", " Chapter 13 Prepare and explore the data "],
["vw-experiment-items.html", "A Items used in the visual world experiment", " A Items used in the visual world experiment Each row of the table represents a set of four images used in a trial for the experiment. There were two blocks of trials with different images and trial orderings. For the two unrelated foils with more than one word listed, the first word was used in block one and the second in block two. Target Phonological Semantic Unrelated bear bell horse ring bee bear fly heart bell bee drum swing bread bear cheese vase cheese shirt bread van dress drum shirt swing drum dress bell sword flag fly kite pear fly flag bee pen gift kite vase bread heart horse ring bread/pan horse heart bear pan kite gift flag shirt pan pear spoon vase pear pen cheese ring/vase pen pear sword van ring swing dress flag shirt cheese dress fly spoon swan pan drum swan spoon bee bell swing spoon kite heart sword swan pen gift van pan horse sword vase van gift swan "],
["mp-experiment-items.html", "B Items used in the mispronunciation experiment", " B Items used in the mispronunciation experiment The stimuli changed between Year 1 and Year 2 so that dog/tog was replaced with rice/wice. Time Points Word Group Condition Audio (IPA) Familiar Object Unfamiliar Object 1 dog Correct Production /dɔg/ dog wombat Mispronunciation /tɔg/ dog wombat Nonword /vef/ ball sextant 1, 2, 3 cake Correct Production /kek/ cake horned melon Mispronunciation /gek/ cake horned melon Nonword /pʌm/ book churn 1, 2, 3 duck Correct Production /dʌk/ duck toy creature Mispronunciation /gʌk/ duck toy creature Nonword /ʃæn/ cup reed 1, 2, 3 girl Correct Production /gɜ˞l/ girl marmoset Mispronunciation /dɜ˞l/ girl marmoset Nonword /nedʒ/ car work holder 1, 2, 3 shoes Correct Production /ʃuz/ shoes flasks Mispronunciation /suz/ shoes flasks Nonword /giv/ sock trolley 1, 2, 3 soup Correct Production /sup/ soup steamer Mispronunciation /ʃup/ soup steamer Nonword /tʃim/ bed pastry mixer 2, 3 rice Correct Production /ɹaɪs/ rice anise Mispronunciation /waɪs/ rice anise Nonword /bep/ ball sextant "],
["related-work.html", "C Related Work", " C Related Work In this section, I clarify relationships between this project and other word recognition research reported from our lab. In short, our lab has reported results about the two-image and four-image experiments from cross-sectional samples, describing child-level measures that predict performance in these tasks. In contrast, my dissertation 1) focuses on the longitudinal development of word recognition and 2) engages with the fine-grained details of lexical processing. Law and Edwards (2015) analyzed a different version of the mispronunciation experiment on a different sample of children (n = 34, 30-46 months old). This earlier version included both real word and the mispronunciation of the real word in the same block of trial. For example, a child would hear “dog” and “tog” during the same session of the experiment. This design might subtly temper the effect of mispronounced stimuli by allowing the listener to compare the mispronunciation to its correctly produced counterpart. The version of the experiment in Specific Aim 2 separates the real words and mispronunciations so that a child never hears a familiar word and its mispronunciation during the same block of trials. With this design, there is no explicit point of comparison for the mispronunciation, and the child has to rely on his or her own lexical representations when processing these words. Law et al. (2016) analyzed data from the four-image experiment in Specific Aim 1. This study featured a diverse cross-sectional sample of 60 children, half of whom received the experiment in African American English and half received it in Mainstream American English. The sample ranged in age from 28 to 60 months. The study “borrowed” data from 23 participants from Year 1 of the longitudinal study to enrich parts of the samples demographics. For this manuscript, we analyzed how vocabulary and maternal education predicted looking patterns, including relative looks to the semantic and phonological foils. Mahr and Edwards (under review) was the manuscript I originally authored for my preliminary examinations. The paper analyzes the same kinds of relations as Weisleder and Fernald (2013) which showed that lexical processing efficiency mediated the effect of language input on future vocabulary size. In particular, I asked whether word recognition performance on the four-image task of Specific Aim 1, vocabulary size, and home language input data from Year 1 predicted vocabulary size at Year 2. The paper only examined looks to the familiar image from one year of the study, so it did not analyze any lexical competition effects or the development of word recognition within children. References "],
["references.html", "References", " References "]
]
