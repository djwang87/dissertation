[
["index.html", "My dissertation Development of word recognition in preschoolers Updates", " My dissertation Tristan Mahr 2017-10-13 Development of word recognition in preschoolers This book, when finished, will contain my dissertation research. Updates 2017-08-18: I migrated what I wrote for my dissertation proposal. In doing so, I worked all the kinks required to generate a nice web version, a lovely pdf version, and a… uh… functional Word document version. Last compiled: 2017-10-13 14:59:06 "],
["scratch-paper.html", "Chapter 1 Scratch paper 1.1 Bookdown cheatsheet 1.2 Debug info", " Chapter 1 Scratch paper This book is made with bookdown, an R package/tool-chain for creating a books in multiple formats. This chapter is just a placeholder section and some scratch-paper so that I have some examples on-hand of how to use bookdown’s syntax and features. This is a book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). For now, you have to install the development versions of bookdown from Github: devtools::install_github(&quot;rstudio/bookdown&quot;) Code settings: knitr::opts_chunk$set( tidy = FALSE, collapse = TRUE, comment = &quot;#&gt;&quot;, out.width = 80 ) options(width = 80) 1.1 Bookdown cheatsheet 1.1.1 Cross-references to sections The headings above were written with the following markdown: ## Bookdown cheatsheet ### Cross-references to sections {#manual-section-label-demo} The first one gets an implicit label. The second one has an explicit section label. I can refer to Section \\@ref(bookdown-cheatsheet) and [link to it](#bookdown-cheatsheet) with its implicit label. I can refer to Section \\@ref(manual-section-label-demo) and [link to it](#manual-section-label-demo) with its explicit label. I can refer to Section 1.1 and link to it with its implicit label. I can refer to Section 1.1.1 and link to it with its explicit label. 1.1.2 Cross-references to appendices The sample principles apply to appendices. This is a reference to [an appendix](#mp-experiment-items) The number of that appendix \\@ref(mp-experiment-items). I hope. Both: See [Appendix \\@ref(mp-experiment-items)](#mp-experiment-items) This is a reference to an appendix The number of that appendix A. I hope. Both: See Appendix A 1.1.3 Cross-references to tables The chunk label `table-single` provides an implicit label for Table \\@ref(tab:table-single). ```{r table-single, echo = FALSE} knitr::kable( head(mtcars[, 1:5], 5), booktabs = TRUE, caption = &#39;A table of the first 5 rows of the mtcars data.&#39; ) ``` The chunk label table-single provides an implicit label for Table 1.1. Table 1.1: A table of the first 5 rows of the mtcars data. mpg cyl disp hp drat Mazda RX4 21.0 6 160 110 3.90 Mazda RX4 Wag 21.0 6 160 110 3.90 Datsun 710 22.8 4 108 93 3.85 Hornet 4 Drive 21.4 6 258 110 3.08 Hornet Sportabout 18.7 8 360 175 3.15 1.1.4 Figure references and using text references as captions The caption for Figure \\@ref(fig:cat) is defined as a _text reference_ below and passed to the `fig.cap` chunk option. (ref:cat-cap) This is a happy cat. ```{r cat, fig.cap = &quot;(ref:cat-cap)&quot;, out.width = &quot;30%&quot;, fig.show = &quot;hold&quot;} knitr::include_graphics( rep(&quot;./misc/happy-cat-grooming-itself-vector-file.png&quot;, 2) ) ``` The caption for Figure 1.1 is defined as a text reference below and passed to the fig.cap chunk option. knitr::include_graphics( rep(&quot;./misc/happy-cat-grooming-itself-vector-file.png&quot;, 2) ) Figure 1.1: This is a happy cat. 1.2 Debug info str(list(html = is_html_output(), latex = is_latex_output(), word = is_word_output(), width = options(&quot;width&quot;)[[1]])) #&gt; List of 4 #&gt; $ html : logi TRUE #&gt; $ latex: logi FALSE #&gt; $ word : logi FALSE #&gt; $ width: int 80 devtools::session_info() #&gt; - Session info --------------------------------------------------------------- #&gt; setting value #&gt; version R version 3.4.1 (2017-06-30) #&gt; os Windows 7 x64 SP 1 #&gt; system x86_64, mingw32 #&gt; ui RTerm #&gt; language (EN) #&gt; collate English_United States.1252 #&gt; tz America/Chicago #&gt; date 2017-10-13 #&gt; #&gt; - Packages ------------------------------------------------------------------- #&gt; package * version date source #&gt; assertthat 0.2.0 2017-04-11 CRAN (R 3.3.2) #&gt; backports 1.1.1 2017-09-25 CRAN (R 3.4.1) #&gt; bookdown 0.5 2017-08-20 CRAN (R 3.4.1) #&gt; clisymbols 1.2.0 2017-08-04 Github (gaborcsardi/clisymbols@e49b4f5) #&gt; crayon 1.3.4 2017-09-16 CRAN (R 3.4.1) #&gt; desc 1.1.1 2017-08-03 CRAN (R 3.4.1) #&gt; devtools 1.13.3.9000 2017-09-13 Github (hadley/devtools@a0c8d73) #&gt; digest 0.6.12 2017-01-27 CRAN (R 3.3.2) #&gt; evaluate 0.10.1 2017-06-24 CRAN (R 3.4.1) #&gt; highr 0.6 2016-05-09 CRAN (R 3.2.3) #&gt; htmltools 0.3.6 2017-04-28 CRAN (R 3.4.0) #&gt; knitr 1.17 2017-08-10 CRAN (R 3.4.0) #&gt; magrittr 1.5 2014-11-22 CRAN (R 3.1.2) #&gt; memoise 1.1.0 2017-04-21 CRAN (R 3.3.2) #&gt; pkgbuild 0.0.0.9000 2017-09-13 Github (r-lib/pkgbuild@6574561) #&gt; pkgload 0.0.0.9000 2017-09-15 Github (r-lib/pkgload@7fd5f55) #&gt; R6 2.2.2 2017-06-17 CRAN (R 3.4.0) #&gt; Rcpp 0.12.13 2017-09-28 CRAN (R 3.4.2) #&gt; rlang 0.1.2.9000 2017-09-20 Github (tidyverse/rlang@99105b6) #&gt; rmarkdown 1.6 2017-06-15 CRAN (R 3.4.0) #&gt; rprojroot 1.2 2017-01-16 CRAN (R 3.3.2) #&gt; sessioninfo 1.0.1 2017-09-13 Github (r-lib/sessioninfo@e813de4) #&gt; stringi 1.1.5 2017-04-07 CRAN (R 3.3.3) #&gt; stringr 1.2.0 2017-02-18 CRAN (R 3.3.2) #&gt; usethis 0.0.0.9000 2017-09-13 Github (r-lib/usethis@f2a6ac7) #&gt; withr 2.0.0 2017-09-15 Github (jimhester/withr@d1f0957) #&gt; yaml 2.1.14 2016-11-12 CRAN (R 3.3.2) last_four_commits &lt;- git2r::commits(git2r::repository(&quot;.&quot;), n = 4) msgs &lt;- lapply(last_four_commits, methods::show) #&gt; [bd59aa2] 2017-10-13: actually save a csv #&gt; [fa1360c] 2017-10-13: clean up random effects extraction script #&gt; [09fd4c8] 2017-10-12: update site #&gt; [a634509] 2017-10-12: using new scores csv "],
["front-matter.html", "Chapter 2 Front Matter", " Chapter 2 Front Matter About This Document This document outlines the research questions, data, and methods for my dissertation. This proposal started out as a grant-writing project, so it has some of the touchstones of NIH F31 grant (Specific Aims, Significance, Approach), but these sections have been expanded considerably. Date of oral presentation of dissertation proposal: April 3, 2017. Dissertation Committee Members Jan Edwards, primary mentor and chair, Department of Hearing and Speech Sciences, University of Maryland Susan Ellis Weismer, official advisor at UW-Madison, Department of Communication Sciences and Disorders Margarita Kaushanskaya, Department of Communication Sciences and Disorders Audra Sterling, Department of Communication Sciences and Disorders David Kaplan, Department of Educational Psychology Bob McMurray, Department of Psychological and Brain Sciences, University of Iowa Planned Dissertation Format Three thematically related manuscripts, one for each specific aim, to be completed by Summer 2018. "],
["specific-aims.html", "Chapter 3 Specific Aims 3.1 Specific Aim 1 (Familiar Word Recognition and Lexical Competition) 3.2 Specific Aim 2 (Referent Selection and Mispronunciations) 3.3 Specific Aim 3 (Computational Modeling) 3.4 Summary", " Chapter 3 Specific Aims Individual differences in language ability are apparent as soon as children start talking, but it is difficult to identify children at risk for language delay or disorder. Recent work suggests word recognition efficiency—that is, how well children map incoming speech to words—may help identify early differences in children’s language trajectories. Children learn spoken language by listening to caregivers, so children who are faster at recognizing words have an advantage for word learning. This view is borne out by some studies suggesting that children who are faster at processing words show greater vocabulary gains months later (e.g., Weisleder &amp; Fernald, 2013). We do not know, however, how word recognition itself develops over time within a child. This is an important open question because word recognition may provide a key mechanism for understanding how individual differences emerge in word learning and persist into early language development. Without a developmental account of word recognition, we lack the context for understanding individual differences in lexical processing. Thus, even the big-picture questions are unclear: Do early differences persist over time so that faster processors remain relatively fast later in childhood? Or, is such a question ill-posed because the magnitude of the differences among children shrink with age? I plan to address this gap in knowledge by analyzing three years of word recognition data collected in recently completed longitudinal study of 180 children. In particular, I will examine the development of familiar word recognition, lexical competition, and fast referent selection (the ability to map novel words to novel objects in the moment). Through these analyses, I will develop a fine-grained description of how the dynamics of word recognition change year over year, and I will study how differences in word recognition performance relate to child-level measures (such as vocabulary and speech perception). I will complement these empirical analyses with computational cognitive models. With these models, I will simulate the word recognition data from each year and study how the models need to change to adapt to children’s developing word recognition abilities. These simulations can identify plausible psychological mechanisms that underlie changes in word recognition behavior. 3.1 Specific Aim 1 (Familiar Word Recognition and Lexical Competition) To characterize the development of familiar word recognition and lexical competition, I will analyze data from a visual world paradigm experiment, conducted at age 3, age 4, and age 5. In these eyetracking experiments, children were presented with four images of familiar objects and heard a prompt to view one of the images. The four images included a target word (e.g., bell), a semantically related word (drum), a phonologically similar word (bee), and an unrelated word (swing). I will use a series of growth curve analyses to describe how children’s familiar word recognition develop year over year. Of interest is how individual differences at Year 1 persist into Year 3. I will also analyze how expressive vocabulary and lexical processing develop together over time. Lastly, I will examine the children’s looks to the distractors to study the developmental course of lexical competition from similar sounding and similar meaning words. Changes in sensitivity to competing words can reveal how lexical competition emerges as a byproduct of learning new words. 3.2 Specific Aim 2 (Referent Selection and Mispronunciations) To characterize how fast referent selection develops longitudinally, I will analyze data from a looking-while-listening mispronunciation experiment, conducted at age 3, age 4, and age 5. In these eyetracking experiments (Law &amp; Edwards, 2015; based on White &amp; Morgan, 2008), children saw an image of a familiar object and an unfamiliar object, and they heard either a correct production of the familiar object (e.g., soup), a one-feature mispronunciation of the familiar object (shoop), or a novel word unrelated to either image (cheem). The correct productions test familiar word recognition and the nonwords test fast referent selection. The mispronunciations test a child’s phonological categories (whether the child permits, rejects, or equivocates about mispronunciations). I will use growth curve analyses to study how children’s responses to the three word types change over time. I will examine familiar word recognition and fast referent selection to determine which feature of lexical processing better predicts vocabulary growth. I plan to examine dissociations or asymmetries in these forms of processing within children as a way to empirically assess the claim that “novel word processing (referent selection) is not distinct from familiar word recognition” (McMurray, Horst, &amp; Samuelson, 2012). Finally, I will examine how individual differences in vocabulary and speech perception predict responses to mispronunciations and novel words. 3.3 Specific Aim 3 (Computational Modeling) To identify plausible psychological mechanisms underlying the development of word recognition, I will simulate the word recognition data using cognitive computation models. The TRACE model of word recognition (McClelland &amp; Elman, 1986) has been used to simulate word recognition data from adults (Allopenna, Magnuson, &amp; Tanenhaus, 1998), adults with aphasia (Mirman, Yee, Blumstein, &amp; Magnuson, 2011), toddlers (Mayor &amp; Plunkett, 2014), and adolescents with language impairments (McMurray, Samelson, Lee, &amp; Tomblin, 2010). In this model, incoming acoustic information activates perceptual units which in turn activate phoneme units which in turn activate word units. Connections are interactive, so the model can accommodate top-down processing effects and competition among units through inhibition. The model is controlled by psychological parameters like inhibition strength, activation decay rates, and lexicon size and composition. The advantage of using this model is how one can map different behavioral patterns onto changes in model parameterizations to develop a plausible psychological account of developmental changes. I will simulate the Year 1 word recognition data in TRACE, and I will study how the model’s parameters need to change in order to accommodate data from Year 2 and Year 3. I will examine how different model parameters map onto individual differences in word recognition. For instance, under what modeling conditions are mispronunciations of familiar words accepted and do these conditions correspond to child-level differences in vocabulary or speech perception? These simulations will provide a psychological account for the developmental trends and individual variation in word recognition. 3.4 Summary This project investigates how word recognition develops during the preschool years. There has been no research studying word recognition longitudinally after age two. Findings will show how individual differences in lexical processing change over time and can reveal how low-level mechanisms underlying word recognition mature longitudinally in children. These findings will have translational value by studying processing abilities that subserve word learning and by assessing the predictive relationships between early word recognition ability and later language outcomes. References "],
["significance.html", "Chapter 4 Significance 4.1 Public Health Significance 4.2 Scientific Significance", " Chapter 4 Significance 4.1 Public Health Significance Vocabulary size in preschool is a robust predictor of later language development, and early language skills predict early literacy skills at school entry (P. L. Morgan, Farkas, Hillemeier, Hammer, &amp; Maczuga, 2015). By studying the mechanisms that shape word learning, we can understand how individual differences in language ability arise and identify strategies for closing language gaps between children. Word recognition—the process of mapping incoming speech sounds to known or novel words—has been shown to predict later language outcomes. We do not know how this ability develops over time, and we do not know when word recognition is most predictive of future outcomes. This project will provide an integrated account of how word recognition and its relationship with vocabulary size change from age 3 to age 5. 4.2 Scientific Significance 4.2.1 Lexical Processing Dynamics Mature listeners recognize words by continuously evaluating incoming speech input for possible word matches through lexical competition. The first part of a word activates multiple candidate words in parallel, and these candidates compete so that the best-fitting word is recognized. For example, the onset “bee” might activate the candidates bee, beam, beetle, beak, beaker, beginning, and so on, but an additional “m” would narrow the candidates to just beam. Semantic relationships also influence lexical processing, and cascading phonological-semantic effects—e.g., where castle activates the phonologically similar candy which in turn activates the semantically related sweet—have been demonstrated (Marslen-Wilson &amp; Zwitserlood, 1989). Both low-level phonetic cues and high-level grammatical, semantic and pragmatic information can influence this process, but the continuous processing of multiple competing candidates is the essential dynamic underlying word recognition (Magnuson, Mirman, &amp; Myers, 2013). What about young children who know considerably fewer words? Eyetracking studies with toddlers have suggested a developmental continuity between toddlers and adult listeners. Children recognize words incrementally (Swingley, Pinto, &amp; Fernald, 1999), match truncated words to their intended referents (Fernald, Swingley, &amp; Pinto, 2001), and use information from neighboring words in a sentence to facilitate word recognition. This information can be grammatical: Lew-Williams and Fernald (2007) found that Spanish-acquiring preschoolers can use grammatical gender on determiners (el or la) to anticipate the word named in a two-object word recognition task. The information can also be subcategorical phonetic variation: We found that English-acquiring toddlers look earlier to a named image when the coarticulatory formant cues on word the predict the noun of the sentence, compared to tokens with neutral coarticulation (Mahr, McMillan, Saffran, Ellis Weismer, &amp; Edwards, 2015). There is some evidence for lexical competition where children are sensitive to phonological and semantic similarities among words. Ellis Weismer, Haebig, Edwards, Saffran, and Venker (2016) showed that toddlers (14–29 months old) looked less reliably to a named image when the onscreen competitor was a semantically related word or perceptually similar image. In Law, Mahr, Schneeberg, and Edwards (2016), preschoolers (28–60 months old) demonstrated sensitivity to semantic and phonological competitors in a four-image eyetracking task. Huang and Snedeker (2011) presented evidence of cascading semantic-phonological activation in five-year-olds such that for a target word like log, children looked more to an indirect phonological competitor like key (competing through its activation of lock) than they looked to an unrelated image like carrot. In contrast to these studies which all demonstrate interference from similar words, Mani and Plunkett (2010) demonstrated cross-modal phonological priming effects in 18-month-olds. In this study, a picture of prime word (e.g., cat or teeth) was presented in silence; then two images (e.g., cup and shoe) were presented, one of which was named (cup). Children on average looked more to the target word (like cup) when it was primed by an image of a phonological neighbor (like cat), and the children performed at chance when the prime was not related to the named word. Mani, Durrant, and Floccia (2012) found a similar result for cascading phonological-semantic priming with 24-month-olds: Children looked more to a target shoe compared to a distractor door when primed by an image of clock, assumed to activate sock which primed shoe. The above studies involved young children of different ages tested under different procedures, sometimes in different dialects and languages. Averaging these results together, so to speak, the studies suggest that early word recognition demonstrates some hallmarks of adult behavior: Continuous processing of words, integration of information from different levels of representation, and the influence of similar, unspoken words on recognition of a word. Nevertheless, we only have a fragmented view of how familiar word recognition and lexical competition develop within children. One open question is how lexical competition develops within children. For example, do phonological similar words exert more interference during word recognition as children grow older? As a guiding hypothesis, we can think of word learning as a gradual process where familiarity with a word moves from shallow receptive knowledge to deeper expressive knowledge. In adult listeners, words compete and inhibit one another, so that a word is truly “learned” and integrated into the lexicon when it can influence the processing of other words (a line of reasoning reviewed by Kapnoula, Packard, Gupta, &amp; McMurray, 2015). Increasing sensitivity to similar sounding words over time would reveal that children improve their ability to consider multiple candidates in parallel. By studying how sensitivity to similar-sounding and similar-meaning words develop over time and within ever-growing vocabularies, this project can reveal how children come to process words efficiently. Another avenue for studying word recognition is to examine how listeners respond to unfamiliar or novel stimuli. A productive line of research has found that children are sensitive to mispronunciations during word recognition (e.g., Swingley &amp; Aslin, 2000, 2002). White and Morgan (2008) presented toddlers with images of a familiar and novel object, and children heard a correct production of the familiar object, mispronunciations of the familiar object of varying severity, or an unrelated nonword. Toddlers looked less to a familiar word when the first segment was mispronounced. Moreover, they demonstrated graded sensitivity such that a 1-feature mispronunciation yielded more looks to an image than a 2-feature mispronunciation, and a 2-feature mispronunciation yielded more looks than a 3-feature one. Finally, in the nonword condition, the children looked more to the novel object than the familiar one, demonstrating fast referent selection as they associated novel words to novel objects in the moment. A similar pattern of effects was observed in the mispronunciation study by Law and Edwards (2015) with preschoolers mapping real words to familiar objects, nonwords to novel objects, and equivocating about mispronunciations of familiar words. As with lexical competition, it is unclear how children’s responses to mispronunciations and novel words change over time or how individual differences among children change over time. For example, do children become more forgiving of mispronunciations as they mature and learn more words? We might expect so, as children become more experienced at listening to noisy, degraded, or misspoken speech. Another open question involves the development of fast referent selection. At face value, we might expect a child’s ability to associate new words with unfamiliar objects to be more direct measure of word-learning capacity than a child’s ability to process known words. Under this assumption, we would expect individual differences in fast referent selection to be highly correlated with vocabulary growth. But McMurray et al. (2012) propose that the same basic process is at play in both recognition of familiar words and fast association of nonwords. In experiments, the observed behaviors are the same: Children hear a word and direct their attention to an appropriate referent. This project can tackle these questions by describing how mispronunciations are processed as children grow older and by examining whether familiar word recognition and fast referent selection dissociate and which one is a better predictor of vocabulary growth. 4.2.2 Individual Differences in Word Recognition We have a rough understanding of the development of word recognition, and these gaps in knowledge matter because young children differ in their word recognition abilities. These differences are usually measured using accuracy (a probability of recognizing to a word) or efficiency (a reaction time or some measure of how quickly accuracy changes over time). These differences are consequential too, as word recognition differences correlate with other language measures concurrently, retrospectively, and prospectively. The best predictor of lexical processing efficiency is concurrent vocabulary size: Children who know more words look more quickly and reliably to a named word (e.g., Law &amp; Edwards, 2015). This fact deserves a brief reflection: Suppose the information processing mechanism behind word recognition were just a naïve table search. Then this finding is somewhat puzzling: Children with larger lexicons have to find a needle in a larger haystack—yet this apparent liability is an advantage. That is why the search analogy is naïve. One explanation follows from the earlier described idea about graded word learning: Children become better at recognizing words as they learn more words because they extract regularities and discover similarities among words and develop more efficient lexical representations—the haystack develops regularity and becomes easier to search. Although it is a robust predictor of word recognition, vocabulary size is nonspecific. For lexical processing dynamics, vocabulary size can be considered an indicator for the organization and efficiency of a child’s lexicon, but it also correlates with other (meaningful) differences. Vocabulary is related to differences in speech perception (Cristia, Seidl, Junge, Soderstrom, &amp; Hagoort, 2014) and environmental factors like language input (e.g., Hart &amp; Risley, 1995; Hoff, 2003). For instance, measures of speech perception at 6–8 months predict vocabulary size at 24 months (Kuhl et al., 2008; e.g., Tsao, Liu, &amp; Kuhl, 2004), so processing predicts future vocabulary predicts concurrent processing. A related complication is the apparent predictive validity of word recognition measures. Marchman and Fernald (2008) found that vocabulary size and lexical processing efficiency at age 2 jointly predicted working memory scores and expressive language scores at age 8. This result would suggest domain-general processing advantages influence word learning. Fernald and Marchman (2012) also found that late talkers who looked more quickly to a named word at 18 months showed larger gains in vocabulary by 30 months compared to late-talkers who looked more slowly at 18 months. Weisleder and Fernald (2013) found that lexical processing and language input at 19 months predict vocabulary size at 25 months and that lexical processing mediated the effect of language input. Word recognition efficiency and vocabulary size are interconnected measures with concurrent and predictive associations. This project can clarify this relationship by examining the co-development of word recognition, vocabulary size, and speech perception. In particular, I will ask how individual differences in word recognition change over time alongside differences in vocabulary. I can also which features of word recognition (fast referent selection, lexical competition, etc.) are most predictive of vocabulary outcomes at age 5. The additional measures of speech perception can also help clarify the specific effects of vocabulary size on word recognition. 4.2.3 Computational Modeling One way to bolster a theory of word recognition is to implement the theory in a computational model and simulate human behavior with the model. If the model can produce responses like those of human listeners, then the behavioral data support the model and the model’s underlying theory. Models have adjustable parameters, and these generally correspond to plausible psychological constructs like the inhibition strength among competing units or a learning rate for modifying connections. Part of the simulation process therefore is to ask under what conditions a model simulates human behavior, and then interpret the simulations in terms of psychological mechanisms. Here is a hypothetical example of this modeling strategy: Suppose we want to investigate the finding that a larger vocabulary size predicts more efficient word recognition. We would ask under which parameters (i.e., model settings) a model with a large lexicon outperforms one with a smaller lexicon at word recognition. It might be the case that these results occur only when parameters are set so that the representations of speech sounds are comparatively noisier in a model with fewer words. In this scenario, the models provide a plausible psychological interpretation for the empirical findings: Children develop better representations of speech sounds as they learn words, and these better representations enable more efficient word recognition. Granted, this example is just a hypothetical case, but it illustrates how I intend to use computational models as a way to describe word recognition trends and variation in terms of psychological mechanisms and processing parameters. The TRACE model of speech perception and word recognition (McClelland &amp; Elman, 1986) is well suited for this kind of simulation work. TRACE can simulate a dozen or so empirical results from speech perception literature (Strauss, Harris, &amp; Magnuson, 2007, Table 1), and it has been used to simulate word-recognition data from adults (Allopenna et al., 1998), adults with aphasia (Mirman et al., 2011), and toddlers (Mayor &amp; Plunkett, 2014). McMurray et al. (2010) used TRACE to simulate word recognition results from adolescents with specific language impairment (SLI). They mapped certain theories about SLI onto different model parameters. To test the theory that listeners with SLI have impairments in acoustic perception, for example, they varied three of the model’s parameters: amount of noise added to the model’s mock-speech input (acoustic resolution), temporal spread of acoustic features in the input (temporal resolution), and rate of decay in the model’s acoustic feature detectors (perceptual memory). Other theories of SLI (and other model parameters) provided a closer fit to the observed data than the perceptual deficit theory. Specifically, lexical decay—“the ability to maintain words in memory” (p. 23)—was the most important model parameter, implying that individual differences in word recognition for listeners with SLI are rooted in lexical processes, as opposed to perceptual or phonological ones. This example shows how simulations with TRACE recast word recognition performance in terms of psychological processes. For this project, I will use TRACE to describe how cognitive processes and representations need to change to simulate the development of word recognition in preschoolers. 4.2.4 Summary This project studies word recognition in children over three years, so it will provide the first longitudinal study of word recognition in preschoolers. Children in this cohort cover a range of vocabulary scores at Time 1, and this variability allows one to investigate individual differences in vocabulary and word recognition over time and assess the predictive value of these measures. Furthermore, this project studies word recognition in two experimental tasks that can tap into different aspects of word recognition. Specifically, a four-image experiment with semantic and phonological foils allows me to study how lexical competition develops, and a two-image experiment with nonwords and mispronunciations enables me to study how fast referent selection develops over time as well. I will use mixed effects modeling to study not just gross measures of accuracy or interference from distractors, but the time course and lexical dynamics of word recognition using growth curve analyses. These empirical models of the longitudinal word recognition data will be supported by computational models, so that the developmental changes can be described by plausible psychological mechanisms of word recognition. References "],
["research-hypotheses.html", "Chapter 5 Research Hypotheses 5.1 Specific Aim 1 (Familiar Word Recognition and Lexical Competition) 5.2 Specific Aim 2 (Referent Selection and Mispronunciations) 5.3 Specific Aim 3 (Computational Modeling)", " Chapter 5 Research Hypotheses In this section, I outline the main hypotheses I plan to study for each specific aim. This section is intended to preregister the main analyses for this project, so I can cleanly separate confirmatory and exploratory results. 5.1 Specific Aim 1 (Familiar Word Recognition and Lexical Competition) Children’s accuracy and efficiency of recognizing words will improve each year. There are stable individual differences in lexical processing of familiar words such that children who are relatively fast at Year 1 remain relatively fast at Year 2 and Year 3. However, the magnitude of these individual differences diminishes over time, as children converge on a mature level of performance for this paradigm. As a consequence, individual differences in word recognition at age 3, for example, will be more discriminating and predictive of age 5 language outcomes than differences at age 4. Vocabulary size and lexical processing will be tightly correlated such that large year-over-year gains in one measure will predict large year-over-years gains in the other measure. Children will become more sensitive to lexical competitors as they age, based on the hypothesis that children discover similarities among words as a consequence of learning more and more words. Children will differ in their sensitivity to lexical competitors, and these individual differences will correlate with other child-level measures. 5.2 Specific Aim 2 (Referent Selection and Mispronunciations) Children’s accuracy and efficiency of recognizing real words and fast-associating nonwords will improve each year. Performance in real word recognition and fast association of nonwords will be highly correlated, based on the hypothesis that the same process (referent selection) operates in both situations. Under the alternative hypothesis, real word recognition and fast referent selection reflect different skills with different developmental trajectories. Thus, if there is any dissociation between recognition of real words and nonwords, it will be observed in younger children. Although these two measures will be correlated, I predict performance in the nonword condition will be a better predict of future vocabulary growth than performance in the real word condition. This hypothesis is based on the idea that fast referent selection is a more relevant skill for learning new words than recognition of known words. For the mispronunciations, I predict children with larger vocabularies (that is, older children) will be more likely to tolerate a mispronunciation as a production of familiar word compared to children with smaller vocabularies. Mispronunciations that feature later-mastered sounds (e.g., rice/wice) will be more likely to be associated to novel objects than earlier-mastered sounds (duck/guck). 5.3 Specific Aim 3 (Computational Modeling) The research questions for this aim are more exploratory than confirmatory. I plan to use the well-studied TRACE model of spoken word recognition (McClelland &amp; Elman, 1986). The TRACE model has no built-in semantic representations, so it will be used to model data from the two-image mispronunciation experiment (Aim 2) and a “semantic-less” subset of the data from the four-image experiment (Aim 1). Research questions include: What parameterizations of the TRACE model of word recognition account Year 1 word recognition data? If the default model of TRACE represents “default” adult listeners, how the children are Year 1 different than adults. How do these parameterizations have to change year over year to accommodate the data from older children at Year 2 and again at Year 3? What is the developmental story for these changing parameterizations? How does the model handle mispronunciations? What changes are required to accommodate mispronunciations and novel words? Mayor and Plunkett (2014) have simulated mispronunciation experiments like White and Morgan (2008), although it is not clear how the model can accommodate the current mispronunciation experiment which presents mispronunciations and nonwords. How does the model’s vocabulary parameters (namely the size and composition of the lexicon used for the simulations) correlate with the vocabulary size of participants? Can the model simulations incorporate other child-level measures? For example, do children who demonstrate poorer speech perception in a non-eyetracking speech discrimination task require lower levels of phoneme inhibition in their word recognition models? References "],
["methods.html", "Chapter 6 Methods 6.1 General Research Design: Participants 6.2 General Eyetracking Procedure 6.3 Specific Procedure: Aim 1 (Familiar Word Recognition and Lexical Competition) 6.4 Specific Procedure: Aim 2 (Referent Selection and Mispronunciations)", " Chapter 6 Methods 6.1 General Research Design: Participants Word recognition data and vocabulary data, among other measures, were collected over a three-year longitudinal study (R01DC002932; the Learning to Talk project). Children were 28–39 months-old at Time 1, 39–52 at Time 2, and 51–65 at Time 3. Approximately, 180 children participated at Time 1, 170 at Time 2, and 160 at Time 3. Of these children, approximately 20 were identified by their parents as late talkers. Prospective families were interviewed over telephone before participating in the study, and “children with an individualized education program or any parent-reported visual problems, language problems, or developmental delays were not scheduled for testing” (Law et al., 2016).1 Recruitment and data collection occurred at two Learning to Talk lab sites—one at the University of Wisconsin–Madison and the other at the University of Minnesota. Table 6.1 summarizes the cohort of children in each year of testing. The numbers and summary statistics here are approximate, describing children who participated at each year, but whose data may still be excluded from the analyses. Some potential reasons for exclusion include: excessive missing data during eyetracking, experiment or technology error, developmental concerns not identified until later in study, or a failed hearing screening. Final sample sizes will depend on the measures needed for an analysis and the results from data screening checks. For each project aim, I will disclose all measurements and data exclusions following guidelines by the Center for Open Science (Nosek et al., 2014). Table 6.1: Participant characteristics. Education levels: Low: less than high school, or high school; Middle: trade school, technical or associates degree, some college, or college degree; and High: graduate degree. Year 1 Year 2 Year 3 N 184 175 160 Boys, Girls 94, 90 89, 86 82, 78 Maternal education: Low, Middle, High 15, 98, 71 12, 92, 71 6, 90, 64 Dialect: MAE, AAE 171, 13 163, 12 153, 7 Parent-identified late talkers 20 19 16 Age (months): Mean (SD) [Range] 33 (3) [28–39] 45 (4) [39–52] 57 (4) [51–66] EVT-2 standard score: Mean (SD) 115 (18) 118 (16) 118 (14) PPVT-4 standard score: Mean (SD) 113 (17) 120 (16) — GFTA-2 standard score: Mean (SD) 92 (13) — 91 (13) 6.2 General Eyetracking Procedure Two eyetracking experiments were performed each year of the longitudinal study. These experiments followed the same essential procedure: During each trial, photographs of images appeared on a computer screen for a few seconds followed by a prompt to view one of the images (e.g., find the dog). This procedure measures a child’s real-time comprehension of words by capturing how the child’s gaze location changes over time in response to speech. 6.2.1 Experiment Administration Children participating in the study were tested over two lab visits (i.e., on different dates). The first portion of each visit involved “watching movies”—that is, performing two blocks of the eyetracking experiments. A play break or hearing screening occurred between the two eyetracking blocks, depending on the visit. Each eyetracking experiment was administered as a block of trials (24 for the four-image task and 38 for the two-image task). Children received two different blocks of each experiment. The blocks for an experiment differed in trial ordering and other features (see Specific Procedures). Experiment order and block selection were counterbalanced over children and visits. For example, a child might have received Exp. 1 Block A and Exp. 2 Block B on Visit 1 and next received Exp. 2 Block A and Exp. 1 Block B on Visit 2. The purpose of this presentation was to control possible ordering effects where a particular experiment or block benefited from consistently occurring first or second. Experiments were administered using E-Prime 2.0 and a Tobii T60XL eyetracker which recorded gaze location at a rate of 60 Hz. The experiments were conducted by two examiners, one “behind the scenes” who controlled the computer running the experiment and another “onstage” who guided the child through the experiment. At the beginning of each block, the child was positioned so the child’s eyes were approximately 60 cm from the screen. The examiners calibrated the eyetracker to the child’s eyes using a five-point calibration procedure (center of screen and centers of four screen quadrants). The examiners would repeated this calibration procedure if one of the five calibration points for one of the eyes did not calibrate successfully. During the experiment, the behind-the-scenes examiner monitored the child’s distance from the screen and whether the eyetracker was capturing the child’s gaze. The onstage examiner coached the child to stay fixated on the screen and repositioned the child as needed to ensure the child’s eyes were being tracked. Every six or seven trials in a block of an experiment, the experiment briefly paused with a reinforcing animation or activity. During these breaks, the onstage examiner could reposition the child if necessary before resuming the experiment. We used a gaze-contingent stimulus presentation. “After 2 s of familiarization time with the images in silence, the experiment paused to verify that the child’s gaze was being tracked. After 300 ms of continuous gaze tracking, the trial advanced. Otherwise, if the gaze could not be verified after 10 s, the trial advanced. This step ensured that for nearly every trial, the gaze was being tracked before playing the carrier phrase, or in other words, that the child was ready to hear the carrier stimuli” (Mahr &amp; Edwards, in revision). During Year 1 and Year 2, an attention-getter (e.g., check it out!) played 1 s following the end of the target noun. These reinforcers were dropped in Year 3 to streamline the experiment for older listeners. 6.2.2 Stimuli For both experiments, “stimuli were presented in children’s home dialect, either Mainstream American English (MAE) or African American English (AAE). We made an initial guess about what the home dialect was likely to be based on a number of factors, including the recruitment source and the child’s address. For most children, the home dialect was MAE. If we thought the home dialect might be AAE, a native AAE speaker who was a fluent dialect-shifter between AAE and MAE was scheduled for the lab visit, and she confirmed the home dialect by listening to the caregiver interact with the child during the consent procedure at the beginning of the visit” (Mahr &amp; Edwards, in revision). Prompts to view the target image of a trial (e.g., find the girl) used the carrier phrases “find the” and “see the”. These carriers were recording in the frame “find/see the egg” and cross-spliced with the target nouns to minimize coarticulatory cues on the determiner “the”. The images used in each experiment consisted of color photographs on gray backgrounds. These images were piloted in a preschool classroom to ensure that children consistently used the same label for familiar objects and did not consistently use the same label for novel/unfamiliar objects. 6.2.3 Data Preparation Data from both experiments were prepared using the same procedure. “We mapped the gaze x-y coordinates onto the images onscreen. We performed deblinking by interpolating short windows of missing data (up to 150 ms) if the child fixated on the same image before and after a missing data window. In other words, if the gaze did not shift to another image, and if the missing data window was short enough, that window was classified as a blink and interpolated, using the fixated image as the imputed value” (Mahr &amp; Edwards, in revision). Next, we performed trial-level cleaning. “We examined eyetracking data in the 2-s window following the onset of the target word. A trial was considered unreliable if at least 50% of the eyetracking data during the 2-s window was missing. These trials were not reliable because the child did not look at the display for the majority of the analysis window” (Mahr &amp; Edwards, in revision). If more than half of a child’s trials, combined across blocks, were unreliable, that child was excluded from analysis. “Finally, we downsampled our data into 50-ms bins, reducing the eyetracking sampling rate from 60 Hz to 20 Hz. This procedure smoothed out high-frequency noise in the data by pooling together data from adjacent frames” (Mahr &amp; Edwards, in revision). 6.3 Specific Procedure: Aim 1 (Familiar Word Recognition and Lexical Competition) Visual World Paradigm Task. In eyetracking studies with toddlers, two familiar images are usually presented: a target and a distractor. This experiment is a four-image eyetracking task that was designed to provide a more demanding word recognition task for preschoolers. In this procedure, four familiar images are presented onscreen followed by a prompt to view one of the images (e.g., find the bell!). The four images include the target word (e.g., bell), a semantically related word (drum), a phonologically similar word (bee), and an unrelated word (swing). Figure 6.1: Example display for the target bell with the semantic foil drum, the phonological foil bee, and the unrelated swing. 6.4 Specific Procedure: Aim 2 (Referent Selection and Mispronunciations) Mispronunciation Task. This experiment is an adaptation of the mispronunciation detection task by White and Morgan (2008) and Law and Edwards (2015). In this experiment, two images are presented onscreen—a familiar object and an unfamiliar object—and the child hears a prompt to view one of the images. In the correct pronunciation (or real word) and mispronunciation conditions, the child hears either the familiar word (e.g., soup) or a one-feature mispronunciation of the first consonant of the target word ([sh]oup). Importantly, within a block of trials, the child never hears both the correct and mispronounced forms of the word. These conditions are designed to test whether children map mispronunciations to novel words. To encourage fast referent selection, there were also trials in a nonword condition where the label was an unambiguous novel word (e.g., cheem presented with images of a bed and a novel-looking pastry mixer). Each nonword was constructed to match the phonotactic probability of one of the mispronunciations. Figure 6.2: Example display for a trial in which duck is mispronounced as “guck”. In a block of trials, there were 12 trials each from the nonword condition, correct production condition, and mispronunciation conditions, and children received two blocks of the experiment. A complete list of the items used in the experiment over the three years of the study is included in Appendix A. References "],
["outcome-measures.html", "Chapter 7 Outcome Measures 7.1 Sample Data", " Chapter 7 Outcome Measures The primary outcome measure for this project is word recognition performance, and I will study how it changes over time, under different experimental conditions, across children, and so on. I will measure word recognition performance using eyetracking growth curves. In this technique, I aggregate looking locations across trials to compute the proportion of looks to the target object (versus the distractor images) at each time point after the onset of the target word. This growth curve measures how the probability of fixating on a named object changes over time. Thus, children who have steeper accuracy growth curves demonstrate faster word recognition because the probability of looking at a named object increases more quickly over time. The other primary child-level measurements of interest for this project are vocabulary and speech perception. Expressive vocabulary was measured at all three years using the Expressive Vocabulary Test (EVT-2, Williams, 2007). Receptive vocabulary was measured at Year 1 and Year 2 using the Peabody Picture Vocabulary Test (PPVT-4, L. M. Dunn &amp; Dunn, 2007). These tests will provide child-level measures of vocabulary and vocabulary growth. At Year 1 and Year 2, we measured speech perception using a minimal-pair discrimination task (as in Baylis, Munson, &amp; Moller, 2008), and Year 2 and Year 3, we measured speech perception with a speech sound judgment task (SAILS as in Rvachew, 2006). Performance on these tasks will quantify individual differences in speech perception. Two other relevant measures include an articulation test administered at Year 1 and Year 3 (GFTA-2, Goldman &amp; Fristoe, 2000) and phonological awareness given at Year 2 and Year 3 (CTOPP-2, Wagner, Torgesen, Rashotte, &amp; Pearson, 2013). They are relevant because they draw on a child’s phonological knowledge of words, but I limit their consideration for only post-hoc, exploratory analyses because they measure a child’s speech production performance. 7.1 Sample Data Figure 7.1 shows the development of word recognition on the four-image task over the three years of the longitudinal study for two participants in this data set. To limit data-peeking, I only considered at data from the first 20 participants and then selected two patterns of development with clean individual differences. Figure 7.1: Data for two children on the four-image task over the three years of the study. The participant on the left showed substantial gains in processing ability from year to year, whereas the child on the right showed approximately the same level of performance each year, with some small gains in peak accuracy and rate of change. The child with substantial changes in word recognition also showed gains in expressive vocabulary with improved standard scores on the EVT-2: 83 at Year 1, 95 at Year 2, and 103 at Year 3. In contrast, the child with consistent processing had an above-average standard expressive vocabulary score during each year of the study: 133 at Year 1, 138 at Year 2, and 130 at Year 3. These two children give a glimpse of the many possible developmental trajectories for this task and how they relate to expressive vocabulary. The main work of this project will be describing and characterizing the trajectories of many children on different aspects of lexical processing. References "],
["analysis.html", "Chapter 8 Analysis 8.1 Growth Curve Analysis 8.2 Aim 1 (Familiar Word Recognition and Lexical Competition) 8.3 Aim 2 (Referent Selection and Mispronunciations) 8.4 Aim 3 (Computational Modeling)", " Chapter 8 Analysis 8.1 Growth Curve Analysis Eyetracking growth curves will be analyzed using Bayesian mixed effects logistic regression. I will use logistic regression because the outcome measurement is a probability (the log-odds of looking to the target image versus a distractor). I will use mixed-effects models because I want to estimate a separate growth curve for each child (to measure individual differences in word recognition) but also treat each child’s individual growth curve as a draw from a distribution of related curves. I plan to use Bayesian techniques to study a generative model of the data. Instead of reporting and describing a single, best-fitting model of some data, Bayesian techniques consider an entire distribution of plausible models that are consistent with the data and any prior information we have about the models. By using this approach, I can explicitly quantify uncertainty about statistical effects and draw inferences using estimates of uncertainty (instead of using statistical significance—which is not a straightforward matter for mixed-effects models).2 The eyetracking growth curves will be fit using an orthogonal cubic polynomial function of time (a now-conventional approach; see Mirman, 2014). Put differently, I will model the probability of looking to the target during an eyetracking task as: \\[ \\text{log-odds}(\\mathit{looking}) = \\beta_0 + \\beta_1 * \\textit{Time}^1 + \\beta_2 * \\textit{Time}^2 + \\beta_3 * \\textit{Time}^3 \\] That the time terms are orthogonal means that \\(\\textit{Time}^1\\), \\(\\textit{Time}^2\\) and \\(\\textit{Time}^3\\) are transformed so that they are uncorrelated. Under this formulation, the parameters \\(\\beta_0\\) and \\(\\beta_1\\) have a clear interpretation in terms of lexical processing performance. The intercept, \\(\\beta_0\\), measures the area under the growth curve—or the probability of fixating on the target word averaged over the whole window. We can think of \\(\\beta_0\\) as a measure of average accuracy or of word recognition reliability. The linear time parameter, \\(\\beta_1\\), estimates the steepness of the growth curve—or how the probability of fixating changes from frame to frame. We can think of \\(\\beta_1\\) as a measure of processing efficiency, because growth curves with stronger linear features exhibit steeper frame-by-frame increases in looking probability.3 For each experimental task, I will study how word recognition changes over time by modeling how growth curves change over developmental time. This amounts to study how the growth curve parameters changes year over year in the study. I can model the data for an eyetracking task by including dummy-coded indicators for Year 1, Year 2, and Year 3 and having these indicators interact with the growth curve parameters. In such a model, Year 2 would be the reference year, so the Year 1 parameters would estimate how the word-recognition-curves change from Year 2 to Year 1, and Year 3 parameters would be interpreted similarly. \\[ \\begin{align*} \\text{Year 2 Growth Curve:}\\\\ \\text{log-odds}(\\mathit{looking}) &amp;= \\beta_0 + \\beta_1 * \\textit{Time}^1 + \\beta_2 * \\textit{Time}^2 + \\beta_3 * \\textit{Time}^3 \\\\ \\text{Adjustments to Year 2:} \\\\ \\beta_i &amp;= \\gamma_{i:2} + \\gamma_{i:1} * \\text{Year1} + \\gamma_{i:3} * \\text{Year3} \\\\ \\end{align*} \\] Thus, the interaction effects for the intercept term (\\(\\gamma_{0:1}\\), \\(\\gamma_{0:3}\\)) describe how overall accuracy changed between years, and interaction effects for the linear-time terms (\\(\\gamma_{1:1}\\), \\(\\gamma_{1:3}\\)) describe changes in overall processing efficiency between years.4 Lastly, a brief comment about priors. Bayesian models require prior information (“priors”). For these models, I will use weakly to moderately informative priors. For example, suppose x and y are scaled to mean 0 and standard deviation 1. A weakly informative prior for the effect of x on y might be Normal(0, 5)—a normal distribution with mean 0 and standard deviation 5. If we fit a regression model and observed an effect size of 12 SD units, our first assumption would be that something went wrong with our software. The weakly informative prior captures this level of prior information. A moderately informative prior would be Normal(0, 1). This prior information captures our disciplinary experience that effect sizes greater than ±1 relatively uncommon in child language research. A strongly informative prior for this effect might be something like Normal(.4, .1) which says that our model should be very skeptical of negative effects and of effects larger than .8. For this project, I will default to the first two levels of prior information. 8.2 Aim 1 (Familiar Word Recognition and Lexical Competition) In the four-image task, I will model the development of familiar word recognition by studying how looks to the target image change year over year, as described in Growth Curve Analysis. I predict that children will be more sensitive to the phonological foil and semantic foils in this task as they age and learn more words. This hypothesis is based on the idea that children discover similarities among words as they learn word and integrate them into their lexicon. To test this hypothesis, I will study how the probability of fixating on the foils changes over trial-time and how these growth curves change from year to year. In the conventional model of eyetracking data, the outcome is a binomial choice—Target versus Distractor—and we can estimate the log-odds of fixating on the target image relative to the distractors. To study the specific effect of the Phonological foil in this task, Law et al. (2016) treated the Unrelated foil as a reference distractor and compared two separate binomial growth curves (see Figure 8.1): Target versus Phonological, and Target versus Unrelated. The same technique was used on the Semantic foil as well. With this approach, shown below, we observed an early negative effect of the phonological foil and a late negative effect of the semantic foil. Figure 8.1: Reprint of Figure 4 from Law et al. (2016) to illustrate the strategy of examining lexical competition effects where the semantic foil and phonological foil are compared to the unrelated image. I plan to employ this technique for studying lexical competition effects and their development from Year 1 to Year 2 to Year 3.5 Increased lexical completion would be reflected in greater interference from the foils compared to the unrelated image. A more comprehensive statistical model for this experiment would capture the fact that the data are multinomial: Target versus Phonological versus Semantic versus Unrelated. However, multinomial mixed effects growth curves have not been used for eyetracking data. They are not estimable with the standard classical modeling software (lme4). I plan to examine whether such a model is feasible with Bayesian techniques, but it may prove to be too unstable. In that case, I will fall back to the above described strategy. I will also examine whether individual differences in lexical processing are stable over time. For an initial analytic step, I will identify how frequently children change quartiles or deciles. The idea here is that stable individual differences in processing should preserve some relative rankings—fast-processing children should remain relatively fast compared to their peers. If children make large swings in their rankings, e.g., changing from the bottom 55th to the 30th percentile, then we have evidence that these rankings are unstable. I am also interested in how the magnitude of individual differences change over time. The differences among children could diminish over time so that the rankings are unstable and reflect small variations among children. Another question concerns the relationship between the development of lexical processing and the development of vocabulary size. Specifically, I will ask how age-adjusted lexical processing measures (accuracy and efficiency) correlate with age-adjusted vocabulary size each year. Moreover, I will also examine whether children who make large gains in vocabulary size also show large concurrent changes in their lexical processing measures. 8.3 Aim 2 (Referent Selection and Mispronunciations) For this task, I will model how the looks to the familiar image differ in each condition (real words, mispronunciations, nonwords) and how the growth curves for each condition change year over year. This model will use growth curve model described in Growth Curve Analysis but augmented with Condition effects. I will examine whether and when any dissociation is observed for word recognition in the real word and nonword conditions. McMurray et al. (2012) argue that familiar word recognition and fast association for novel words reflect the same cognitive process: referent selection. Data from this task would support with this hypothesis when the growth curves for looks to the familiar image are symmetrical for the real word and nonword conditions. Figure 8.2, showing data from Law and Edwards (2015, n = 34 children, 30-46 months old), shows some symmetry for the real word and nonword conditions. Figure 8.2: Condition averages for data described by Law and Edwards (2015). Compare to Figure 2 in the original manuscript. I will test whether the two measures ever dissociate by computing the posterior predicted difference between the growth curves. This approach is similar to the bootstrap-based divergence analyses used in some word recognition experiments (Dink &amp; Ferguson, 2016; e.g., Oleson, Cavanaugh, McMurray, &amp; Brown, 2015). The essential question is when—at which specific time points—do two growth curves differ significantly from one another. The bootstrap approach uses resampling to get an estimate, whereas I will use posterior predicted samples to estimate these differences. (I have not seen my approach used yet in the literature, so it is a small innovation.) Specifically, I will compute the posterior-predicted looks to the familiar object in the real word condition, P(Familiar | Real Word, Time t, Child i) and the analogous looks to the unfamiliar object in the nonword condition, P(Unfamiliar | Nonword, Time t, Child i). The difference between these two probabilities estimates how the time course of word recognition differs between these two conditions, and I can use 50% and 90% uncertainty intervals to determine during which time points the curves credibly differ from each other. Figure 8.3 shows this calculation performed on data from Law and Edwards (2015). If feasible, I will also examine whether these measures dissociate within children and examine which child-level factors are associated with these kinds of listeners. Figure 8.3: Demonstration of posterior difference technique on data from Law and Edwards (2015). Even though performance on the real word and nonword conditions might be highly correlated, one might intuitively hypothesize that that performance on the nonword condition to be a better predictor of concurrent or future vocabulary size. The rationale would be that referent selection for novel words is a more transparent test of the word learner’s basic task of associating new labels with objects. Therefore, I will examine how each of these measures relates to vocabulary growth. I will describe how looking behavior in the mispronunciation condition changes over time and changes for specific mispronunciation patterns. Overall, I predict that children will be more tolerant of mispronunciations as they age, because older children know more words and have more implicit knowledge about the similarities among words. As for specific mispronunciation items, let us (safely) suppose that speech perception improves with age, especially for later mastered sounds. Then we should expect that looking patterns for the rice-wice trials change significantly between Year 2 and Year 3, at least compared to looking patterns on trials with mispronunciations of earlier acquired sounds (e.g., girl-dirl or duck-guck). Therefore, I will examine individual mispronunciation effects and how they are associated with child-level measures, including speech perception. 8.4 Aim 3 (Computational Modeling) 8.4.1 TRACE Model Architecture TRACE (McClelland &amp; Elman, 1986) is an interactive activation model, and it interprets an input pattern by spreading energy (activation) through a network of processing units. The pattern of activation over the network is its interpretation of the input signal, so that more active units represent more likely interpretations. Over many processing cycles, the network propagates energy among its connections until it settles into a stable pattern of activation. The input for TRACE is a mock-speech signal that activates perceptual feature-detectors. These units respond to phonetic features like voicing or vocalic resonance. The perceptual units activate phoneme units, and the phoneme units activate lexical word units, as shown in Figure 8.4. Figure 8.4: TRACE model architecture. Thick arrows indicate excitatory connections between layers, including a top-down connections from words onto phoneme units. Lines with points at the end reflect inhibitory connections among competing units within a layer. This image is a lightly modified public-domain version of Figure 1 in Strauss et al. (2007): https://en.wikipedia.org/wiki/File:TRACE_architecture.jpg Units within a level (phonetic, phonemic, lexical) compete through lateral inhibition, so that more active units can suppress less active units. This inhibition allows the network to rule out possible units and narrow its interpretations over time. There are also top-down connections so that word units in the lexical layer can reinforce the sound units that make up those words. One consequence of this feature is that the network can resolve ambiguous phonemes, as in Xift where X is a sound between /k/ and /g/ and top-down influence supports gift rather than kift. Lastly, activation in units gradually decays over time, and the network will eventually “forget” it input pattern to return to a resting state. The model parameters that govern how activation propagates through the network map onto psychological processing constructs. For instance, the phoneme inhibition strength parameter controls how decisively (or categorically) the network interprets speech sounds. Phoneme-to-word activation strength reflects how quickly speech sounds begin to activate words. Decay parameters control the model’s temporary memory for different kinds of representations. 8.4.2 Modeling Looking Data To simulate behavioral data, we need a linking hypothesis for translating between human behavior and network behavior (Magnuson, Mirman, &amp; Harris, 2012). During each network cycle, some words are activated, some more activated than others, so that the unit with the highest activation is the preferred interpretation of the input. We can convert these activations into probabilities using the softmax function: \\[P(\\textit{word}_w) = \\mathrm{softmax}(\\textit{activation}_w) = \\frac{\\exp(k * \\textit{activation}_w)} {\\sum{\\exp(k * \\textit{activation}_{[\\text{words to choose from}]})}}\\] In other words, scale the activation using some parameter k, exponentiate the scaled activation values of all relevant choices, and the proportion of the total exponentiated activation belonging to word w is the probability of fixating on word w.6 The scaling value k is manipulated to help the model-simulated probabilities match the observed looking probabilities. The next step, and this process will be rather open-ended and iterative, is to simulate the behavioral data with the model. As a first step, I will need to create a developmentally appropriate lexicon. I will use developmental norms and databases to determine an appropriate set of items. I will first try to simulate the mispronunciation experiment. Mayor and Plunkett (2014) successfully simulated some mispronunciation data from experiments with data, so I will borrow some of their strategies for modeling these kinds of experiments. After I get the model to simulate Year 1 data, I will explore which parameters need to change to account for Year 2 and then Year 3 data. There are many degrees of freedom (model parameters) for how to replicate these development changes, so I will plan to systematically report the model fitting and the development consequences of the model fit. The four-image experiment is slightly trickier, because it incorporates semantically related information, and TRACE has no built-in semantic representations. Two options for modeling the experiment are 1) to ignore looks to the semantic foil, following the Luce choice rule (1959, 2008) and 2) to try to incorporate semantic information into the model by modifying connections between semantically related words—although it is unclear whether either is tractable. At any rate, part of this project will be to explore how to reconcile these data with this apparent limitation with the TRACE model. Finally, I would like to account for individual differences among children and incorporate child-level information into the simulations. Specifically, I would like to test how vocabulary differences are associated with large lexicons for model simulations. Moreover, I would like to whether children’s speech perception abilities systematically relate to the model’s phonological activation parameters. Such a correspondence would further validate the models. These simulations would further validate these child-level measures by describing how they specifically affect word recognition. For these research questions, I will following the modeling heuristics described by Magnuson et al. (2012). For example, for any modeling failures, I will assess the failure in terms of the computational consequences: Is the failure a problem with the theory, implementation, parameters or link between human data and model activity? Similarly, can the model fit specific item effects or just condition effects? The authors provide other heuristics, and these guidelines heuristics will help formalize the assessment and comparison of computational models. References "],
["prepare-and-explore-the-data.html", "Chapter 9 Prepare and explore the data 9.1 Data cleaning", " Chapter 9 Prepare and explore the data First, let’s load in all the data and plot the data from each year. library(dplyr) #&gt; Warning: package &#39;dplyr&#39; was built under R version 3.4.2 library(rlang) library(littlelisteners) library(ggplot2) #&#39; group `data` by some grouping variables (`...`), #&#39; randomly select `size` of the groups #&#39; keep just the data from those sampled groups sample_n_of &lt;- function(data, size, ...) { dots &lt;- quos(...) rows &lt;- data_frame(row = seq_len(nrow(data))) rows[, &quot;group&quot;] &lt;- data %&gt;% group_by(!!! dots) %&gt;% group_indices() subset &lt;- rows %&gt;% filter(.data$group %in% sample(unique(.data$group), size)) %&gt;% pull(.data$row) data[subset, ] } create_pairs &lt;- function(xs) { if (!is.factor(xs)) xs &lt;- ordered(xs) xs %&gt;% levels() %&gt;% rev() %&gt;% combn(2) %&gt;% t() %&gt;% as.data.frame() %&gt;% set_names(&quot;x1&quot;, &quot;x2&quot;) %&gt;% mutate(name = paste0(.data$x1, &quot;-&quot;, .data$x2)) %&gt;% mutate_all(as.character) %&gt;% arrange(x1, desc(x2)) } compare_pairs &lt;- function(data, levels, values, f = `-`) { levels &lt;- enquo(levels) values &lt;- enquo(values) pairs &lt;- data %&gt;% pull(!! levels) %&gt;% create_pairs() wide &lt;- tidyr::spread(data, !! levels, !! values) for (row_i in seq_len(nrow(pairs))) { pair_i &lt;- pairs[row_i, ] wide[, pair_i$name] &lt;- f(wide[[pair_i$x1]], wide[[pair_i$x2]]) } wide %&gt;% select(-one_of(c(pairs$x1), c(pairs$x2))) %&gt;% tidyr::gather(&quot;pair&quot;, &quot;value&quot;, one_of(c(pairs$name))) %&gt;% mutate(pair = factor(.data$pair, levels = pairs$name)) } tidy_corr &lt;- function(df, ..., .type = c(&quot;pearson&quot;, &quot;spearman&quot;)) { vars &lt;- quos(...) select(df, !!! vars) %&gt;% as.matrix() %&gt;% Hmisc::rcorr(type = .type) %&gt;% broom::tidy() %&gt;% tibble::remove_rownames() %&gt;% arrange(.data$column1, .data$column2) } looks1 &lt;- readr::read_csv(&quot;./data-raw/rwl_timepoint1_looks.csv.gz&quot;) looks2 &lt;- readr::read_csv(&quot;./data-raw/rwl_timepoint2_looks.csv.gz&quot;) looks3 &lt;- readr::read_csv(&quot;./data-raw/rwl_timepoint3_looks.csv.gz&quot;) looks &lt;- bind_rows(looks1, looks2, looks3) %&gt;% filter(Version == &quot;Standard&quot;) resp_def &lt;- create_response_def( primary = &quot;Target&quot;, others = c(&quot;PhonologicalFoil&quot;, &quot;SemanticFoil&quot;, &quot;Unrelated&quot;), elsewhere = &quot;tracked&quot;, missing = NA ) # Keep only frames from -500 to 2000 plus or minus any frames to make the # number of frames divisible by 3 (for binning) times_to_keep &lt;- looks %&gt;% distinct(Time) %&gt;% trim_to_bin_width(3, time_var = Time, key_time = 0, key_position = 2, min_time = -500, max_time = 2000) %&gt;% pull(Time) %&gt;% range() raw_data &lt;- looks %&gt;% filter(between(Time, times_to_keep[1], times_to_keep[2])) %&gt;% aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI) Make some plots of overall averages. ggplot(raw_data) + aes(x = Time, y = Prop, color = Study) + geom_hline(size = 2, color = &quot;white&quot;, yintercept = .25) + stat_summary() + labs(x = &quot;Time after target onset [ms]&quot;, y = &quot;Proportion looks to target&quot;, caption = &quot;Mean +/- SE&quot;) + theme_grey(base_size = 9) + theme(legend.position = c(0.05, 0.95), legend.justification = c(0, 1)) #&gt; Warning: Removed 22 rows containing non-finite values (stat_summary). #&gt; No summary function supplied, defaulting to `mean_se() ggplot(raw_data) + aes(x = Time, y = Prop, color = Study) + geom_hline(size = 2, color = &quot;white&quot;, yintercept = .25) + stat_smooth() + labs(x = &quot;Time after target onset [ms]&quot;, y = &quot;Proportion looks to target&quot;, caption = &quot;GAM Smooth&quot;) + theme_grey(base_size = 9) + theme(legend.position = c(0.05, 0.95), legend.justification = c(0, 1)) #&gt; `geom_smooth()` using method = &#39;gam&#39; #&gt; Warning: Removed 22 rows containing non-finite values (stat_smooth). The raw data plainly confirm hypothesis 1: Children’s accuracy and efficiency of recognizing words will improve each year. Look at a spaghetti plot… ggplot(raw_data) + aes(x = Time, y = Prop, group = ResearchID) + geom_hline(size = 2, color = &quot;white&quot;, yintercept = .25) + geom_line(alpha = .15) + facet_grid(~ Study) + theme_grey(base_size = 9) + labs(x = &quot;Time after target onset [ms]&quot;, y = &quot;Proportion looks to target&quot;, caption = &quot;Lines: Individual participants&quot;) + theme(legend.position = c(0.05, 0.95), legend.justification = c(0, 1)) 9.1 Data cleaning We use the following options for data screening. rules &lt;- list( screening_window = c(0, 2020), missing_data_limit = .5, min_trials = 12 ) That is: Filter out bad trials. These have at least 50% missing data between 0 to 2020ms. Filter out bad blocks. These have fewer than 12 trials. These are the default conventions four lab on these eyetracking experiments. # offset to catch frame before and after window # (to reflect binning boundaries) screening_times &lt;- looks %&gt;% distinct(Time) %&gt;% trim_to_bin_width(3, 0, 2, Time, min_time = rules$screening_window[1], max_time = rules$screening_window[2]) %&gt;% pull(Time) %&gt;% range() screening_times #&gt; [1] -16.6546 2015.2100 missing_data_by_trial &lt;- looks %&gt;% filter(screening_times[1] &lt;= Time, Time &lt;= screening_times[2]) %&gt;% aggregate_looks( resp_def, Study + Version + ResearchID + Basename + TrialNo ~ GazeByImageAOI) %&gt;% mutate(BadTrial = rules$missing_data_limit &lt;= PropNA) bad_trial_counts &lt;- missing_data_by_trial %&gt;% count(Study, ResearchID, Basename, BadTrial) %&gt;% tidyr::spread(BadTrial, n) %&gt;% rename(n_bad = `TRUE`, n_good = `FALSE`) %&gt;% # Replace NAs with 0, in case there were 0 good trials in a block or # 0 bad trials in a block mutate(n_bad = coalesce(n_bad, 0L), n_good = coalesce(n_good, 0L), trials = n_good + n_bad, prop_bad = round(n_bad / trials, 2)) blocks_to_drop &lt;- bad_trial_counts %&gt;% filter(.5 &lt;= prop_bad) blocks_to_drop #&gt; # A tibble: 110 x 7 #&gt; Study ResearchID Basename n_good n_bad trials prop_bad #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 TimePoint1 013L RWL_Block2_013L32MA1 11 13 24 0.54 #&gt; 2 TimePoint1 023L RWL_Block1_023L29MA1 10 14 24 0.58 #&gt; 3 TimePoint1 027L RWL_Block2_027L38MS2 6 18 24 0.75 #&gt; 4 TimePoint1 033L RWL_Block1_033L35FS2 2 22 24 0.92 #&gt; 5 TimePoint1 033L RWL_Block2_033L35FS2 2 22 24 0.92 #&gt; 6 TimePoint1 035L RWL_Block1_035L32FA1 8 16 24 0.67 #&gt; 7 TimePoint1 079L RWL_Block1_079L37FS2 10 14 24 0.58 #&gt; 8 TimePoint1 109L RWL_Block1_109L34MS2 8 16 24 0.67 #&gt; 9 TimePoint1 114L RWL_Block1_114L30MS1 7 17 24 0.71 #&gt; 10 TimePoint1 114L RWL_Block2_114L30MS1 9 15 24 0.62 #&gt; # ... with 100 more rows leftover_bad_trials &lt;- missing_data_by_trial %&gt;% anti_join(blocks_to_drop, by = c(&quot;Study&quot;, &quot;ResearchID&quot;, &quot;Basename&quot;)) %&gt;% filter(.5 &lt;= PropNA) clean_looks &lt;- looks %&gt;% anti_join(blocks_to_drop, by = c(&quot;Study&quot;, &quot;ResearchID&quot;, &quot;Basename&quot;)) %&gt;% anti_join(leftover_bad_trials) #&gt; Joining, by = c(&quot;Study&quot;, &quot;ResearchID&quot;, &quot;Version&quot;, &quot;Basename&quot;, &quot;TrialNo&quot;) Do some head counts. screening_results &lt;- list(Screened = clean_looks, Raw = looks) %&gt;% bind_rows(.id = &quot;Dataset&quot;) %&gt;% distinct(Dataset, Study, ResearchID, TrialID) %&gt;% group_by(Dataset, Study) %&gt;% summarise( `Num Children` = n_distinct(ResearchID), `Num Trials` = n_distinct(TrialID)) screening_results %&gt;% knitr::kable(caption = &quot;Eyetracking data before and after data screening&quot;) Table 9.1: Eyetracking data before and after data screening Dataset Study Num Children Num Trials Raw TimePoint1 178 7967 Raw TimePoint2 180 8327 Raw TimePoint3 163 7724 Screened TimePoint1 163 5951 Screened TimePoint2 165 6421 Screened TimePoint3 156 6483 Plot the data after partial data screening. data &lt;- clean_looks %&gt;% filter(between(Time, times_to_keep[1], times_to_keep[2])) %&gt;% readr::write_csv(&quot;./data/aim1-screened.csv.gz&quot;) agg_data &lt;- data %&gt;% aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI) We include the curves from the earlier plots in gray. The data-cleaning process slightly increases the average accuracy during the plateau-ed portion of the growth curve. ggplot(agg_data) + aes(x = Time, y = Prop, color = Study) + geom_hline(size = 2, color = &quot;white&quot;, yintercept = .25) + stat_summary(aes(group = Study), data = raw_data, color = &quot;gray70&quot;) + stat_summary() + labs(x = &quot;Time after target onset [ms]&quot;, y = &quot;Proportion looks to target&quot;, caption = &quot;Mean +/- SE&quot;) + theme_grey(base_size = 9) + theme(legend.position = c(0.05, 0.95), legend.justification = c(0, 1)) #&gt; Warning: Removed 22 rows containing non-finite values (stat_summary). #&gt; No summary function supplied, defaulting to `mean_se() #&gt; No summary function supplied, defaulting to `mean_se() ggplot(agg_data) + aes(x = Time, y = Prop, color = Study) + geom_hline(size = 2, color = &quot;white&quot;, yintercept = .25) + stat_smooth(aes(group = Study), data = raw_data, color = &quot;gray70&quot;) + stat_smooth() + labs(x = &quot;Time after target onset [ms]&quot;, y = &quot;Proportion looks to target&quot;, caption = &quot;GAM smooth on partially screened data&quot;) + theme_grey(base_size = 9) + theme(legend.position = c(0.05, 0.95), legend.justification = c(0, 1)) #&gt; `geom_smooth()` using method = &#39;gam&#39; #&gt; Warning: Removed 22 rows containing non-finite values (stat_smooth). #&gt; `geom_smooth()` using method = &#39;gam&#39; 9.1.1 Add a note about the bad version of the experiment (Skip for now.) 9.1.2 Special case data screening (Skip for now. This is where I review the participant notes and will remove children who have to be excluded for other reasons, like being diagnosed with a language disorder at TimePoint 3.) # stub for saving the final-final data 9.1.3 Interim summary Visual evidence that group averages get faster and more reliable at looking to target each year. "],
["analyze-familiar-word-recognition.html", "Chapter 10 Analyze familiar word recognition 10.1 Data preparation 10.2 Maximum likelihood results 10.3 Bayesian model results", " Chapter 10 Analyze familiar word recognition Next steps: Model year over year changes. Download test scores and individual differences. Analyze individual differences 10.1 Data preparation Earlier we cleaned the data to remove trials with excessive missing data and blocks of trials with too few trials. Read in that data. data &lt;- readr::read_csv(&quot;./data/aim1-screened.csv.gz&quot;) Downsample into 50 ms bins. data &lt;- data %&gt;% select(Study, ResearchID, TrialID:GazeByImageAOI) %&gt;% assign_bins(bin_width = 3, Time, TrialID) # Compute time at center of each bin bin_times &lt;- data %&gt;% distinct(Time, .bin) %&gt;% group_by(.bin) %&gt;% mutate(BinTime = round(median(Time), -1)) %&gt;% ungroup() # Attach bin times binned &lt;- data %&gt;% left_join(bin_times, by = c(&quot;Time&quot;, &quot;.bin&quot;)) %&gt;% ungroup() %&gt;% select(-Time) %&gt;% rename(Time = BinTime) resp_def &lt;- create_response_def( primary = &quot;Target&quot;, others = c(&quot;PhonologicalFoil&quot;, &quot;SemanticFoil&quot;, &quot;Unrelated&quot;), elsewhere = &quot;tracked&quot;, missing = NA ) d &lt;- binned %&gt;% aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI) d_m &lt;- d %&gt;% filter(250 &lt;= Time, Time &lt;= 1500) %&gt;% polypoly::poly_add_columns(Time, degree = 3, scale_width = 1, prefix = &quot;ot&quot;) Plot the model-ready data. Use empirical logit because straight log-odds has too high of values for plotting. ggplot(d_m) + aes(x = Time, y = empirical_logit(Primary, Others)) + geom_line(aes(group = ResearchID), alpha = .2) + stat_smooth() + theme_grey(base_size = 9) + facet_grid(. ~ Study) + labs(x = &quot;Time after target onset (smoothed to 50 ms bins)&quot;, y = &quot;Emp. logit looking to target&quot;) #&gt; `geom_smooth()` using method = &#39;gam&#39; 10.2 Maximum likelihood results Fit a maximum likelihood model as a first pass for the analysis. We won’t fit the model automatically (whenever this page is updated). It’s too time consuming. Instead, we do it manually here, and save the results. library(lme4) m &lt;- glmer( cbind(Primary, Others) ~ (ot1 + ot2 + ot3) * Study + (ot1 + ot2 + ot3 | ResearchID/Study), family = binomial, data = d_m) readr::write_rds(m, &quot;./data/aim1_cubic_model.rds.gz&quot;) And reload the saved model here. library(lme4) #&gt; Loading required package: Matrix #&gt; Loading required package: methods m &lt;- readr::read_rds(&quot;./data/aim1_cubic_model.rds.gz&quot;) arm::display(m) #&gt; glmer(formula = cbind(Primary, Others) ~ (ot1 + ot2 + ot3) * #&gt; Study + (ot1 + ot2 + ot3 | ResearchID/Study), data = d_m, #&gt; family = binomial) #&gt; coef.est coef.se #&gt; (Intercept) -0.47 0.03 #&gt; ot1 1.58 0.06 #&gt; ot2 0.05 0.04 #&gt; ot3 -0.17 0.03 #&gt; StudyTimePoint2 0.41 0.03 #&gt; StudyTimePoint3 0.70 0.04 #&gt; ot1:StudyTimePoint2 0.56 0.08 #&gt; ot1:StudyTimePoint3 1.10 0.08 #&gt; ot2:StudyTimePoint2 -0.16 0.05 #&gt; ot2:StudyTimePoint3 -0.35 0.05 #&gt; ot3:StudyTimePoint2 -0.12 0.04 #&gt; ot3:StudyTimePoint3 -0.21 0.04 #&gt; #&gt; Error terms: #&gt; Groups Name Std.Dev. Corr #&gt; Study:ResearchID (Intercept) 0.30 #&gt; ot1 0.68 0.18 #&gt; ot2 0.44 -0.12 0.03 #&gt; ot3 0.29 -0.09 -0.44 -0.05 #&gt; ResearchID (Intercept) 0.27 #&gt; ot1 0.46 0.86 #&gt; ot2 0.09 -0.99 -0.85 #&gt; ot3 0.03 -0.92 -0.98 0.92 #&gt; Residual 1.00 #&gt; --- #&gt; number of obs: 12584, groups: Study:ResearchID, 484; ResearchID, 195 #&gt; AIC = 74467.3, DIC = -61745.9 #&gt; deviance = 6328.7 d_m$cubic_fit &lt;- fitted(m) ggplot(d_m) + aes(x = Time, y = cubic_fit) + geom_line(aes(group = ResearchID), alpha = .2) + facet_grid(. ~ Study) + labs( x = &quot;Time after target onset (smoothed to 50 ms bins)&quot;, y = &quot;Proportion looks to target (fitted)&quot;) + theme_grey(base_size = 9) 10.2.1 What’s being captured by the random effects? First, let’s plot just the fixed effect predictions. predict_y &lt;- function(...) predict(..., type = &quot;response&quot;) d_m$fixef_fit &lt;- predict_y(m, re.form = ~ 0) d_m$subj_fit &lt;- predict_y(m, re.form = ~ (ot1 + ot2 + ot3 | ResearchID)) d_m$study_fit &lt;- predict_y(m, re.form = ~ (ot1 + ot2 + ot3 | Study:ResearchID)) d_m$full_fit &lt;- predict_y(m, re.form = ~ (ot1 + ot2 + ot3 | Study:ResearchID) + (ot1 + ot2 + ot3 | ResearchID)) ggplot(d_m) + aes(x = Time, y = fixef_fit) + geom_line(aes(group = ResearchID), alpha = .2) + facet_grid(. ~ Study) + theme_grey(base_size = 9) + labs( x = &quot;Time after target onset (smoothed to 50 ms bins)&quot;, y = &quot;Proportion looks to target (fitted)&quot;, caption = &quot;Conditioned on no random effects&quot;) Now, we condition on child level effects. ggplot(d_m) + aes(x = Time, y = subj_fit) + geom_line(aes(group = ResearchID), alpha = .2) + facet_grid(. ~ Study) + theme_grey(base_size = 9) + labs( x = &quot;Time after target onset (smoothed to 50 ms bins)&quot;, y = &quot;Proportion looks to target (fitted)&quot;, caption = &quot;Conditioned on Child effects&quot;) ggplot(d_m) + aes(x = Time, y = subj_fit - fixef_fit) + geom_line(aes(group = ResearchID), alpha = .2) + facet_grid(. ~ Study) + theme_grey(base_size = 9) + labs( x = &quot;Time after target onset (smoothed to 50 ms bins)&quot;, y = &quot;Child-conditioned minus study means&quot;) It looks like the range of y values is smaller in TimePoint2 and TimePoint3, but could that just be the different numbers of participants who contribute to each study? d_m %&gt;% distinct(ResearchID, Study) %&gt;% count(Study) %&gt;% rename(`Num children in model` = n) %&gt;% knitr::kable() Study Num children in model TimePoint1 163 TimePoint2 165 TimePoint3 156 Now we condition on Study x Child effects. These would be capturing the subject-x-study variability. ggplot(d_m) + aes(x = Time, y = study_fit) + geom_line(aes(group = ResearchID), alpha = .2) + facet_grid(. ~ Study) + theme_grey(base_size = 9) + labs( x = &quot;Time after target onset (smoothed to 50 ms bins)&quot;, y = &quot;Proportion looks to target (fitted)&quot;, caption = &quot;Conditioned on Study x Child effects&quot;) ggplot(d_m) + aes(x = Time, y = study_fit - fixef_fit) + geom_line(aes(group = ResearchID), alpha = .2) + facet_grid(. ~ Study) + theme_grey(base_size = 9) + labs( x = &quot;Time after target onset (smoothed to 50 ms bins)&quot;, y = &quot;Child-x-Study-conditioned minus study means&quot;) Look for weak spots in the time series. d_corr &lt;- d_m %&gt;% group_by(Time, Study) %&gt;% summarise(r = cor(Prop, cubic_fit)) ggplot(d_corr) + aes(x = Time, y = r, color = Study) + geom_point(shape = 1, size = 3) + ylim(c(.8, 1)) + labs( x = &quot;Time after target onset (smoothed to 50 ms bins)&quot;, y = &quot;Correlation of fitted and observed&quot;) + theme_grey(base_size = 9) + theme( legend.position = c(0.025, 0.05), legend.justification = c(0, 0)) Rank the participants by their growth curve parameters—that is, the growth curve features when condition on child ID. xstudy_effects &lt;- m %&gt;% ranef() %&gt;% getElement(&quot;ResearchID&quot;) %&gt;% tibble::rownames_to_column(&quot;ResearchID&quot;) %&gt;% as_tibble() %&gt;% select(ResearchID, intercept = `(Intercept)`, slope = ot1) top_20 &lt;- top_n(xstudy_effects, 20, slope) bot_20 &lt;- top_n(xstudy_effects, 20, -slope) ggplot(d_m %&gt;% filter(Study == &quot;TimePoint2&quot;)) + aes(x = Time, y = subj_fit) + geom_line(aes(group = ResearchID), alpha = .2) + geom_line(aes(group = ResearchID), data = semi_join(d_m, top_20) %&gt;% filter(Study == &quot;TimePoint2&quot;), size = .7, color = &quot;#0074D9&quot;) + geom_line(aes(group = ResearchID), data = semi_join(d_m, bot_20) %&gt;% filter(Study == &quot;TimePoint2&quot;), size = .7, color = &quot;#FF4136&quot;) + theme_grey(base_size = 9) + labs(y = &quot;TP2 fits conditioned on Child effects&quot;, x = &quot;Time after target onset (smoothed to 50 ms bins)&quot;, caption = &quot;Colors: Top 20 and bottom 20 children by linear time effect&quot;) #&gt; Joining, by = &quot;ResearchID&quot; #&gt; Joining, by = &quot;ResearchID&quot; Visualize the model fits for the top and bottom 20 children. This plot illustrates that the children with strongest and weakest linear time components overall stay clustered away from each other when looking study level predictions. That is, the top 20 in general perform bunch together in all three studies. ggplot(d_m) + aes(x = Time, y = cubic_fit) + geom_line(aes(group = ResearchID), alpha = .2) + geom_line(aes(group = ResearchID), data = semi_join(d_m, top_20), size = .7, color = &quot;#0074D9&quot;) + geom_line(aes(group = ResearchID), data = semi_join(d_m, bot_20), size = .7, color = &quot;#FF4136&quot;) + facet_grid(. ~ Study) + theme_grey(base_size = 9) + labs(y = &quot;Proportion looks to target [model fits]&quot;, x = &quot;Time after target onset (smoothed to 50 ms bins)&quot;, caption = &quot;Colors: Top 20 and bottom 20 children by linear time effect&quot;) #&gt; Joining, by = &quot;ResearchID&quot; #&gt; Joining, by = &quot;ResearchID&quot; To confirm that this differences are not just an artifact of modeling, visualize the ranks on the observed data. ggplot(d_m) + aes(x = Time, y = Prop) + geom_line(aes(group = ResearchID), alpha = .2) + geom_line(aes(group = ResearchID), data = semi_join(d_m, top_20), size = .7, color = &quot;#0074D9&quot;) + geom_line(aes(group = ResearchID), data = semi_join(d_m, bot_20), size = .7, color = &quot;#FF4136&quot;) + facet_grid(. ~ Study) + labs(y = &quot;Proportion looks to target&quot;, x = &quot;Time after target onset (smoothed to 50 ms bins)&quot;, caption = &quot;Colors: Top 20 and bottom 20 children by linear time effect&quot;) #&gt; Joining, by = &quot;ResearchID&quot; #&gt; Joining, by = &quot;ResearchID&quot; Open questions: How to test for stability of individual differences over time? Intuitively, I would say that the differences are unstable if the red and blue lines got shuffled in each study. What stats formalize this intuition? 10.3 Bayesian model results Here is the code used to fit the model with Stan. It took about 24 hours to run the model. library(rstanarm) options(mc.cores = parallel::detectCores()) m &lt;- stan_glmer( cbind(Primary, Others) ~ (ot1 + ot2 + ot3) * Study + (ot1 + ot2 + ot3 | ResearchID/Study), family = binomial, prior = normal(0, 1), prior_intercept = normal(0, 5), prior_covariance = decov(2, 1, 1), data = d_m) readr::write_rds(m, &quot;./data/stan_aim1_cubic_model.rds.gz&quot;) Let’s try to understand our model by making some plots. 10.3.1 Fixed effects plots First, let’s prepare to plot the intervals for the fixed effects. library(rstanarm) #&gt; Loading required package: Rcpp #&gt; Warning: package &#39;Rcpp&#39; was built under R version 3.4.2 #&gt; rstanarm (Version 2.15.3, packaged: 2017-04-29 06:18:44 UTC) #&gt; - Do not expect the default priors to remain the same in future rstanarm versions. #&gt; Thus, R scripts should specify priors explicitly, even if they are just the defaults. #&gt; - For execution on a local, multicore CPU with excess RAM we recommend calling #&gt; options(mc.cores = parallel::detectCores()) library(bayesplot) #&gt; This is bayesplot version 1.4.0.9000 #&gt; - Plotting theme set to bayesplot::theme_default() #&gt; - Online documentation at mc-stan.org/bayesplot theme_set(theme_grey()) library(stringr) library(ggstance) #&gt; #&gt; Attaching package: &#39;ggstance&#39; #&gt; The following objects are masked from &#39;package:ggplot2&#39;: #&gt; #&gt; geom_errorbarh, GeomErrorbarh parse_text &lt;- function(x) parse(text = x) b &lt;- readr::read_rds(&quot;./data/stan_aim1_cubic_model.rds.gz&quot;) b #&gt; stan_glmer #&gt; family: binomial [logit] #&gt; formula: cbind(Primary, Others) ~ (ot1 + ot2 + ot3) * Study + (ot1 + ot2 + #&gt; ot3 | ResearchID/Study) #&gt; ------ #&gt; #&gt; Estimates: #&gt; Median MAD_SD #&gt; (Intercept) -0.5 0.0 #&gt; ot1 1.6 0.1 #&gt; ot2 0.0 0.0 #&gt; ot3 -0.2 0.0 #&gt; StudyTimePoint2 0.4 0.0 #&gt; StudyTimePoint3 0.7 0.0 #&gt; ot1:StudyTimePoint2 0.6 0.1 #&gt; ot1:StudyTimePoint3 1.1 0.1 #&gt; ot2:StudyTimePoint2 -0.2 0.0 #&gt; ot2:StudyTimePoint3 -0.4 0.1 #&gt; ot3:StudyTimePoint2 -0.1 0.0 #&gt; ot3:StudyTimePoint3 -0.2 0.0 #&gt; #&gt; Error terms: #&gt; Groups Name Std.Dev. Corr #&gt; Study:ResearchID (Intercept) 0.305 #&gt; ot1 0.691 0.20 #&gt; ot2 0.437 -0.11 0.02 #&gt; ot3 0.294 -0.11 -0.44 -0.06 #&gt; ResearchID (Intercept) 0.264 #&gt; ot1 0.423 0.78 #&gt; ot2 0.125 -0.75 -0.56 #&gt; ot3 0.058 -0.23 -0.31 0.19 #&gt; Num. levels: Study:ResearchID 484, ResearchID 195 #&gt; #&gt; Sample avg. posterior predictive #&gt; distribution of y (X = xbar): #&gt; Median MAD_SD #&gt; mean_PPD 49.9 0.1 #&gt; #&gt; ------ #&gt; For info on the priors used see help(&#39;prior_summary.stanreg&#39;). intervals &lt;- mcmc_intervals_data(as.data.frame(b), pars = names(fixef(b))) # Rename to use mathematical formatting intervals$pname &lt;- intervals$parameter %&gt;% str_replace(&quot;ot(2|3)&quot;, &quot;Time^\\\\1&quot;) %&gt;% str_replace(&quot;ot(1)&quot;, &quot;Time&quot;) %&gt;% str_replace(&quot;.Intercept.&quot;, &quot;Intercept&quot;) %&gt;% str_replace(&quot;Study&quot;, &quot;&quot;) %&gt;% str_replace(&quot;:&quot;, &quot; %*% &quot;) %&gt;% factor(., levels = rev(.)) Below the TimePoint2, TimePoint3, Time x TimePoint2, and Time x TimePoint3 effects confirm that children get more reliable and faster each year of the study. Only the Time2 effect is near 0, which does not matter. We mostly care about the intercept and time terms. ggplot(intervals) + aes(y = pname) + geom_vline(xintercept = 0, size = 2, color = &quot;white&quot;) + geom_linerangeh(aes(xmin = ll, xmax = hh)) + geom_linerangeh(aes(xmin = l, xmax = h), size = 2) + geom_point(aes(x = m), size = 3, shape = 3) + scale_y_discrete(labels = parse_text) + labs(x = NULL, y = NULL, caption = &quot;90% and 50% intervals&quot;) + ggtitle(&quot;Average effects&quot;) Now, let’s undo interactions by adding year 1 main effects to interaction effects and plot the effects for each year of the study. For each effect, there appears to be a linear trend in the change from TP1 to TP2 and from TP2 to TP3. # Column names will have mathematical formatting too draws &lt;- as.data.frame(b) %&gt;% as_tibble() %&gt;% transmute( `Intercept~~(TP1)` = `(Intercept)`, `Intercept~~(TP2)` = `(Intercept)` + StudyTimePoint2, `Intercept~~(TP3)` = `(Intercept)` + StudyTimePoint3, `Time~~(TP1)` = ot1, `Time~~(TP2)` = ot1 + `ot1:StudyTimePoint2`, `Time~~(TP3)` = ot1 + `ot1:StudyTimePoint3`, `Time^2~~(TP1)` = ot2, `Time^2~~(TP2)` = ot2 + `ot2:StudyTimePoint2`, `Time^2~~(TP3)` = ot2 + `ot2:StudyTimePoint3`, `Time^3~~(TP1)` = ot3, `Time^3~~(TP2)` = ot3 + `ot3:StudyTimePoint2`, `Time^3~~(TP3)` = ot3 + `ot3:StudyTimePoint3`) intervals2 &lt;- draws %&gt;% mcmc_intervals_data(prob = 0.5, prob_outer = 0.9) %&gt;% mutate(parameter = factor(parameter, levels = rev(parameter))) ggplot(intervals2) + aes(y = parameter) + geom_vline(xintercept = 0, size = 2, color = &quot;white&quot;) + ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) + geom_point(aes(x = m), size = 3, shape = 3) + scale_y_discrete(labels = parse_text) + labs(x = NULL, y = NULL, caption = &quot;90% and 50% intervals&quot;) + ggtitle(&quot;Average effects by study&quot;) Compute pairwise comparisons. clean_names &lt;- . %&gt;% stringr::str_replace_all(&quot;[()]&quot;, &quot;&quot;) %&gt;% stringr::str_replace(&quot;~~&quot;, &quot;_&quot;) pairwise &lt;- draws %&gt;% tibble::rowid_to_column(&quot;.draw&quot;) %&gt;% set_names(clean_names) %&gt;% tidyr::gather(parameter, value, -.draw) %&gt;% tidyr::separate(parameter, c(&quot;parameter&quot;, &quot;year&quot;), sep = &quot;_&quot;) %&gt;% compare_pairs(year, value) %&gt;% tidyr::spread(pair, value) %&gt;% split(.$parameter) %&gt;% lapply(. %&gt;% select(-.draw, -parameter) %&gt;% mcmc_intervals_data(prob = 0.5, prob_outer = 0.9) %&gt;% rename(pair = parameter)) %&gt;% bind_rows(.id = &quot;parameter&quot;) ggplot(pairwise) + aes(y = forcats::fct_rev(pair)) + geom_vline(xintercept = 0, size = 2, color = &quot;white&quot;) + ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) + geom_point(aes(x = m), size = 3, shape = 3) + labs(x = NULL, y = NULL, caption = &quot;90% and 50% intervals&quot;) + facet_wrap(&quot;parameter&quot;, ncol = 1, strip.position = &quot;left&quot;, labeller = label_parsed) + theme(strip.placement = &quot;outside&quot;, strip.background = element_rect(fill = NA), axis.text.y = element_text(size = rel(1.2))) + ggtitle(&quot;Differences in average effects&quot;) Bayesplot supports transformations so we could invert the log-odds measure to see the intercepts (area under curve/average accuracy) in proportion units. intervals3 &lt;- draws %&gt;% mcmc_intervals_data(regex_pars = &quot;Intercept&quot;, transformations = &quot;plogis&quot;, prob = 0.5, prob_outer = 0.9) %&gt;% mutate(parameter = factor(parameter, levels = rev(parameter))) intervals3 %&gt;% mutate_if(is.numeric, round, 3) %&gt;% select(-point_est) %&gt;% rename(outer = outer_width, inner = inner_width) %&gt;% knitr::kable() parameter outer inner ll l m h hh plogis(Intercept~~(TP1)) 0.9 0.5 0.372 0.380 0.385 0.390 0.397 plogis(Intercept~~(TP2)) 0.9 0.5 0.473 0.480 0.485 0.490 0.498 plogis(Intercept~~(TP3)) 0.9 0.5 0.544 0.551 0.557 0.562 0.569 ggplot(intervals3) + aes(y = parameter) + geom_vline(xintercept = .25, size = 2, color = &quot;white&quot;) + ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) + geom_point(aes(x = m), size = 3, shape = 3) + scale_y_discrete(labels = parse_text) + labs(x = NULL, y = NULL, caption = &quot;90% and 50% intervals&quot;) + ggtitle(&quot;Average accuracy by year&quot;) We can compute differences in average accuracy as well. proportions &lt;- b %&gt;% as.data.frame() %&gt;% transmute( `TP1` = `(Intercept)`, `TP2` = `(Intercept)` + StudyTimePoint2, `TP3` = `(Intercept)` + StudyTimePoint3) %&gt;% tibble::rowid_to_column(&quot;.draw&quot;) %&gt;% tidyr::gather(parameter, estimate, -.draw) %&gt;% mutate(estimate = plogis(estimate)) %&gt;% as_tibble() prop_diffs &lt;- proportions %&gt;% compare_pairs(parameter, estimate) %&gt;% tidyr::spread(pair, value) %&gt;% select(-.draw) %&gt;% mcmc_intervals_data(prob = 0.5, prob_outer = 0.9) ggplot(prop_diffs) + aes(y = forcats::fct_rev(parameter)) + geom_vline(xintercept = 0, size = 2, color = &quot;white&quot;) + ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) + geom_point(aes(x = m), size = 3, shape = 3) + scale_y_discrete(labels = parse_text) + labs(x = NULL, y = NULL, caption = &quot;90% and 50% intervals&quot;) + ggtitle(&quot;Differences in average accuracy&quot;) The average accuracy was 0.385 [90% UI: 0.372–0.397] for timepoint 1, 0.485 [0.473–0.498] for timepoint 2, and 0.557 [0.544–0.569] for timepoint 3. The average accuracy increased by 0.1 [0.087–0.114] from timepoint 1 to timepoint 2 and by 0.072 [0.058–0.085] from timepoint 2 to timepoint 3. 10.3.2 Plot the intervals for the random effect parameters These are the parameters governing the random effect distributions. First, we plot the standard deviations. sdcors &lt;- tristan::draw_var_corr(b) sdcors_wide &lt;- sdcors %&gt;% select(.draw, .parameter, sdcor) %&gt;% tidyr::spread(.parameter, sdcor) %&gt;% select(-.draw) # Create the mathematical labels for parameters group_info &lt;- sdcors %&gt;% select(.parameter:var2) %&gt;% distinct() group_info$group &lt;- group_info$grp %&gt;% stringr::str_replace(&quot;Study:ResearchID&quot;, &quot;Child-Study&quot;) %&gt;% stringr::str_replace(&quot;ResearchID&quot;, &quot;Child&quot;) group_info$r &lt;- ifelse(is.na(group_info$var2), &quot;&quot;, paste0(&quot;,&quot;, group_info$var2)) group_info$sym &lt;- ifelse(is.na(group_info$var2), &quot;sigma&quot;, &quot;rho&quot;) group_info$var1 &lt;- ifelse(group_info$var1 == &quot;(Intercept)&quot;, &quot;Intercept&quot;, group_info$var1) group_info$math &lt;- sprintf(&quot;%s[list(%s%s)]&quot;, group_info$sym, group_info$var1, group_info$r) group_info$class &lt;- ifelse(is.na(group_info$var2), &quot;scale&quot;, &quot;correlation&quot;) group_info &lt;- group_info %&gt;% select(group, class, var1, var2, parameter = .parameter, math) %&gt;% mutate(parameter = as.factor(parameter)) intervals &lt;- as.data.frame(sdcors_wide) %&gt;% mcmc_intervals_data() %&gt;% left_join(group_info, by = &quot;parameter&quot;) %&gt;% mutate(math = forcats::fct_rev(math)) ggplot(intervals %&gt;% filter(class == &quot;scale&quot;)) + aes(y = math) + # Draw medians with + then draw white horizontal lines over the horizontal # parts of the + symbols geom_point(aes(x = m), size = 3, shape = 3) + geom_hline(aes(yintercept = as.numeric(parameter)), color = &quot;white&quot;) + geom_linerangeh(aes(xmin = ll, xmax = hh)) + geom_linerangeh(aes(xmin = l, xmax = h), size = 2) + scale_y_discrete(labels = parse_text) + facet_wrap(&quot;group&quot;, ncol = 1, strip.position = &quot;left&quot;) + theme(strip.placement = &quot;outside&quot;, strip.background = element_rect(fill = NA), axis.text.y = element_text(size = rel(1.2))) + labs(title = &quot;Random effect scales&quot;, x = NULL, y = NULL, caption = &quot;90% and 50% intervals&quot;) Then the correlations. ggplot(intervals %&gt;% filter(class == &quot;correlation&quot;)) + aes(y = math) + geom_vline(xintercept = 0, size = 2, color = &quot;white&quot;) + # Draw medians with + then draw white horizontal lines over the horizontal # parts of the + symbols geom_point(aes(x = m), size = 3, shape = 3) + geom_hline(aes(yintercept = as.numeric(parameter)), color = &quot;white&quot;) + geom_linerangeh(aes(xmin = ll, xmax = hh)) + geom_linerangeh(aes(xmin = l, xmax = h), size = 2) + scale_y_discrete(labels = parse_text) + facet_wrap(&quot;group&quot;, ncol = 1, strip.position = &quot;left&quot;) + theme(strip.placement = &quot;outside&quot;, strip.background = element_rect(fill = NA), axis.text.y = element_text(size = rel(1.2))) + labs(title = &quot;Random effect correlations&quot;, x = NULL, y = NULL, caption = &quot;90% and 50% intervals&quot;) 10.3.3 Posterior predictive checks Next, we let’s check how well the model can simulate the observed data. rstanarm::pp_check(b, nreps = 200, seed = &quot;09272017&quot;) + labs( x = &quot;Proportion of looks&quot;, title = &quot;Observed data and 200 posterior simulations&quot;) + guides(color = &quot;none&quot;) + coord_cartesian(xlim = c(0, 1)) 10.3.4 Look at some predictions Plot the posterior predictions for random participants. This is the model simulating new data for these participants. set.seed(09272017) ppred &lt;- d_m %&gt;% sample_n_of(8, ResearchID) %&gt;% tristan::augment_posterior_predict(b, newdata = ., nsamples = 100) %&gt;% mutate(trials = Primary + Others) ggplot(ppred) + aes(x = Time, y = Prop, color = Study, group = Study) + geom_line(aes(y = .posterior_value / trials, group = interaction(.draw, Study)), alpha = .20) + geom_line(size = 1, color = &quot;grey50&quot;) + facet_wrap(&quot;ResearchID&quot;) + theme( legend.position = c(.95, 0), legend.justification = c(1, 0), legend.margin = margin(0)) + guides(color = guide_legend(title = NULL, override.aes = list(alpha = 1))) + labs( title = &quot;Observed means and 100 simulations of new data&quot;, x = &quot;Time after target onset&quot;, y = &quot;Proportion looks to target&quot;) Or we can plot the linear predictions. These are posterior predictions of the log-odds of looking to target before adding binomial noise. lpred &lt;- d_m %&gt;% sample_n_of(8, ResearchID) %&gt;% tristan::augment_posterior_linpred(b, newdata = ., nsamples = 100) ggplot(lpred) + aes(x = Time, y = .posterior_value, color = Study) + geom_line(aes(group = interaction(Study, ResearchID, .draw)), alpha = .1) + facet_wrap(&quot;ResearchID&quot;) + geom_point(aes(y = qlogis(Prop)), shape = 1) + theme( legend.position = c(.95, 0), legend.justification = c(1, 0), legend.margin = margin(0)) + guides(color = guide_legend(title = NULL, override.aes = list(alpha = 1))) + labs( title = &quot;Observed data and 100 posterior predictions&quot;, x = &quot;Time after target onset&quot;, y = &quot;Posterior log-odds&quot;) We can consider predictions for hypothetical, new children as well. dummy_data &lt;- d_m %&gt;% distinct(Study, Time, ot1, ot2, ot3) %&gt;% mutate(ResearchID = &quot;NEW&quot;, Primary = 0, Others = 0) lpred &lt;- dummy_data %&gt;% tristan::augment_posterior_linpred(b, newdata = ., nsamples = 1000) ggplot(lpred) + aes(x = Time, y = plogis(.posterior_value), color = Study) + geom_hline(yintercept = .25, size = 2, color = &quot;white&quot;) + geom_line(aes(group = interaction(Study, .draw)), alpha = .1, show.legend = FALSE) + facet_wrap(&quot;Study&quot;) + guides(color = guide_legend(&quot;none&quot;)) + labs( title = &quot;Posterior predictions for 1000 new participants&quot;, x = &quot;Time after target onset&quot;, y = &quot;Proportion looks to target&quot;) ggplot(lpred) + aes(x = Time, y = plogis(.posterior_value), color = Study) + geom_hline(yintercept = .25, size = 2, color = &quot;white&quot;) + stat_summary(fun.data = median_hilow, fun.args = list(conf.int = .9), size = 1, geom = &quot;linerange&quot;) + stat_summary(fun.data = median_hilow, fun.args = list(conf.int = .5), size = 1.5, geom = &quot;linerange&quot;) + stat_summary(fun.y = median, fun.args = list(conf.int = .5), size = 2.5, geom = &quot;point&quot;) + facet_wrap(&quot;Study&quot;) + guides(color = guide_legend(&quot;none&quot;)) + labs( title = &quot;Posterior predictions for 1000 new participants&quot;, x = &quot;Time after target onset&quot;, y = &quot;Proportion looks to target&quot;) by_draw &lt;- lpred %&gt;% group_by(Study, Time) %&gt;% summarise( iqr = IQR(plogis(.posterior_value)), min = min(plogis(.posterior_value)), max = max(plogis(.posterior_value)), range = max - min) ggplot(by_draw) + aes(x = Time, y = range, color = Study) + stat_summary(fun.data = median_hilow, fun.args = list(conf.int = .9), size = 1, geom = &quot;linerange&quot;) + stat_summary(fun.data = median_hilow, fun.args = list(conf.int = .5), size = 1.5, geom = &quot;linerange&quot;) + stat_summary(fun.y = median, fun.args = list(conf.int = .5), size = 2.5, geom = &quot;point&quot;) + labs( title = &quot;Ranges of predictions for 1000 new participants&quot;, x = &quot;Time after target onset&quot;, y = &quot;Max - min predicted value&quot;) ggplot(by_draw) + aes(x = Time, y = iqr, color = Study) + stat_summary(fun.data = median_hilow, fun.args = list(conf.int = .9), size = 1, geom = &quot;linerange&quot;) + stat_summary(fun.data = median_hilow, fun.args = list(conf.int = .5), size = 1.5, geom = &quot;linerange&quot;) + stat_summary(fun.y = median, fun.args = list(conf.int = .5), size = 2.5, geom = &quot;point&quot;) + labs( title = &quot;25%-75% ranges of predictions&quot;, x = &quot;Time after target onset&quot;, y = &quot;Interquartile range of predictions&quot;) + ylim(0, .25) # in_every_study &lt;- d_m %&gt;% # distinct(Study, ResearchID) %&gt;% # split(.$Study) %&gt;% # lapply(getElement, &quot;ResearchID&quot;) %&gt;% # Reduce(intersect, .) # # d_m_every &lt;- d_m %&gt;% # filter(ResearchID %in% in_every_study) # # sims &lt;- tristan::augment_posterior_linpred(b, d_m_every, nsamples = 1000) # # ggplot(sims) + # aes(x = Time, y = plogis(.posterior_value, color = Study) + # geom_hline(yintercept = .25, size = 2, color = &quot;white&quot;) + # stat_summary(fun.data = median_hilow, fun.args = list(conf.int = .9), # size = 1, geom = &quot;linerange&quot;) + # stat_summary(fun.data = median_hilow, fun.args = list(conf.int = .5), # size = 1.5, geom = &quot;linerange&quot;) + # stat_summary(fun.y = median, fun.args = list(conf.int = .5), # size = 2.5, geom = &quot;point&quot;) + # # geom_line(aes(group = interaction(Study, ResearchID, .draw)), # # alpha = .1) + # # facet_wrap(&quot;ResearchID&quot;) + # geom_point(aes(y = qlogis(Prop)), shape = 1) + # theme( # legend.position = c(.95, 0), # legend.justification = c(1, 0), # legend.margin = margin(0)) + # guides(color = guide_legend(title = NULL, override.aes = list(alpha = 1))) + # labs( # title = &quot;Observed data and 100 posterior predictions&quot;, # x = &quot;Time after target onset&quot;, # y = &quot;Posterior log-odds&quot;) # sds &lt;- fits %&gt;% # group_by(Study, coef, .draw) %&gt;% # summarise( # mean = mean(.posterior_value), # min = min(.posterior_value), # max = max(.posterior_value), # range = max - min, # sd = sd(.posterior_value)) %&gt;% # group_by(Study, coef) %&gt;% # select(-.draw) %&gt;% # do(bayesplot::mcmc_intervals_data(select(., mean:sd))) %&gt;% # ungroup() # # ggplot(sds %&gt;% filter(coef %in% c(&quot;intercept&quot;, &quot;ot1&quot;))) + # aes(y = forcats::fct_rev(Study)) + # geom_vline(xintercept = 0, size = 2, color = &quot;white&quot;) + # ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + # ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) + # geom_point(aes(x = m), size = 3, shape = 3) + # labs(x = NULL, y = NULL, caption = &quot;90% and 50% intervals&quot;) + # facet_grid(parameter ~ coef, scales = &quot;free&quot;) # facet_gird(&quot;coef&quot;, ncol = 1, strip.position = &quot;left&quot;, # labeller = label_parsed, scales = &quot;free&quot;) + # theme(strip.placement = &quot;outside&quot;, # strip.background = element_rect(fill = NA), # axis.text.y = element_text(size = rel(1.2))) 10.3.5 Predicting the future As a consequence, individual differences in word recognition at age 3, for example, will be more discriminating and predictive of age 5 language outcomes than differences at age 4. fits &lt;- readr::read_csv(&quot;./data/fits.csv.gz&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; .draw = col_integer(), #&gt; Study = col_character(), #&gt; ResearchID = col_character(), #&gt; coef = col_character(), #&gt; .posterior_value = col_double() #&gt; ) fits &lt;- fits %&gt;% semi_join(d_m) #&gt; Joining, by = c(&quot;Study&quot;, &quot;ResearchID&quot;) point_ests &lt;- fits %&gt;% group_by(Study, ResearchID, coef) %&gt;% summarise(point = median(.posterior_value)) %&gt;% ungroup() %&gt;% tidyr::spread(coef, point) scores &lt;- readr::read_csv(&quot;./data-raw/test_scores.csv&quot;) %&gt;% select(Study, ResearchID, Age, EVT_GSV, EVT_Standard, PPVT_GSV, PPVT_Standard) #&gt; Parsed with column specification: #&gt; cols( #&gt; .default = col_integer(), #&gt; Study = col_character(), #&gt; ResearchID = col_character(), #&gt; Female = col_logical(), #&gt; Male = col_logical(), #&gt; MAE = col_logical(), #&gt; AAE = col_logical(), #&gt; Maternal_Education_LMH = col_character(), #&gt; MinPair_ProportionCorrect = col_double(), #&gt; SAILS_ProportionTestCorrect = col_double() #&gt; ) #&gt; See spec(...) for full column specifications. with_ests &lt;- scores %&gt;% inner_join(point_ests) #&gt; Joining, by = c(&quot;Study&quot;, &quot;ResearchID&quot;) ggplot(with_ests) + aes(x = intercept, y = EVT_GSV, color = Study) + geom_point() + stat_smooth(method = &quot;lm&quot;) #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 2 rows containing missing values (geom_point). ggplot(with_ests) + aes(x = ot1, y = EVT_GSV, color = Study) + geom_point() + stat_smooth(method = &quot;lm&quot;) #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 2 rows containing missing values (geom_point). ggplot(with_ests) + aes(x = Age, y = intercept, color = Study) + geom_point() + stat_smooth(method = &quot;lm&quot;) ggplot(with_ests) + aes(x = Age, y = ot1, color = Study) + geom_point() + stat_smooth(method = &quot;lm&quot;) widely &lt;- with_ests %&gt;% tidyr::gather(&quot;Test&quot;, &quot;Value&quot;, -ResearchID, -Study) %&gt;% tidyr::unite(&quot;Col&quot;, Study, Test) %&gt;% tidyr::spread(Col, Value) ggplot(widely) + aes(x = TimePoint1_intercept, y = TimePoint3_EVT_GSV) + geom_point() + stat_smooth(method = &quot;lm&quot;) #&gt; Warning: Removed 67 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 67 rows containing missing values (geom_point). ggplot(widely) + aes(x = TimePoint2_intercept, y = TimePoint3_EVT_GSV) + geom_point() + stat_smooth(method = &quot;lm&quot;) #&gt; Warning: Removed 45 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 45 rows containing missing values (geom_point). ggplot(widely) + aes(x = TimePoint1_ot1, y = TimePoint3_EVT_GSV) + geom_point() + stat_smooth(method = &quot;lm&quot;) #&gt; Warning: Removed 67 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 67 rows containing missing values (geom_point). ggplot(widely) + aes(x = TimePoint2_ot1, y = TimePoint3_EVT_GSV) + geom_point() + stat_smooth(method = &quot;lm&quot;) #&gt; Warning: Removed 45 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 45 rows containing missing values (geom_point). ggplot(widely) + aes(x = TimePoint3_ot1, y = TimePoint3_EVT_GSV) + geom_point() + stat_smooth(method = &quot;lm&quot;) #&gt; Warning: Removed 35 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 35 rows containing missing values (geom_point). cor_complete &lt;- function(...) cor(..., use = &quot;pairwise.complete&quot;) test &lt;- fits %&gt;% left_join(widely) %&gt;% group_by(Study, coef, .draw) %&gt;% summarise( r_TimePoint3_EVT_Standard = cor_complete(.posterior_value, TimePoint3_EVT_Standard), r_TimePoint3_EVT_GSV = cor_complete(.posterior_value, TimePoint3_EVT_GSV), r_TimePoint2_PPVT_GSV = cor_complete(.posterior_value, TimePoint2_PPVT_GSV), r_TimePoint2_PPVT_Standard = cor_complete(.posterior_value, TimePoint2_PPVT_Standard)) #&gt; Joining, by = &quot;ResearchID&quot; c_intervals &lt;- test %&gt;% do(bayesplot::mcmc_intervals_data( select(., r_TimePoint3_EVT_Standard:r_TimePoint2_PPVT_Standard))) %&gt;% ungroup() c_intervals %&gt;% filter(parameter == &quot;r_TimePoint3_EVT_Standard&quot;) %&gt;% filter(coef %in% c(&quot;intercept&quot;, &quot;ot1&quot;)) %&gt;% ggplot() + aes(y = forcats::fct_rev(Study)) + geom_vline(xintercept = 0, size = 2, color = &quot;white&quot;) + ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) + geom_point(aes(x = m), size = 3, shape = 3) + facet_wrap(&quot;coef&quot;) + labs(x = NULL, y = NULL, caption = &quot;90% and 50% intervals&quot;) + ggtitle(&quot;Correlation of curve features and TP3 EVT Standard&quot;) c_intervals %&gt;% filter(parameter == &quot;r_TimePoint2_PPVT_Standard&quot;) %&gt;% filter(coef %in% c(&quot;intercept&quot;, &quot;ot1&quot;)) %&gt;% filter(Study != &quot;TimePoint3&quot;) %&gt;% ggplot() + aes(y = forcats::fct_rev(Study)) + geom_vline(xintercept = 0, size = 2, color = &quot;white&quot;) + ggstance::geom_linerangeh(aes(xmin = ll, xmax = hh)) + ggstance::geom_linerangeh(aes(xmin = l, xmax = h), size = 2) + geom_point(aes(x = m), size = 3, shape = 3) + facet_wrap(&quot;coef&quot;) + labs(x = NULL, y = NULL, caption = &quot;90% and 50% intervals&quot;) + ggtitle(&quot;Correlation of curve features and TP2 PPVT Standard&quot;) # test %&gt;% # filter(coef == &quot;intercept&quot;) %&gt;% # ggplot() + # aes(x = c1) + # geom_histogram() + # facet_grid(Study ~ coef) # # test %&gt;% # filter(coef == &quot;ot1&quot;) %&gt;% # ggplot() + # aes(x = c1) + # geom_histogram() + # facet_grid(Study ~ coef) # # test %&gt;% # filter(coef == &quot;ot2&quot;) %&gt;% # ggplot() + # aes(x = c1) + # geom_histogram() + # facet_grid(Study ~ coef) widely %&gt;% tidy_corr(ends_with(&quot;EVT_GSV&quot;)) #&gt; column1 column2 estimate n p.value #&gt; 1 TimePoint1_EVT_GSV TimePoint2_EVT_GSV 0.7087281 128 0 #&gt; 2 TimePoint1_EVT_GSV TimePoint3_EVT_GSV 0.6874591 121 0 #&gt; 3 TimePoint2_EVT_GSV TimePoint3_EVT_GSV 0.8511192 143 0 widely %&gt;% tidy_corr(TimePoint3_EVT_GSV, ends_with(&quot;intercept&quot;), ends_with(&quot;ot1&quot;)) %&gt;% filter(column1 == &quot;TimePoint3_EVT_GSV&quot;) #&gt; column1 column2 estimate n p.value #&gt; 1 TimePoint3_EVT_GSV TimePoint1_intercept 0.5057735 121 3.271201e-09 #&gt; 2 TimePoint3_EVT_GSV TimePoint2_intercept 0.4398412 143 3.877217e-08 #&gt; 3 TimePoint3_EVT_GSV TimePoint3_intercept 0.3648115 153 3.547623e-06 #&gt; 4 TimePoint3_EVT_GSV TimePoint1_ot1 0.4517970 121 1.975589e-07 #&gt; 5 TimePoint3_EVT_GSV TimePoint2_ot1 0.3252970 143 7.363810e-05 #&gt; 6 TimePoint3_EVT_GSV TimePoint3_ot1 0.3143955 153 7.564233e-05 widely %&gt;% tidy_corr(TimePoint2_PPVT_GSV, TimePoint1_ot1, TimePoint2_ot1) %&gt;% filter(column1 == &quot;TimePoint2_PPVT_GSV&quot;) #&gt; column1 column2 estimate n p.value #&gt; 1 TimePoint2_PPVT_GSV TimePoint1_ot1 0.4806688 127 1.068902e-08 #&gt; 2 TimePoint2_PPVT_GSV TimePoint2_ot1 0.3418107 161 9.092367e-06 widely %&gt;% tidy_corr(TimePoint2_PPVT_GSV, TimePoint1_ot1, TimePoint2_ot1, TimePoint1_intercept, TimePoint2_intercept) %&gt;% filter(column1 == &quot;TimePoint2_PPVT_GSV&quot;) #&gt; column1 column2 estimate n p.value #&gt; 1 TimePoint2_PPVT_GSV TimePoint1_ot1 0.4806688 127 1.068902e-08 #&gt; 2 TimePoint2_PPVT_GSV TimePoint2_ot1 0.3418107 161 9.092367e-06 #&gt; 3 TimePoint2_PPVT_GSV TimePoint1_intercept 0.4997462 127 2.211273e-09 #&gt; 4 TimePoint2_PPVT_GSV TimePoint2_intercept 0.4248143 161 1.947097e-08 10.3.6 Relationship with child-level variables Vocabulary size and lexical processing will be tightly correlated such that large year-over-year gains in one measure will predict large year-over-years gains in the other measure. "],
["visualize-looks-to-each-image-type.html", "Chapter 11 Visualize looks to each image type 11.1 Comparing strong versus weak foils 11.2 Look for individual differences in competitor sensitivity 11.3 Interim summary", " Chapter 11 Visualize looks to each image type We continue our exploration of the raw data by aggregating looks to each image type. library(dplyr) #&gt; Warning: package &#39;dplyr&#39; was built under R version 3.4.2 library(rlang) library(littlelisteners) library(ggplot2) #&#39; group `data` by some grouping variables (`...`), #&#39; randomly select `size` of the groups #&#39; keep just the data from those sampled groups sample_n_of &lt;- function(data, size, ...) { dots &lt;- quos(...) rows &lt;- data_frame(row = seq_len(nrow(data))) rows[, &quot;group&quot;] &lt;- data %&gt;% group_by(!!! dots) %&gt;% group_indices() subset &lt;- rows %&gt;% filter(.data$group %in% sample(unique(.data$group), size)) %&gt;% pull(.data$row) data[subset, ] } create_pairs &lt;- function(xs) { if (!is.factor(xs)) xs &lt;- ordered(xs) xs %&gt;% levels() %&gt;% rev() %&gt;% combn(2) %&gt;% t() %&gt;% as.data.frame() %&gt;% set_names(&quot;x1&quot;, &quot;x2&quot;) %&gt;% mutate(name = paste0(.data$x1, &quot;-&quot;, .data$x2)) %&gt;% mutate_all(as.character) %&gt;% arrange(x1, desc(x2)) } compare_pairs &lt;- function(data, levels, values, f = `-`) { levels &lt;- enquo(levels) values &lt;- enquo(values) pairs &lt;- data %&gt;% pull(!! levels) %&gt;% create_pairs() wide &lt;- tidyr::spread(data, !! levels, !! values) for (row_i in seq_len(nrow(pairs))) { pair_i &lt;- pairs[row_i, ] wide[, pair_i$name] &lt;- f(wide[[pair_i$x1]], wide[[pair_i$x2]]) } wide %&gt;% select(-one_of(c(pairs$x1), c(pairs$x2))) %&gt;% tidyr::gather(&quot;pair&quot;, &quot;value&quot;, one_of(c(pairs$name))) %&gt;% mutate(pair = factor(.data$pair, levels = pairs$name)) } tidy_corr &lt;- function(df, ..., .type = c(&quot;pearson&quot;, &quot;spearman&quot;)) { vars &lt;- quos(...) select(df, !!! vars) %&gt;% as.matrix() %&gt;% Hmisc::rcorr(type = .type) %&gt;% broom::tidy() %&gt;% tibble::remove_rownames() %&gt;% arrange(.data$column1, .data$column2) } Earlier we cleaned the data to remove trials with excessive missing data and blocks of trials with too few trials. Read in that data. data &lt;- readr::read_csv(&quot;./data/aim1-screened.csv.gz&quot;) Plot growth curves to each AOI. resp_def &lt;- create_response_def( primary = &quot;Target&quot;, others = c(&quot;PhonologicalFoil&quot;, &quot;SemanticFoil&quot;, &quot;Unrelated&quot;), elsewhere = &quot;tracked&quot;, missing = NA ) defs &lt;- cycle_response_def(resp_def) # We use this later on semy_defs &lt;- defs %&gt;% purrr::keep(~ .x$primary %in% c(&quot;Target&quot;, &quot;SemanticFoil&quot;)) phon_defs &lt;- defs %&gt;% purrr::keep(~ .x$primary %in% c(&quot;Target&quot;, &quot;PhonologicalFoil&quot;)) # Aggregate looks using each rule looks_by_aoi &lt;- data %&gt;% aggregate_looks(defs, Study + ResearchID + Time ~ GazeByImageAOI) %&gt;% rename(AOI = .response_def) %&gt;% select(AOI:Time, Prop, Primary, Unrelated) %&gt;% mutate(AOI = factor(AOI, c(&quot;Target&quot;, &quot;PhonologicalFoil&quot;, &quot;SemanticFoil&quot;, &quot;Unrelated&quot;))) ggplot(looks_by_aoi) + aes(x = Time, y = Prop, color = Study) + geom_hline(size = 2, color = &quot;white&quot;, yintercept = .25) + stat_smooth() + facet_grid( ~ AOI) + labs(x = &quot;Time after target onset [ms]&quot;, y = &quot;Proportion looks&quot;, caption = &quot;GAM smooth on partially screened data&quot;) + theme_grey(base_size = 9) + theme(legend.position = c(0.95, 0.95), legend.justification = c(1, 1)) #&gt; `geom_smooth()` using method = &#39;gam&#39; Normalize by using ratio of looks to each AOI versus the unrelated image. ggplot(looks_by_aoi %&gt;% filter(AOI != &quot;Unrelated&quot;)) + aes(x = Time, y = log(Primary / Unrelated), color = Study) + geom_hline(size = 2, color = &quot;white&quot;, yintercept = 0) + stat_smooth() + facet_grid( ~ AOI) + labs(x = &quot;Time after target onset [ms]&quot;, y = &quot;Log odds looking to word vs. unrelated word&quot;, caption = &quot;GAM smooth on partially screened data&quot;) + theme_grey(base_size = 9) + theme(legend.position = c(0.95, 0.95), legend.justification = c(1, 1)) #&gt; `geom_smooth()` using method = &#39;gam&#39; #&gt; Warning: Removed 10559 rows containing non-finite values (stat_smooth). Each curve is the log odds of looking to the target, phonological foil, and semantic foil versus the unrelated word. Positive values mean more looks to an image type than the unrelated. If you think of the y axis as the image’s relatedness to the target, you can see a time course of relatedness in each panel: Here early phonological effects meaning early relatedness and later, flatter semantic effects meaning late relate relatedness. (Which makes extra sense if phonological representations come into play before semantic ones.) This plot suggests an important finding: Children becoming more sensitive to the phonological and semantic foils as they grow older. (I use the verb suggest because this is still a preliminary finding). Jan and I had made opposite predictions about whether this would happen. Her argument, I think, was that children become better at word recognition by becoming better able to inhibit interference from competing words. This plot would suggest that they show increased sensitive to the target and foils words by looking less to the unrelated word as they age and reapportioning those looks to the other three lexically relevant words. 11.1 Comparing strong versus weak foils In Law et al. (2016), we ignored trials for certain items where we didn’t think the phonological or semantic similarity was strong enough. trial_info &lt;- bind_rows( readr::read_csv(&quot;data-raw/rwl_timepoint1_trials.csv.gz&quot;), readr::read_csv(&quot;data-raw/rwl_timepoint2_trials.csv.gz&quot;), readr::read_csv(&quot;data-raw/rwl_timepoint3_trials.csv.gz&quot;)) %&gt;% select(TrialID, Target = WordTarget, PhonologicalFoil = WordPhonologicalFoil, SemanticFoil = WordSemanticFoil, Unrelated = WordUnrelated) good_phono &lt;- c(&quot;bear&quot;, &quot;bee&quot;, &quot;bell&quot;, &quot;dress&quot;, &quot;drum&quot;, &quot;flag&quot;, &quot;fly&quot;, &quot;heart&quot;, &quot;horse&quot;, &quot;pan&quot;, &quot;pear&quot;, &quot;pen&quot;, &quot;vase&quot;) good_semy &lt;- c(&quot;bear&quot;, &quot;bee&quot;, &quot;bell&quot;, &quot;bread&quot;, &quot;cheese&quot;, &quot;dress&quot;, &quot;drum&quot;, &quot;fly&quot;, &quot;horse&quot;, &quot;pan&quot;, &quot;pear&quot;, &quot;shirt&quot;, &quot;spoon&quot;) words &lt;- trial_info %&gt;% distinct(Target, PhonologicalFoil, SemanticFoil, Unrelated) phono_foils &lt;- split(words, words$Target %in% good_phono) %&gt;% lapply(arrange, Target) %&gt;% setNames(c(&quot;weak_foil&quot;, &quot;strong_foil&quot;)) semy_foils &lt;- split(words, words$Target %in% good_semy) %&gt;% lapply(arrange, Target) %&gt;% setNames(c(&quot;weak_foil&quot;, &quot;strong_foil&quot;)) phono_foils$strong_foil %&gt;% knitr::kable(caption = &quot;Trials with strong phonological foils.&quot;) Table 11.1: Trials with strong phonological foils. Target PhonologicalFoil SemanticFoil Unrelated bear bell horse ring bee bear fly heart bell bee drum swing dress drum shirt swing drum dress bell sword flag fly kite pear fly flag bee pen heart horse ring bread heart horse ring pan horse heart bear pan pan pear spoon vase pan pear spoon bell pear pen cheese ring pear pen cheese vase pen pear sword van vase van gift swan phono_foils$weak_foil %&gt;% knitr::kable(caption = &quot;Trials with weak phonological foils.&quot;) Table 11.1: Trials with weak phonological foils. Target PhonologicalFoil SemanticFoil Unrelated bread bear cheese vase cheese shirt bread van gift kite vase bread kite gift flag shirt ring swing dress flag shirt cheese dress fly spoon swan pan drum swan spoon bee bell swan spoon bee ring swing spoon kite heart sword swan pen gift van pan horse sword semy_foils$strong_foil %&gt;% knitr::kable(caption = &quot;Trials with strong semantic foils.&quot;) Table 11.1: Trials with strong semantic foils. Target PhonologicalFoil SemanticFoil Unrelated bear bell horse ring bee bear fly heart bell bee drum swing bread bear cheese vase cheese shirt bread van dress drum shirt swing drum dress bell sword fly flag bee pen horse heart bear pan pan pear spoon vase pan pear spoon bell pear pen cheese ring pear pen cheese vase shirt cheese dress fly spoon swan pan drum semy_foils$weak_foil %&gt;% knitr::kable(caption = &quot;Trials with weak semantic foils.&quot;) Table 11.1: Trials with weak semantic foils. Target PhonologicalFoil SemanticFoil Unrelated flag fly kite pear gift kite vase bread heart horse ring bread heart horse ring pan kite gift flag shirt pen pear sword van ring swing dress flag swan spoon bee bell swan spoon bee ring swing spoon kite heart sword swan pen gift van pan horse sword vase van gift swan We should verify that the two sets of words behave differently. weak_phon_looks &lt;- trial_info %&gt;% semi_join(phono_foils$weak_foil) %&gt;% inner_join(data) %&gt;% mutate(PhonFoil = &quot;Weak&quot;) #&gt; Joining, by = c(&quot;Target&quot;, &quot;PhonologicalFoil&quot;, &quot;SemanticFoil&quot;, &quot;Unrelated&quot;) #&gt; Joining, by = &quot;TrialID&quot; strong_phon_looks &lt;- trial_info %&gt;% semi_join(phono_foils$strong_foil) %&gt;% inner_join(data) %&gt;% mutate(PhonFoil = &quot;Strong&quot;) #&gt; Joining, by = c(&quot;Target&quot;, &quot;PhonologicalFoil&quot;, &quot;SemanticFoil&quot;, &quot;Unrelated&quot;) #&gt; Joining, by = &quot;TrialID&quot; phon_data &lt;- bind_rows(strong_phon_looks, weak_phon_looks) weak_semy_looks &lt;- trial_info %&gt;% semi_join(semy_foils$weak_foil) %&gt;% inner_join(data) %&gt;% mutate(SemyFoil = &quot;Weak&quot;) #&gt; Joining, by = c(&quot;Target&quot;, &quot;PhonologicalFoil&quot;, &quot;SemanticFoil&quot;, &quot;Unrelated&quot;) #&gt; Joining, by = &quot;TrialID&quot; strong_semy_looks &lt;- trial_info %&gt;% semi_join(semy_foils$strong_foil) %&gt;% inner_join(data) %&gt;% mutate(SemyFoil = &quot;Strong&quot;) #&gt; Joining, by = c(&quot;Target&quot;, &quot;PhonologicalFoil&quot;, &quot;SemanticFoil&quot;, &quot;Unrelated&quot;) #&gt; Joining, by = &quot;TrialID&quot; phon_data &lt;- bind_rows(strong_phon_looks, weak_phon_looks) semy_data &lt;- bind_rows(weak_semy_looks, strong_semy_looks) looks_by_aoi2 &lt;- phon_data %&gt;% aggregate_looks(phon_defs, Study + ResearchID + PhonFoil + Time ~ GazeByImageAOI) %&gt;% rename(AOI = .response_def) %&gt;% select(AOI:Time, PhonFoil, Prop, Primary, PhonologicalFoil, Unrelated) %&gt;% mutate(AOI = factor(AOI, c(&quot;Target&quot;, &quot;PhonologicalFoil&quot;, &quot;SemanticFoil&quot;, &quot;Unrelated&quot;))) looks_by_aoi3 &lt;- semy_data %&gt;% aggregate_looks(semy_defs, Study + ResearchID + SemyFoil + Time ~ GazeByImageAOI) %&gt;% rename(AOI = .response_def) %&gt;% select(AOI:Time, SemyFoil, Prop, Primary, SemanticFoil, Unrelated) %&gt;% mutate(AOI = factor(AOI, c(&quot;Target&quot;, &quot;PhonologicalFoil&quot;, &quot;SemanticFoil&quot;, &quot;Unrelated&quot;))) ggplot(looks_by_aoi2 %&gt;% filter(AOI != &quot;Unrelated&quot;)) + aes(x = Time, y = log(Primary / Unrelated), color = Study, linetype = PhonFoil) + geom_hline(size = 2, color = &quot;white&quot;, yintercept = 0) + stat_smooth() + facet_grid( ~ AOI) + labs(x = &quot;Time after target onset [ms]&quot;, y = &quot;Log odds looking to word vs. unrelated word&quot;, caption = &quot;GAM smooth on partially screened data&quot;) + theme_grey(base_size = 9) #&gt; `geom_smooth()` using method = &#39;gam&#39; #&gt; Warning: Removed 48304 rows containing non-finite values (stat_smooth). ggplot(looks_by_aoi3 %&gt;% filter(AOI != &quot;Unrelated&quot;)) + aes(x = Time, y = log(Primary / Unrelated), color = Study, linetype = SemyFoil) + geom_hline(size = 2, color = &quot;white&quot;, yintercept = 0) + stat_smooth() + facet_grid( ~ AOI) + labs(x = &quot;Time after target onset [ms]&quot;, y = &quot;Log odds looking to word vs. unrelated word&quot;, caption = &quot;GAM smooth on partially screened data&quot;) + theme_grey(base_size = 9) #&gt; `geom_smooth()` using method = &#39;gam&#39; #&gt; Warning: Removed 43071 rows containing non-finite values (stat_smooth). What’s going on here: The weak phonological foils are indeed weaker than the strong foils. The strong semantic foils appear stronger than the weak ones. The strong foils show a growth curve pattern of increasing looks away from baseline and there a developmental difference among the growth curves for each time point. Children have a lower advantage for the target (vs unrelated) in weak foil trials because… why? Now let’s look at the target versus each foil and the unrelated. semantic_target_curves &lt;- looks_by_aoi3 %&gt;% filter(AOI == &quot;Target&quot;) %&gt;% mutate(`Target vs Semantic` = log(Primary / SemanticFoil), `Target vs Unrelated` = log(Primary / Unrelated)) %&gt;% tidyr::gather(&quot;Comparison&quot;, &quot;LogOdds&quot;, `Target vs Semantic`, `Target vs Unrelated`) phonological_target_curves &lt;- looks_by_aoi2 %&gt;% filter(AOI == &quot;Target&quot;) %&gt;% mutate(`Target vs Phonological` = log(Primary / PhonologicalFoil), `Target vs Unrelated` = log(Primary / Unrelated)) %&gt;% tidyr::gather(&quot;Comparison&quot;, &quot;LogOdds&quot;, `Target vs Phonological`, `Target vs Unrelated`) ggplot(phonological_target_curves) + aes(x = Time, y = LogOdds, color = Study, linetype = PhonFoil) + geom_hline(size = 2, color = &quot;white&quot;, yintercept = 0) + stat_smooth() + facet_grid( ~ Comparison) + labs(x = &quot;Time after target onset [ms]&quot;, y = &quot;Log odds looking&quot;, caption = &quot;GAM smooth on partially screened data&quot;) + theme_grey(base_size = 9) #&gt; `geom_smooth()` using method = &#39;gam&#39; #&gt; Warning: Removed 37498 rows containing non-finite values (stat_smooth). Both comparisons attain the same height, so phonological and unrelated foils affect processing equally later in the trial. The strong phonological foils curves in the Target vs Phonological comparison rise later than the weak foils, reflecting early looks to the phonological foils. ggplot(semantic_target_curves) + aes(x = Time, y = LogOdds, color = Study, linetype = SemyFoil) + geom_hline(size = 2, color = &quot;white&quot;, yintercept = 0) + stat_smooth() + facet_grid( ~ Comparison) + labs(x = &quot;Time after target onset [ms]&quot;, y = &quot;Log odds looking&quot;, caption = &quot;GAM smooth on partially screened data&quot;) + theme_grey(base_size = 9) #&gt; `geom_smooth()` using method = &#39;gam&#39; #&gt; Warning: Removed 29775 rows containing non-finite values (stat_smooth). The two comparisons do not attain the same height, so the semantic foil reduces odds of fixating to the target later on in a trial. There appears to be no difference in strong and weak foils in Year 2 and Year 3, so I might be able to collapse to remove this distinction and include more items in the analysis. 11.2 Look for individual differences in competitor sensitivity […put this on hold for a while…] 11.3 Interim summary Visual evidence that the semantic foil and phonological foil become more relevant (compared to unrelated foil) each. Our previous distinction between strong and weak foils still applies, although it might be better to exclude only the (a priori) weakest foils, like the rime phonological foils. References "],
["mp-experiment-items.html", "A Items used in the mispronunciation experiment", " A Items used in the mispronunciation experiment The stimuli changed between Year 1 and Year 2 so that dog/tog was replaced with rice/wice. Time Points Word Group Condition Audio (IPA) Familiar Object Unfamiliar Object 1 dog Correct Production /dɔg/ dog wombat Mispronunciation /tɔg/ dog wombat Nonword /vef/ ball sextant 1, 2, 3 cake Correct Production /kek/ cake horned melon Mispronunciation /gek/ cake horned melon Nonword /pʌm/ book churn 1, 2, 3 duck Correct Production /dʌk/ duck toy creature Mispronunciation /gʌk/ duck toy creature Nonword /ʃæn/ cup reed 1, 2, 3 girl Correct Production /gɜ˞l/ girl marmoset Mispronunciation /dɜ˞l/ girl marmoset Nonword /nedʒ/ car work holder 1, 2, 3 shoes Correct Production /ʃuz/ shoes flasks Mispronunciation /suz/ shoes flasks Nonword /giv/ sock trolley 1, 2, 3 soup Correct Production /sup/ soup steamer Mispronunciation /ʃup/ soup steamer Nonword /tʃim/ bed pastry mixer 2, 3 rice Correct Production /ɹaɪs/ rice anise Mispronunciation /waɪs/ rice anise Nonword /bep/ ball sextant "],
["vw-experiment-items.html", "B Items used in the visual world experiment", " B Items used in the visual world experiment Each row of the table represents a set of four images used in a trial for the experiment. There were two blocks of trials with different images and trial orderings. For the two unrelated foils with more than one word listed, the first word was used in block one and the second in block 2. Target Phonological Semantic Unrelated bear bell horse ring bee bear fly heart bell bee drum swing bread bear cheese vase cheese shirt bread van dress drum shirt swing drum dress bell sword flag fly kite pear fly flag bee pen gift kite vase bread heart horse ring bread/pan horse heart bear pan kite gift flag shirt pan pear spoon vase pear pen cheese ring/vase pen pear sword van ring swing dress flag shirt cheese dress fly spoon swan pan drum swan spoon bee bell swing spoon kite heart sword swan pen gift van pan horse sword vase van gift swan "],
["related-work.html", "C Related Work", " C Related Work In this section, I clarify relationships between this project and other word recognition research reported from our lab. In short, our lab has reported results about the two-image and four-image experiments from cross-sectional samples, describing child-level measures that predict performance in these tasks. In contrast, my dissertation 1) focuses on the longitudinal development of word recognition and 2) engages with the fine-grained details of lexical processing. Law and Edwards (2015) analyzed a different version of the mispronunciation experiment on a different sample of children (n = 34, 30-46 months old). This earlier version included both real word and the mispronunciation of the real word in the same block of trial. For example, a child would hear “dog” and “tog” during the same session of the experiment. This design might subtly temper the effect of mispronounced stimuli by allowing the listener to compare the mispronunciation to its correctly produced counterpart. The version of the experiment in Specific Aim 2 separates the real words and mispronunciations so that a child never hears a familiar word and its mispronunciation during the same block of trials. With this design, there is no explicit point of comparison for the mispronunciation, and the child has to rely on his or her own lexical representations when processing these words. Law et al. (2016) analyzed data from the four-image experiment in Specific Aim 1. This study featured a diverse cross-sectional sample of 60 children, half of whom received the experiment in African American English and half received it in Mainstream American English. The sample ranged in age from 28 to 60 months. The study “borrowed” data from 23 participants from Year 1 of the longitudinal study to enrich parts of the samples demographics. For this manuscript, we analyzed how vocabulary and maternal education predicted looking patterns, including relative looks to the semantic and phonological foils. Mahr and Edwards (under review) was the manuscript I originally authored for my preliminary examinations. The paper analyzes the same kinds of relations as Weisleder and Fernald (2013) which showed that lexical processing efficiency mediated the effect of language input on future vocabulary size. In particular, I asked whether word recognition performance on the four-image task of Specific Aim 1, vocabulary size, and home language input data from Year 1 predicted vocabulary size at Year 2. The paper only examined looks to the familiar image from one year of the study, so it did not analyze any lexical competition effects or the development of word recognition within children. References "],
["references.html", "References", " References "]
]
