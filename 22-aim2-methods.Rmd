Method {#aim2-method}
=======================================================================

```{r include = FALSE}
knitr::read_chunk("./helpers.R")
if (interactive()) source("./helpers.R")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r helpers, message = FALSE, warning = FALSE}
```

```{r include = FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

```{r load-mp-data, include = FALSE}
# Load trial info
trials <- c(
  "./data-raw/mp_timepoint1_trials.csv.gz", 
  "./data-raw/mp_timepoint2_trials.csv.gz", 
  "./data-raw/mp_timepoint3_trials.csv.gz") %>% 
  lapply(readr::read_csv) %>% 
  bind_rows() %>% 
  select(TrialID, Condition = StimType, WordGroup)

# Load gazes
looks <- c(
  "./data-raw/mp_timepoint1_looks.csv.gz", 
  "./data-raw/mp_timepoint2_looks.csv.gz", 
  "./data-raw/mp_timepoint3_looks.csv.gz") %>% 
  lapply(readr::read_csv) %>% 
  bind_rows()

looks <- left_join(looks, trials) %>%
  filter(Version == "Standard") %>% 
  mutate(Age = convert_study_to_age(Study))

# Keep only frames from -500 to 2000 plus or minus any frames to make the 
# number of frames divisible by 3 (for binning)
times_to_keep <- looks %>% 
  distinct(Time) %>% 
  trim_to_bin_width(
    bin_width = 3, time_var = Time, key_time = 0, 
    key_position = 2, min_time = -500, max_time = 2000) %>% 
  pull(Time) %>% 
  range()

# Aggregate looks to target
resp_def <- create_response_def(
  primary = "Target",
  others = "Distractor",
  elsewhere = "tracked",
  missing = NA)

raw_data <- looks %>% 
  filter(between(Time, times_to_keep[1], times_to_keep[2])) %>% 
  aggregate_looks(
    resp_def, Study + Age + ResearchID + Condition + Time ~ GazeByImageAOI)
```


Data collection for this experiment occurred during the same
longitudinal study as the familiar word recognition experiment of Aim 1.
Thus, this experiment used the same participants, same stimulus
preparation, and same general experimental procedure that were reported
in detail in [Chapter \@ref(aim1-method)](#aim1-method).


Mispronunciation task
------------------------------------------------------------------------

This experiment is an adaptation of the mispronunciation detection task
by @WhiteMorgan2008 and @MPPaper. In the experiment, two images are
presented onscreen—a familiar object and an unfamiliar object—and the
child hears a prompt to view one of the images. In the *correct
pronunciation* (or *real word*) and *mispronunciation* conditions, the
child hears either the familiar word (e.g., *soup*) or a one-feature
mispronunciation of the first consonant of the target word (*shoup*).
These conditions are designed to test whether children map
mispronunciations to novel words. To encourage fast referent selection,
there were also trials in a *nonword* condition where the label was an
unambiguous novel word (e.g., *cheem* presented with images of a bed and
a novel-looking pastry mixer). Each nonword was constructed to match the
phonotactic probability of one of the mispronunciations. 
Figure \@ref(fig:`r sample-mp-screen`) shows the displays used in two
trials. Importantly, within a block of trials, the child never hears
both the correct and mispronounced forms of the same word. A child
hearing "duck" then a few trials later hearing "guck" would provide a
basis of comparison so that the child can decide that "guck" is probably
not "duck"---our design avoided this situation.

(ref:sample-mp-screen-cap) Example displays for (*left*) a trial in
which *duck* is mispronounced as "guck"and (*right*) a trial in which
the nonword "cheem" is presented.

```{r sample-mp-screen, fig.cap = "(ref:sample-mp-screen-cap)", echo = FALSE, out.width = "100%"}
library(magick)
dir_screens <- "./misc/mp-screens/TimePoint1/actual/"

i1 <- dir_screens %>% 
  file.path("Block1_03_MP_bull2_duck2_ImageR_guk.png") %>% 
  image_read() %>% 
  image_scale("x400")

i2 <- dir_screens %>% 
  file.path("Block1_14_nonsense_bed2_mixer2_ImageL_Cim.png") %>% 
  image_read() %>% 
  image_scale("x400")

image_append(c(i1, i2))
```

<!-- (Maybe explain why the nonword condition give children the license to -->
<!-- interpret a mispronunciation as a nonword.) -->
<!-- So, children sometimes heard correct productions that can be mapped to familiar -->
<!-- objects and sometimes heard flagrant nonwords that could be quickly associated -->
<!-- to unfamilar objects. Both of these behaviors are encouraged by the experiment. -->

In a block of the experiment, there were 12 trials each from the correct
production, mispronunciation, and nonword conditions, and children
received two blocks of the task. A complete list of the items used in
the experiment over the three years of the study is included in
[Appendix \@ref(mp-experiment-items)](#mp-experiment-items).


Visual stimuli
------------------------------------------------------------------------

The images used in the experiment consisted of color photographs on gray
backgrounds. As in the familiar word recognition, these images were
piloted in two preschool classrooms. Piloting confirmed that children
consistently used the same label for familiar objects and did not
consistently use the same label for novel/unfamiliar objects.


Data screening {#aim2-screening}
------------------------------------------------------------------------

```{r aim2-screening-rules}
# We use the following options for data screening.
rules <- list(
  screening_window = c(0, 2000),
  missing_data_limit = .5,
  min_trials_per_block = 18,
  min_trials_per_condition = 6)

opts_model <- list(
  bin_width = 3,
  start_time = 250,
  end_time = 1500
)

opts_model$bin_length <- round(opts_model$bin_width * 16.67, -1)
opts_model
```

```{r apply-data-screening-rules, include = FALSE}
# The first and last times are offset by 20 ms because we downsample our data
# into 50-ms (3-frame) bins. The frames at -16.65, 0, +16.65 ms make up the
# frames in the bin at time 0, so we need to capture the frame at -16.65. The
# same reasoning applies to the frame at 2016.65.
screening_times <- looks %>% 
  distinct(Time) %>% 
  trim_to_bin_width(
    bin_width = 3, key_time = 0, 
    key_position = 2, time_var = Time, 
    min_time = rules$screening_window[1] - 20, 
    max_time = rules$screening_window[2] + 20) %>% 
  pull(Time) %>% 
  range()
screening_times

missing_data_by_trial <- looks %>% 
  filter(screening_times[1] <= Time, Time <= screening_times[2]) %>% 
  aggregate_looks(
    resp_def, 
    Study + Version + ResearchID + Basename + 
      Condition + TrialNo ~ GazeByImageAOI) %>% 
  mutate(BadTrial = rules$missing_data_limit <= PropNA)

bad_trial_counts <- missing_data_by_trial %>% 
  count(Study, ResearchID, Basename, BadTrial) %>% 
  tidyr::spread(BadTrial, n) %>% 
  rename(n_bad = `TRUE`, n_good = `FALSE`) %>% 
  # Replace NAs with 0, in case there were 0 good trials in a block or 
  # 0 bad trials in a block
  mutate(
    n_bad = coalesce(n_bad, 0L),
    n_good = coalesce(n_good, 0L),
    trials = n_good + n_bad,
    prop_bad = round(n_bad / trials, 2)) 

# Min trials per block cleaning
blocks_to_drop <- bad_trial_counts %>% 
  filter(n_good < rules$min_trials_per_block)
blocks_to_drop

# Identifying unreliable trials
leftover_bad_trials <- missing_data_by_trial %>% 
  anti_join(blocks_to_drop, by = c("Study", "ResearchID", "Basename")) %>% 
  filter(rules$missing_data_limit <= PropNA)

# Count reliable trials per Study x ResearchID x Condition
good_trials_per_condition <- missing_data_by_trial %>% 
  anti_join(blocks_to_drop, by = c("Study", "Basename")) %>% 
  anti_join(leftover_bad_trials, by = c("Study", "Basename", "TrialNo")) %>% 
  count(Study, ResearchID, Condition, BadTrial) %>% 
  tidyr::spread(BadTrial, n) %>% 
  rename(n_good = `FALSE`) %>% 
  mutate(n_good = coalesce(n_good, 0L)) %>% 
  # Make sure that there are three Condition rows for each Study x ResearchID
  tidyr::complete(
    tidyr::nesting(Study, ResearchID), Condition, 
    fill = list(n_good = 0))

too_few_trials_per_condition <- good_trials_per_condition %>% 
  filter(n_good < rules$min_trials_per_condition) %>% 
  distinct(Study, ResearchID)

clean_looks <- looks %>% 
  anti_join(blocks_to_drop, by = c("Study", "Basename")) %>% 
  anti_join(leftover_bad_trials, by = c("Study", "Basename", "TrialNo")) %>% 
  anti_join(too_few_trials_per_condition, by = c("Study", "ResearchID"))

data <- clean_looks %>% 
  filter(between(Time, times_to_keep[1], times_to_keep[2])) %>% 
  readr::write_csv("./data/aim2-screened.csv.gz") 
```

```{r bad-version-counts, message = FALSE}
ids_by_version <- readr::read_csv("./data-raw/mp_timepoint1_blocks.csv") %>% 
  select(ResearchID, Basename, Version) %>% 
  split(.$Version) %>% 
  lapply(getElement, "ResearchID")

n_lost_in_bad_version <- ids_by_version$`Early attention getter` %>% 
  setdiff(ids_by_version$Standard) %>% 
  length()
```

Table \@ref(tab:mp-screening-counts) shows the numbers of participants and
trials excluded during each of year of the study due to unreliable data. As with the experiment
in Aim 1, there were more children in the second timepoint than the first
timepoint due to a timing error in the initial version of this experiment,
leading to the exclusion of `r n_lost_in_bad_version` participants from the
first timepoint.

```{r mp-screening-counts, echo = FALSE}
screening_results <- list(Screened = clean_looks, Raw = looks) %>% 
  bind_rows(.id = "Dataset") %>% 
  distinct(Dataset, Age, ResearchID, Basename, TrialID) %>% 
  group_by(Dataset, Age) %>% 
  summarise(
    `N Children` = n_distinct(ResearchID),
    `N Blocks` = n_distinct(Basename),
    `N Trials` = n_distinct(TrialID)) %>% 
  ungroup()

# Add an extra set of rows computing difference
screening_results2 <- screening_results %>% 
  tidyr::gather("Stat", "Value", -Age, -Dataset) %>% 
  tidyr::spread(Dataset, Value) %>% 
  mutate(`Raw &minus; Screened` = Raw - Screened) %>% 
  tidyr::gather(Dataset, "Count", -Age, -Stat) %>% 
  tidyr::spread(Stat, Count) %>% 
  mutate(
    Dataset = Dataset %>% 
      factor(c("Raw", "Screened", "Raw &minus; Screened"))) %>% 
  select(Dataset, Age, `N Children`, `N Blocks`, `N Trials`) %>% 
  arrange(Dataset, Age) %>% 
  mutate(Dataset = Dataset %>% as.character() %>% replace_if_same_as_last(""))

knitr::kable(
  screening_results2,
  caption = "Eyetracking data before and after data screening.", 
  booktabs = TRUE)
```



After mapping the gaze coordinates onto the onscreen images, I performed
data screening following the same set of steps as in Chapter X. First, I
plotted the average growth curve for each Year x Condition to determine
the main time window for the analyses, based on where the data
plateaued. The window was from `r rules$screening_window[1]` to
`r rules$screening_window[2]` ms after noun onset. Next, I identified a
trial as *unreliable* if at least `r rules$missing_data_limit * 100`% of
the looks were missing during the time window, and I excluded an entire
block of trials if it had fewer than `r rules$min_trials_per_block`
reliable trials. As an additional criterion, I excluded participants who
failed to provide at least `r rules$min_trials_per_condition` reliable
trials per experimental condition.




