Method {#aim2-method}
=======================================================================

```{r include = FALSE}
knitr::read_chunk("./helpers.R")
if (interactive()) source("./helpers.R")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r helpers, message = FALSE, warning = FALSE}
```

```{r include = FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

```{r load-mp-data, include = FALSE}
# Load trial info
trials <- c(
  "./data-raw/mp_timepoint1_trials.csv.gz", 
  "./data-raw/mp_timepoint2_trials.csv.gz", 
  "./data-raw/mp_timepoint3_trials.csv.gz") %>% 
  lapply(readr::read_csv) %>% 
  bind_rows() %>% 
  select(TrialID, Condition = StimType, WordGroup, TargetWord, Bias_ImageAOI)

# Include whether the bias image was the familiar or unfamiliar image
bias_familiarity <- tribble(
  ~Bias_ImageAOI, ~Bias_Fam,
  "Target", "Familiar",
  "Distractor", "Unfamiliar"
)

trials <- trials %>% 
  left_join(bias_familiarity)

# Load gazes
looks <- c(
  "./data-raw/mp_timepoint1_looks.csv.gz", 
  "./data-raw/mp_timepoint2_looks.csv.gz", 
  "./data-raw/mp_timepoint3_looks.csv.gz") %>% 
  lapply(readr::read_csv) %>% 
  bind_rows()

check <- left_join(looks, trials) %>% 
  distinct(Bias_ImageAOI, StimType, ImageL, GazeByAOI, GazeByImageAOI)


looks <- left_join(looks, trials) %>%
  filter(Version == "Standard") %>% 
  mutate(Age = convert_study_to_age(Study))

# Keep only frames from -500 to 2000 plus or minus any frames to make the 
# number of frames divisible by 3 (for binning)
times_to_keep <- looks %>% 
  distinct(Time) %>% 
  trim_to_bin_width(
    bin_width = 3, time_var = Time, key_time = 0, 
    key_position = 2, min_time = -500, max_time = 2000) %>% 
  pull(Time) %>% 
  range()

# Aggregate looks to target
resp_def <- create_response_def(
  primary = "Target",
  others = "Distractor",
  elsewhere = "tracked",
  missing = NA)

raw_data <- looks %>% 
  filter(between(Time, times_to_keep[1], times_to_keep[2])) %>% 
  aggregate_looks(
    resp_def, Study + Age + ResearchID + Condition + Time ~ GazeByImageAOI)
```


Data collection for this experiment occurred during the same
longitudinal study as the familiar word recognition experiment of Aim 1.
Thus, this experiment used the same participants, the same stimulus
preparation and style, and the same general experimental procedure as
those reported in detail in [Chapter \@ref(aim1-method)](#aim1-method).


Mispronunciation task
------------------------------------------------------------------------

This experiment is an adaptation of the mispronunciation detection task
by @WhiteMorgan2008 and @MPPaper. In the experiment, two images are
presented onscreen—a familiar object and an unfamiliar object—and the
child hears a prompt to view one of the images. In the *correct
pronunciation* (or *real word*) and *mispronunciation* conditions, the
child hears either the familiar word (e.g., *soup*) or a one-feature
mispronunciation of the first consonant of the target word (*shoup*).
These conditions are designed to test whether children map
mispronunciations to novel words. To encourage fast referent selection,
there were also trials in a *nonword* condition where the label was an
unambiguous novel word (e.g., *cheem* presented with images of a bed and
a novel-looking pastry mixer). Each nonword was constructed to match the
phonotactic probability of one of the mispronunciations. 
Figure \@ref(fig:`r sample-mp-screen`) shows the screens used in two
trials. 
Importantly, within a block of trials, the child never hears
both the correct and mispronounced forms of the same word. A child
hearing "duck" then a few trials later hearing "guck" would provide a
basis of comparison so that the child can decide that "guck" is probably
not "duck"---the design used here avoided this situation.

(ref:sample-mp-screen-cap) Example displays for a trial in which *duck*
is mispronounced as "guck" (*left*) and a trial in which the nonword
"cheem" is presented (*right*).

```{r sample-mp-screen, fig.cap = "(ref:sample-mp-screen-cap)", echo = FALSE, out.width = "100%"}
library(magick)
dir_screens <- "./misc/mp-screens/TimePoint1/actual/"

i1 <- dir_screens %>% 
  file.path("Block1_03_MP_bull2_duck2_ImageR_guk.png") %>% 
  image_read() %>% 
  image_scale("x400")

i2 <- dir_screens %>% 
  file.path("Block1_14_nonsense_bed2_mixer2_ImageL_Cim.png") %>% 
  image_read() %>% 
  image_scale("x400")

image_append(c(i1, i2))
```

<!-- (Maybe explain why the nonword condition give children the license to -->
<!-- interpret a mispronunciation as a nonword.) -->
<!-- So, children sometimes heard correct productions that can be mapped to familiar -->
<!-- objects and sometimes heard flagrant nonwords that could be quickly associated -->
<!-- to unfamilar objects. Both of these behaviors are encouraged by the experiment. -->

In a block of the experiment, there were 12 trials each from the correct
production, mispronunciation, and nonword conditions, and children
received two blocks of the task. A complete list of the items used in
the experiment over the three years of the study is included in
[Appendix \@ref(mp-experiment-items)](#mp-experiment-items).


Visual stimuli
------------------------------------------------------------------------

The images used in the experiment consisted of color photographs on gray
backgrounds. As in the familiar word recognition, these images were
piloted in two preschool classrooms. Piloting confirmed that children
consistently used the same label for familiar objects. For the novel
objects, the children reported to not know a word for the object, or if
they did name the object, they did not consistently use the same word
for an object.


Data screening {#aim2-screening}
------------------------------------------------------------------------

```{r aim2-screening-rules}
# We use the following options for data screening.
rules <- list(
  screening_window = c(0, 2000),
  missing_data_limit = .5,
  min_trials_per_block = 18,
  min_trials_per_condition = 6)

opts_model <- list(
  bin_width = 3,
  start_time = 250,
  end_time = 1500
)

opts_model$bin_length <- round(opts_model$bin_width * 16.67, -1)
opts_model
```

```{r apply-data-screening-rules, include = FALSE}
# The first and last times are offset by 20 ms because we downsample our data
# into 50-ms (3-frame) bins. The frames at -16.65, 0, +16.65 ms make up the
# frames in the bin at time 0, so we need to capture the frame at -16.65. The
# same reasoning applies to the frame at 2016.65.
screening_times <- looks %>% 
  distinct(Time) %>% 
  trim_to_bin_width(
    bin_width = 3, key_time = 0, 
    key_position = 2, time_var = Time, 
    min_time = rules$screening_window[1] - 20, 
    max_time = rules$screening_window[2] + 20) %>% 
  pull(Time) %>% 
  range()
screening_times

missing_data_by_trial <- looks %>% 
  filter(screening_times[1] <= Time, Time <= screening_times[2]) %>% 
  aggregate_looks(
    resp_def, 
    Study + Version + ResearchID + Basename + 
      Condition + TrialNo ~ GazeByImageAOI) %>% 
  mutate(BadTrial = rules$missing_data_limit <= PropNA)

bad_trial_counts <- missing_data_by_trial %>% 
  count(Study, ResearchID, Basename, BadTrial) %>% 
  tidyr::spread(BadTrial, n) %>% 
  rename(n_bad = `TRUE`, n_good = `FALSE`) %>% 
  # Replace NAs with 0, in case there were 0 good trials in a block or 
  # 0 bad trials in a block
  mutate(
    n_bad = coalesce(n_bad, 0L),
    n_good = coalesce(n_good, 0L),
    trials = n_good + n_bad,
    prop_bad = round(n_bad / trials, 2)) 

# Min trials per block cleaning
blocks_to_drop <- bad_trial_counts %>% 
  filter(n_good < rules$min_trials_per_block)
blocks_to_drop

# Identifying unreliable trials
leftover_bad_trials <- missing_data_by_trial %>% 
  anti_join(blocks_to_drop, by = c("Study", "ResearchID", "Basename")) %>% 
  filter(rules$missing_data_limit <= PropNA)

# Count reliable trials per Study x ResearchID x Condition
good_trials_per_condition <- missing_data_by_trial %>% 
  anti_join(blocks_to_drop, by = c("Study", "Basename")) %>% 
  anti_join(leftover_bad_trials, by = c("Study", "Basename", "TrialNo")) %>% 
  count(Study, ResearchID, Condition, BadTrial) %>% 
  tidyr::spread(BadTrial, n) %>% 
  rename(n_good = `FALSE`) %>% 
  mutate(n_good = coalesce(n_good, 0L)) %>% 
  # Make sure that there are three Condition rows for each Study x ResearchID
  tidyr::complete(
    tidyr::nesting(Study, ResearchID), Condition, 
    fill = list(n_good = 0))

too_few_trials_per_condition <- good_trials_per_condition %>% 
  filter(n_good < rules$min_trials_per_condition) %>% 
  distinct(Study, ResearchID)

clean_looks <- looks %>% 
  anti_join(blocks_to_drop, by = c("Study", "Basename")) %>% 
  anti_join(leftover_bad_trials, by = c("Study", "Basename", "TrialNo")) %>% 
  anti_join(too_few_trials_per_condition, by = c("Study", "ResearchID"))

data <- clean_looks %>% 
  filter(between(Time, times_to_keep[1], times_to_keep[2])) %>% 
  readr::write_csv("./data/aim2-screened.csv.gz") 
```

```{r bad-version-counts, message = FALSE}
ids_by_version <- readr::read_csv("./data-raw/mp_timepoint1_blocks.csv") %>% 
  select(ResearchID, Basename, Version) %>% 
  split(.$Version) %>% 
  lapply(getElement, "ResearchID")

n_lost_in_bad_version <- ids_by_version$`Early attention getter` %>% 
  setdiff(ids_by_version$Standard) %>% 
  length()
```

Table \@ref(tab:mp-screening-counts) shows the numbers of participants
and trials excluded during each of year of the study due to unreliable
data. There were more children in the second year than the first year
due to a timing error in the initial version of this experiment, leading
to the exclusion of `r n_lost_in_bad_version` participants from the
first year.

```{r mp-screening-counts, echo = FALSE}
screening_results <- list(Screened = clean_looks, Raw = looks) %>% 
  bind_rows(.id = "Dataset") %>% 
  distinct(Dataset, Age, ResearchID, Basename, TrialID) %>% 
  group_by(Dataset, Age) %>% 
  summarise(
    `Children` = n_distinct(ResearchID),
    `Blocks` = n_distinct(Basename),
    `Trials` = n_distinct(TrialID)) %>% 
  ungroup()

# Add an extra set of rows computing difference
screening_results2 <- screening_results %>% 
  tidyr::gather("Stat", "Value", -Age, -Dataset) %>% 
  tidyr::spread(Dataset, Value) %>% 
  mutate(`Raw &minus; Screened` = Raw - Screened) %>% 
  tidyr::gather(Dataset, "Count", -Age, -Stat) %>% 
  tidyr::spread(Stat, Count) %>% 
  mutate(
    Dataset = Dataset %>% 
      factor(c("Raw", "Screened", "Raw &minus; Screened"))) %>% 
  select(Dataset, Age, `Children`, `Blocks`, `Trials`) %>% 
  arrange(Dataset, Age) %>% 
  rename(Study = Age) %>% 
  mutate(Dataset = Dataset %>% as.character() %>% replace_if_same_as_last(""))

knitr::kable(
  screening_results2,
  caption = "Eyetracking data counts before and after data screening.", 
  booktabs = TRUE)
```

After mapping the gaze coordinates onto the onscreen images, I performed
data screening following the same set of steps as in
[Chapter \@ref(aim1-method)](#aim1-method). To make data quality
judgments, I only considered the window from
`r rules$screening_window[1]` to `r rules$screening_window[2]` ms after
noun onset. Next, I identified a trial as *unreliable* if at least
`r rules$missing_data_limit * 100`% of the looks were missing during the
time window, and I excluded an entire block of trials if it had fewer
than `r rules$min_trials_per_block` reliable trials. As an additional
criterion, I excluded participants who failed to provide at least
`r rules$min_trials_per_condition` reliable trials per experimental
condition.

### Classifying trials based on initial fixation location

During preliminary visualization of the study-level growth curves, I
observed an increasing preference for the unfamiliar image for the
nonword condition. The growth curves showed a typical pattern of a
baseline at noun onset followed by a quick change in height as the word
unfolded. For the nonword condition, this baseline level moved further
from .5 (chance with two images) with each year of the study: Children
became more likely to fixate on the novel object at the start of these
trials. 

Because this was a two-image task, I was able to account for the
location of the child's gaze at the onset of the target noun. For each
trial, I counted the number of looks to the familiar object and the
unfamiliar object during the first 250 ms after target noun onset
(specifically, 0 ≤ *time* \< 250 ms). If the majority of the looks
landed on the familiar object, then the trial was a familiar-initial
trial. An analogous rule labeled trials as unfamiliar-initial trials.
Ties were broken by favoring the earlier fixated image. For example, a
tie might be a trial with 7 frames of looking to the unfamiliar image,
followed by 1 frame between the two images, followed by 7 frames to the
familiar image. In this case, the unfamilar image was viewed first, so
the trial is classified as unfamiliar-initial. If there were no looks to
either image during that window, the trial was not classified for either
image and it was excluded.

Table \@ref(mp-initial-looks-counts) shows the number and percentage for
the trial classification. The table shows that about 5% of trials were
excluded because the child looked to neither image during the
first 250 ms of the noun onset. The table also shows how the percentage
of unfamiliar-initial trials increased with each year of the study.
Accounting for this trend was the rationale for classifying trials based
on the initial fixation location.

```{r mp-initial-looks-counts, echo = FALSE}
l_counts <- data %>% 
  distinct(Study, Condition, ResearchID, TrialID, Bias_Fam) %>% 
  count(ResearchID, Study, Condition, Bias_Fam) %>% 
  group_by(Study, Condition) %>% 
  filter(!is.na(Bias_Fam)) %>% 
  ungroup() %>% 
  tidyr::complete(
    tidyr::nesting(ResearchID, Study, Condition), 
    Bias_Fam, 
    fill = list(n = 0)) 

counts <- l_counts %>% 
  tidyr::spread(Bias_Fam, n)

# library(lme4)
# test <- glmer(
#   cbind(Familiar, Unfamiliar) ~ 
#     Study * Condition + 
#     (1 | ResearchID:Condition),
#   counts, 
#   family = binomial)
# summary(test)

# ggplot(l_counts) + 
#   aes(x = Study, y = n, color = Bias_Fam) + 
#   geom_point(position = position_jitter(width = .2), alpha = .1) + 
#   facet_wrap("Condition") + 
#   stat_summary()



fmt_n_with_percent <- function(xs, totals, digits = 3) {
  proportion <- round((xs / totals), digits)
  percent <- scales::percent(proportion)
  sprintf("%s (%s)", xs, percent)
}


data %>% 
  mutate(
    Study = convert_study_to_age(Study),
    Condition = factor(
      Condition, 
      levels = c("real", "nonsense", "MP"), 
      labels = c("Real word", "Nonword", "Mispronunciation"))) %>% 
  distinct(Study, Condition, ResearchID, TrialID, Bias_Fam) %>% 
  count(Study, Condition, Bias_Fam) %>% 
  tidyr::spread(Bias_Fam, n) %>% 
  mutate(
    total = (Familiar + Unfamiliar + `<NA>`),
    Familiar = fmt_n_with_percent(Familiar, total),
    Unfamiliar = fmt_n_with_percent(Unfamiliar, total),
    `<NA>` = fmt_n_with_percent(`<NA>`, total),
    Study = Study %>% 
      as.character() %>% 
      printy::str_replace_same_as_previous(" ")) %>% 
  select(-total) %>% 
    # PropNA = PropNA %>% round(3) %>% scales::percent()) %>%
  setNames(c("Study", "Condition", "Familiar initial",
             "Unfamiliar initial", "Neither/excluded")) %>%
  knitr::kable(
    align = c("l", "l", "r", "r", "r"), 
    booktabs = TRUE,
    caption = "Number of trials classified based on initial fixation location.")

# ggplot(counts) +
#   aes(x = Study, y = Familiar / (Familiar + Unfamiliar), color = Condition) +
#   geom_point(position = position_jitter(width = .2), alpha = .1) +
#   geom_line(
#     aes(group = ResearchID),
#     data = . %>% sample_n_of(10, ResearchID)) +
#   facet_wrap("Condition") +
#   stat_summary()
```




```{r, eval = FALSE}
# Just some scratch paper to doublecheck my bias calculation math
x <- data %>% 
  tjmisc::sample_n_of(1, TrialID) %>% 
  filter(0 <= Time, Time <= 245)
x 

f <- function(x) {
  subtrial <- x
  subtrial$WindowDur <- nrow(subtrial) * lookr::lwl_constants$ms_per_frame
  subtrial$LastPossible <- max(subtrial$Time)
  
  plyr::ddply(
    subtrial, "GazeByImageAOI", plyr::summarize, 
    Frames = length(Time), 
    Weight = Frames * unique(WindowDur), 
    FirstLook = min(Time), 
    Earliness = unique(LastPossible) - FirstLook, 
    Bias = Weight + Earliness)
  
  list(
    Frames = Frames, Weight = Weight, FirstLook = FirstLook, 
    LastPossible = LastPossible,
    Earliness = Earliness, 
    Bias = Bias)
}
```


```{r, eval = FALSE}
counts <- data %>% 
  distinct(Study, Condition, ResearchID, TrialID, Bias_Fam) %>% 
  count(Study, ResearchID, Condition, Bias_Fam)

ggplot(counts) + 
  aes(x = n, fill = Condition) + 
  geom_histogram(binwidth = 1, boundary = 1) + 
  facet_grid(Bias_Fam ~ Study) + 
  geom_vline(xintercept = 2)
```



## Model prepation

To prepare the data for modeling, I downsampled the data
into `r opts_model$bin_length`-ms (`r opts_model$bin_width`-frame) bins.
For the real word and nonword trials, I modeled looks from XX to XX ms.
For the mispronunciation trials, I modeled looks from XX to XX ms.
Lastly, I aggregated looks by child, study, condition, initial fixation
location, and time, and I created orthogonal polynomials to use as time
features for the model.



```{r, echo = FALSE, message = FALSE, warnings = FALSE}
m_data <- data %>% 
  select(Study, ResearchID, Condition, Bias_Fam, TrialID:GazeByImageAOI) %>% 
  assign_bins(bin_width = opts_model$bin_width, Time, TrialID)

# Compute time at center of each bin
bin_times <- m_data %>% 
  distinct(Time, .bin) %>% 
  group_by(.bin) %>% 
  mutate(BinTime = round(median(Time), -1)) %>% 
  ungroup()

# Attach bin times
binned <- m_data %>% 
  left_join(bin_times, by = c("Time", ".bin")) %>% 
  ungroup() %>% 
  select(-Time) %>% 
  rename(Time = BinTime) 

d <- binned %>% 
  aggregate_looks(
    resp_def, Study + ResearchID + Condition + Bias_Fam + Time ~ GazeByImageAOI)

d_m <- d %>% 
  filter(0 <= Time, Time <= opts_model$end_time) %>% 
  polypoly::poly_add_columns(
    Time, degree = 3, scale_width = 1, prefix = "ot")

readr::write_csv(d_m, "./data/aim2-model-ready.csv.gz")
```

Figure \@ref(fig:aim2-spaghetti) depicts the results from the real word
and nonword conditions following these data screening and preparation
steps. The lines start around .5 which is chance performance on this
two-image task. The lines rise as the word unfolds and peak and plateau
around 1400 ms.

(ref:aim2-spaghetti) Empirical word recognition growth curves from each
year of the study. Each line represents an individual child's proportion
of looks to the target image over time. The heavy lines are the averages
of the lines for each year.

```{r aim2-spaghetti, fig.cap = "(ref:aim2-spaghetti)", echo = FALSE, message = FALSE, out.width = "100%", fig.height = 6, fig.width = 5}
d_plot <- d %>% 
  filter(250 <= Time, Time <= opts_model$end_time) %>%
  mutate(
    Study = convert_study_to_age(Study),
    Condition = factor(
      Condition, 
      levels = c("real", "nonsense", "MP"), 
      labels = c("Real word", "Nonword", "Mispronunciation"))) %>% 
  filter(!is.na(Bias_Fam)) 

rw_age_labs <- tibble::tribble(
  ~Bias_Fam, ~Study, ~Time, ~Prop,
  "Familiar", "Age 3", 250, .08,
  "Familiar", "Age 4", 250, .08,
  "Familiar", "Age 5", 250, .08
)

p1 <- d_plot %>%
  filter(Condition == "Real word") %>% 
  ggplot() + 
    aes(x = Time, y = Prop) + 
    geom_hline(yintercept = .5, color = "white", size = 2) + 
    geom_line(
      aes(group = ResearchID), 
      color = constants$col_off_black, 
      alpha = .10, 
      position = position_jitter(height = .015)) +
    stat_summary(
      aes(color = Study,  linetype = Bias_Fam), 
      fun.y = mean, geom = "line", size = 1.25) +
    geom_text(
      aes(color = Study, label = Study),
      data = rw_age_labs,
      size = 4.5, 
      hjust = 0) +
    facet_grid(Bias_Fam ~ Study) +
    labs(
      x = constants$x_time,
      y = NULL,
      caption = "One line per child. Heavy line: Average of lines.") + 
    guides(color = FALSE, linetype = FALSE) + 
    theme(
      strip.background.y = element_blank(), 
      strip.text.y = element_blank(),
      strip.background.x = element_blank(), 
      strip.text.x = element_blank())
p1

p2 <- d_plot %>%
  filter(Condition == "Real word") %>% 
  ggplot() + 
    aes(x = Time, y = Prop) + 
    geom_hline(yintercept = .5, color = "white", size = 2) + 
    stat_summary(
      aes(color = Study, linetype = Bias_Fam),
      fun.y = mean, geom = "line", size = 1.25) + 
    labs(
      x = constants$x_time,
      y = constants$y_prop_fam, 
      caption = " ") + 
    guides(color = FALSE, linetype = FALSE) + 
    annotate(
      geom = "text", x = 350, y = .97, hjust = 0, color = "grey20",
      label = "trials starting on familiar image", size = 3.25) +
    annotate(
      "text", x = 350, y = .05, hjust = 0, color = "grey20",
      label = "trials starting on unfamiliar image", size = 3.25)
p2

```

```{r aim2-real-word-spaghetti, fig.width = 7, fig.height = 5, out.width = "100%"}
library(patchwork)
rw_super <- p2 + p1 + 
  plot_layout(widths = c(4, 6)) + 
  plot_annotation("\"find the dog\": Child hears a real word")

p1_mp <- p1 %+% (d_plot %>% filter(Condition == "Mispronunciation"))
p2_mp <- p2 %+% (d_plot %>% filter(Condition == "Mispronunciation"))
mp_super <- p2_mp + p1_mp + 
  plot_layout(widths = c(4, 6)) + 
  plot_annotation("\"find the tog\": Child hears a mispronunciation")

p1_nw <- p1 %+% (d_plot %>% filter(Condition == "Nonword"))
p2_nw <- p2 %+% (d_plot %>% filter(Condition == "Nonword"))
nw_super <- p2_nw + p1_nw + 
  plot_layout(widths = c(4, 6)) + 
  plot_annotation("\"find the vafe\": Child hears a nonword")

rw_super
```

```{r aim2-nonword-spaghetti, fig.width = 7, fig.height = 5, out.width = "100%"}
nw_super
```

```{r aim2-mispronunciation-spaghetti, fig.width = 7, fig.height = 5, out.width = "100%"}
mp_super
```


```{r}
d2 <- binned %>% 
  left_join(trials) %>% 
  aggregate_looks(
    resp_def, Study + ResearchID + Bias_Fam + Condition + Time ~ GazeByImageAOI)

d2_plot <- d2 %>% 
  mutate(
    Study = convert_study_to_age(Study),
    Condition = factor(
      Condition, 
      levels = c("real", "nonsense", "MP"), 
      labels = c("Real word", "Nonword", "Mispronunciation"))) %>% 
  filter(0 <= Time)

d2_plot %>% 
  filter(Condition == "Real word") %>%
  ggplot() + 
    aes(x = Time, y = Prop) + 
    geom_hline(yintercept = .5, color = "white", size = 2) + 
    stat_summary(aes(color = Study, linetype = Bias_Fam), 
                 fun.y = mean, geom = "line", size = 1.25) +
    # facet_grid(Condition ~ .) + 
    labs(x = constants$x_time,
         y = constants$y_prop_fam) 

last_plot() %+% filter(d2_plot, Condition == "Mispronunciation")
last_plot() %+% filter(d2_plot, Condition == "Nonword")


d3 <- binned %>% 
  left_join(trials) %>% 
  aggregate_looks(
    resp_def, Study + ResearchID + Bias_Fam + Condition + Time ~ GazeByImageAOI)

d3_plot <- d3 %>% 
  mutate(
    Study = convert_study_to_age(Study),
    Condition = factor(
      Condition, 
      levels = c("real", "nonsense", "MP"), 
      labels = c("Real word", "Nonword", "Mispronunciation"))) %>% 
  filter(0 <= Time)



d3_plot %>% 
  mutate(PropTarget = ifelse(Condition == "Nonword", 1 - Prop, Prop)) %>%
  filter(Condition != "Mispronunciation") %>% 
  ggplot() + 
    aes(x = Time, y = PropTarget) + 
    geom_hline(yintercept = .5, color = "white", size = 2) + 
    stat_summary(aes(color = Condition, linetype = Bias_Fam), 
                 fun.y = mean, geom = "line", size = 1.25) +
    facet_grid(. ~ Study) +
    labs(x = constants$x_time,
         y = constants$y_prop_target) 

```

