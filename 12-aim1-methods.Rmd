Method {#aim1-method}
=======================================================================

```{r include = FALSE}
knitr::read_chunk("./helpers.R")
if (interactive()) source("./helpers.R")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r helpers, message = FALSE, warning = FALSE}
```

```{r include = FALSE}
knitr::opts_chunk$set(cache = TRUE)
```



Participants {#aim1-participants}
-----------------------------------------------------------------------

The data were collected as part of a three-year longitudinal
study.[^1] For convenience, I refer to the three years as Age 3,
Age 4, and Age 5, although the participants on average were three
months younger than those nominal ages. In particular, the participants
were 28–39 months-old at Age 3, 39–52 at Age 4, and 51–65 at Age 5.
Approximately, 180 children participated at Age 3, 170 at Age 4, and 160
at Age 5. Of these children, approximately 20 were identified by their
parents as late talkers. Prospective families were interviewed over
telephone before participating in the study. Children were not scheduled
for testing if a parent reported language problems, vision problems,
developmental delays, or an individualized education program for the
child. Recruitment and data collection occurred at two Learning to Talk
lab sites—one at the University of Wisconsin–Madison and the other at
the University of Minnesota.

[^1]: [Appendix \@ref(related-work)](#related-work) describes how this
      dissertation relates to other work from our lab.

Table \@ref(tab:participant-info) summarizes the cohort of children in
each year of testing. The numbers and summary statistics here are
general, describing children who participated at each year, but whose
data may have been excluded from the analyses. Some potential reasons
for exclusion include: excessive missing data during eyetracking,
experiment or technology error, developmental concerns not identified
until later in study, or a failed hearing screening. Final sample sizes
will depend on the measures needed for an analysis and the results from
data screening checks.

```{r participant-info}
path <- file.path(".", "data-raw", "scores_for_participant_summary.csv")
df_all_three <- readr::read_csv(path)

# Don't let NAs break common summary functions
narmly <- function(f) function(...) f(..., na.rm = TRUE)
narm_mean <- narmly(mean)
narm_sd <- narmly(sd)
narm_range <- narmly(range)
narm_sum <- narmly(sum)

# xs -> "Mean (SD) [Range]"
mean_sd_range <- function(xs) {
  mean <- narm_mean(xs)
  sd <- narm_sd(xs)
  range <- narm_range(xs)
  sprintf("%.0f (%.0f) [%.0f–%.0f]", mean, sd, range[1], range[2])
}

# xs -> "Mean (SD)"
mean_sd <- function(xs) {
  mean <- narm_mean(xs)
  sd <- narm_sd(xs)
  sprintf("%.0f (%.0f)", mean, sd)
}

# c(x1, x2) -> "x1, x2"
paste_comma <- function(...) paste(..., sep = ", ")

df <- df_all_three %>%
  group_by(Study) %>%
  summarise(
    N = n(),
    BoyGirl = paste_comma(sum(Male), sum(Female)),
    MAEAAE = paste_comma(sum(MAE), sum(AAE)),
    Age = mean_sd_range(Age),
    LateTalker = sum(LateTalker),
    Medu_Low = narm_sum(Maternal_Education_LMH == "Low"),
    Medu_Mid = narm_sum(Maternal_Education_LMH == "Mid"),
    Medu_High = narm_sum(Maternal_Education_LMH == "High"),
    Medu_NA = narm_sum(is.na(Maternal_Education_LMH)),
    Medu_LMHN = paste_comma(Medu_Low, Medu_Mid, Medu_High),
    PPVT_Standard = mean_sd(PPVT_Standard),
    EVT_Standard = mean_sd(EVT_Standard),
    GFTA_Standard = mean_sd(GFTA_Standard)) %>%
  select(Study:LateTalker, Medu_LMHN:GFTA_Standard)

# We didn't administer tests to every participant in these studies
not_collected <- tibble::tribble(
  ~Study, ~Key,
  "TimePoint2", "GFTA_Standard",
  "TimePoint3", "PPVT_Standard"
)

# Plan for ordering and labeling values
row_labels <- tibble::tribble(
  ~Order, ~Key, ~Label,
  01, "N",             "N",
  02, "BoyGirl",       "Boys, Girls",
  03, "Medu_LMHN",     "Maternal ed.: Low, Middle, High",
  04, "MAEAAE",        "Dialect: MAE, AAE",
  05, "LateTalker",    "Parent-identified late talkers",
  06, "Age",           "Age (months): Mean (SD) [Range]",
  07, "EVT_Standard",  "EVT-2 standard: Mean (SD)",
  08, "PPVT_Standard", "PPVT-4 standard: Mean (SD)",
  09, "GFTA_Standard", "GFTA-2 standard: Mean (SD)"
)

df_summary <- df %>%
  tidyr::gather("Key", "Value", -Study) %>%
  anti_join(not_collected) %>%
  bind_rows(not_collected) %>%
  mutate(Value = ifelse(is.na(Value), "—", Value)) %>%
  tidyr::spread(Study, Value) %>%
  left_join(row_labels) %>%
  arrange(Order) %>%
  select(
    ` ` = Label, 
    `Year 1 (Age 3)` = TimePoint1, 
    `Year 2 (Age 4)` = TimePoint2,
    `Year 3 (Age 5)` = TimePoint3)

if (knitr:::is_latex_output()) { 
  caption <- glue::glue(
    "Participant characteristics. Education levels: \\textit{Low}: less than 
     high school, or high school; \\textit{Middle}: trade school, technical or 
     associates degree, some college, or college degree; and \\textit{High}: 
     graduate degree. Dialects: \\textit{MAE}: Mainstream American English; 
     \\textit{AAE}: African-American English.", 
    .open = "<<", .close = ">>")
} else {
  caption <- glue::glue(
    "Participant characteristics. Education levels: *Low*: less than high 
     school, or high school; *Middle*: trade school, technical or associates 
     degree, some college, or college degree; and *High*: graduate degree.
     Dialects: *MAE*: Mainstream American English; *AAE*: African-American 
     English.")
}

df_summary %>% 
  knitr::kable(
    booktabs = TRUE,
    caption = caption,
    caption.short = "Participant characteristics."
  )
```


Visual World Paradigm {#aim1-procedure}
-----------------------------------------------------------------------

This experiment used a version of the Visual World Paradigm for word
recognition experiments [@RWLPaper]. In eyetracking studies with
toddlers, two familiar images are usually presented: a target and a
distractor. This experiment is a four-image eyetracking task that was
designed to provide a more demanding word recognition task for
preschoolers. In this procedure, four familiar images are presented
onscreen followed by a prompt to view one of the images (e.g., *find the
bell!*). The four images include the target word (e.g., *bell*), a
semantically related word (*drum*), a phonologically similar word
(*bee*), and an unrelated word (*swing*).
Figure \@ref(fig:sample-vw-screen) shows an example of a trial's items.
This procedure measures a child’s real-time comprehension of words by
capturing how the child’s gaze location changes over time in response to
speech.

(ref:sample-vw-screen-cap2) Example display for the target *bell* with the semantic foil *drum*, the phonological foil *bee*, and the unrelated *swing*.

```{r sample-vw-screen, echo = FALSE, fig.cap = "(ref:sample-vw-screen-cap2)", out.width = "100%"}
knitr::include_graphics(
  "./misc/rwl-screens/TimePoint1/actual/Block2_17_swing2_bell2_bee2_drum2_UpperRightImage_bell.png",
  auto_pdf = TRUE)
```



Experiment administration
-----------------------------------------------------------------------

Children participating in the study were tested over two lab visits
(on different dates). The first portion of each visit involved
“watching movies”—that is, performing two blocks of eyetracking
experiments. A play break or hearing screening occurred between the two
eyetracking blocks, depending on the visit.

Each eyetracking experiment was administered as a block of trials (24
for this experiment and 36 for a two-image task---see [Chapter \@ref(aim2-method)](#aim2-method)).
Children received two different blocks of each experiment. The blocks
for an experiment differed in trial ordering and other features.
Experiment order and block selection were counterbalanced over children
and visits. For example, a child might have received Exp. 1 Block A and
Exp. 2 Block B on Visit 1 and next received Exp. 2 Block A and Exp. 1
Block B on Visit 2. The purpose of this presentation was to control
possible ordering effects where a particular experiment or block
benefited from consistently occurring first or second.

Experiments were administered using E-Prime 2.0 and a Tobii T60XL
eyetracker which recorded gaze location at a rate of 60 Hz. The
experiments were conducted by two examiners, one “behind the scenes” who
controlled the computer running the experiment and another “onstage” who
guided the child through the experiment. At the beginning of each block,
the child was positioned so the child’s eyes were approximately 60 cm
from the screen. The examiners calibrated the eyetracker to the child’s
eyes using a five-point calibration procedure (center of screen and
centers of four screen quadrants). The examiners repeated this
calibration procedure if one of the five calibration points for one of
the eyes did not calibrate successfully. During the experiment, the
behind-the-scenes examiner monitored the child’s distance from the
screen and whether the eyetracker was capturing the child’s gaze. The
onstage examiner coached the child to stay fixated on the screen and
repositioned the child as needed to ensure the child’s eyes were being
tracked. Every six or seven trials in a block of an experiment, the
experiment briefly paused with a reinforcing animation or activity.
During these breaks, the onstage examiner could reposition the child if
necessary before resuming the experiment.

We used a gaze-contingent stimulus presentation. First, the images
appeared in silence on screen for 2 s as a familiarization period. The
experiment then checked whether the child's gaze was being recorded. If
the experiment could continuously track the child's gaze for 300 ms, the
child's gaze was verified and the trial continued. If the experiment
could not verify the gaze after 10 s, the trial continued. This
procedure guaranteed that for most trials, the child was looking to the
display before presenting the carrier phrase and that the experiment was
ready to record the child's response to the carrier. During year 1
(age 3) and year 2 (age 4), an attention-getter (e.g., *check it
out!*) played 1 s following the end of the target noun. These
reinforcers were dropped in year 3 (age 5) to streamline the experiment
for older listeners.



Stimuli
-----------------------------------------------------------------------

The four images on each trial consisted of a target noun, a phonological
foil, a semantic foil, and an unrelated word. The phonological
competitors shared a syllable onset (e.g., *flag*–*fly*, *bell*–*bee*),
shared an initial consonant (*bread*–*bear*, *swing*–*spoon*), had a
similar phonetically similar consonant onset (*kite*–*gift*), or shared
a syllable rime (*van*–*pan*). The semantic competitors included words
from the same category (e.g., *shirt*–*dress*, *horse*–*bear*), words
that were perceptually similar (*sword*–*pen*, *flag*–*kite*), and words
with less obvious relationships (*van*–*horse*, *swan*-*bee*). A
complete list of the items used in the experiment in 
[Appendix \@ref(vw-experiment-items)](#vw-experiment-items).

The stimuli were recorded in both Mainstream American English (MAE) and
African American English (AAE), so that the experiment could accommodate
the child's home dialect. Prior to the lab visit, we made a preliminary
guess about the child's home dialect, based on the recruitment channel,
address, among other factors. If we expected the dialect to be AAE, then
the lab visit was led by an examiner who natively spoke AAE and could
fluently dialect-shift between AAE and MAE. At the beginning of the lab
visit, the examiner listened to the interactions between the child and
caregiver in order to confirm the child's home dialect. Prompts to view
the target image of a trial (e.g., *find the girl*) used the carrier
phrases “find the” and “see the”. These carriers were recorded in the
frame “find/see the egg” and cross-spliced with the target nouns to
minimize coarticulatory cues on the determiner “the”. The stimuli were
re-recorded after the first year of the study with the same speakers so
that the average duration of the two dialect versions were more similar.

The images used in the experiment consisted of color photographs on gray
backgrounds. These images were piloted with 30 children from two
preschool classrooms to ensure that children consistently used the same
label for familiar objects. The two preschool classrooms differed in
their students' SES demographics: One classroom (13 piloting students)
was part of a university research center which predominantly serves
higher-SES families, and the other classroom (17 piloting students) was
part of Head Start center which predominantly serves lower-SES families.
The images were tested by presenting four images (a target, a
phonological foil, a semantic foil and an unrelated word) and having the
student point to the named image. The pictures had to be recognized by
at least 80% of students in each classroom.

Data screening
-----------------------------------------------------------------------

```{r prep-raw-data, message = FALSE, warnings = FALSE}
looks1 <- readr::read_csv("./data-raw/rwl_timepoint1_looks.csv.gz")
looks2 <- readr::read_csv("./data-raw/rwl_timepoint2_looks.csv.gz")
looks3 <- readr::read_csv("./data-raw/rwl_timepoint3_looks.csv.gz")
looks <- bind_rows(looks1, looks2, looks3) %>%
  filter(Version == "Standard")

resp_def <- create_response_def(
  primary = "Target",
  others = c("PhonologicalFoil", "SemanticFoil", "Unrelated"),
  elsewhere = "tracked",
  missing = NA)

# Keep only frames from -500 to 2000 plus or minus any frames to make the
# number of frames divisible by 3 (for binning)
times_to_keep <- looks %>%
  distinct(Time) %>%
  trim_to_bin_width(
    bin_width = 3, time_var = Time, key_time = 0, key_position = 2,
    min_time = -500, max_time = 2000) %>%
  pull(Time) %>%
  range()

raw_data <- looks %>%
  filter(between(Time, times_to_keep[1], times_to_keep[2])) %>%
  aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI)
```

```{r aim1-screening-rules, include = FALSE}
# We use the following options for data screening.
rules <- list(
  screening_window = c(0, 2000),
  missing_data_limit = .5,
  min_trials = 12
)
```

To process the eyetracking data, I first mapped gaze *x*-*y* coordinates
onto the onscreen images. I next performed *deblinking*. I interpolated
short runs of missing gaze data (up to 150 ms) if the same image was
fixated before and after the missing data run. Put differently, I
classified a window of missing data as a blink if the window was brief
and the gaze remained on the same image before and after the blink. I
interpolated missing data from blinks using the fixated image.

After mapping the gaze coordinates onto the onscreen images, I performed
data screening. I considered the time window
from `r rules$screening_window[1]` to `r rules$screening_window[2]` ms
after target noun onset. I identified a trial as *unreliable* if at
least `r rules$missing_data_limit * 100`% of the looks were missing
during the time window. I excluded an entire block of trials if it had
fewer than `r rules$min_trials` reliable trials. The rationale for
blockwise exclusion was that if the majority of trials were unreliable,
then there was probably a problem during the session, such as a
technical difficulty with the eyetracker or the child not complying with
the task. As a result, all of the trials would be of questionable quality.

```{r apply-data-screening-rules, echo = FALSE, results = 'hide'}
# The first and last times are offset by 20 ms because we downsample our data
# into 50-ms (3-frame) bins. The frames at -16.65, 0, +16.65 ms make up the
# frames in the bin at time 0, so we need to capture the frame at -16.65. The
# same reasoning applies to the frame at 2016.65.
screening_times <- looks %>% 
  distinct(Time) %>% 
  trim_to_bin_width(
    bin_width = 3, key_time = 0, 
    key_position = 2, time_var = Time, 
    min_time = rules$screening_window[1] - 20, 
    max_time = rules$screening_window[2] + 20) %>% 
  pull(Time) %>% 
  range()
screening_times

missing_data_by_trial <- looks %>% 
  filter(screening_times[1] <= Time, Time <= screening_times[2]) %>% 
  aggregate_looks(
    resp_def, 
    Study + Version + ResearchID + Basename + TrialNo ~ GazeByImageAOI) %>% 
  mutate(BadTrial = rules$missing_data_limit <= PropNA)

bad_trial_counts <- missing_data_by_trial %>% 
  count(Study, ResearchID, Basename, BadTrial) %>% 
  tidyr::spread(BadTrial, n) %>% 
  rename(n_bad = `TRUE`, n_good = `FALSE`) %>% 
  # Replace NAs with 0, in case there were 0 good trials in a block or 
  # 0 bad trials in a block
  mutate(
    n_bad = coalesce(n_bad, 0L),
    n_good = coalesce(n_good, 0L),
    trials = n_good + n_bad,
    prop_bad = round(n_bad / trials, 2)) 

blocks_to_drop <- bad_trial_counts %>% 
  filter(.5 <= prop_bad)
blocks_to_drop

leftover_bad_trials <- missing_data_by_trial %>% 
  anti_join(blocks_to_drop, by = c("Study", "ResearchID", "Basename")) %>% 
  filter(.5 <= PropNA)

clean_looks <- looks %>% 
  anti_join(blocks_to_drop, by = c("Study", "ResearchID", "Basename")) %>% 
  anti_join(leftover_bad_trials)

data <- clean_looks %>% 
  filter(between(Time, times_to_keep[1], times_to_keep[2])) %>% 
  readr::write_csv("./data/aim1-screened.csv.gz") 
```

```{r bad-version-counts, message = FALSE}
ids_by_version <- readr::read_csv("./data-raw/rwl_timepoint1_blocks.csv") %>% 
  select(ResearchID, Basename, Version) %>% 
  split(.$Version) %>% 
  lapply(getElement, "ResearchID")

n_lost_in_bad_version <- ids_by_version$`Early attention getter` %>% 
  setdiff(ids_by_version$Standard) %>% 
  length()
```

Table \@ref(tab:screening-counts) shows the numbers of participants and
trials at each year before and after data screening. There were more
children in the second year than the first due to a timing error in the
initial version of this experiment, leading to the exclusion of
`r n_lost_in_bad_version` participants from the first year.

```{r screening-counts, echo = FALSE}
screening_results <- list(Screened = clean_looks, Raw = looks) %>% 
  bind_rows(.id = "Dataset") %>% 
  distinct(Dataset, Study, ResearchID, Basename, TrialID) %>% 
  group_by(Dataset, Study) %>% 
  summarise(
    `N Children` = n_distinct(ResearchID),
    `N Blocks` = n_distinct(Basename),
    `N Trials` = n_distinct(TrialID)) %>% 
  ungroup()



# Add an extra set of rows computing difference
screening_results2 <- screening_results %>% 
  tidyr::gather("Stat", "Value", -Study, -Dataset) %>% 
  tidyr::spread(Dataset, Value) %>% 
  mutate(`Raw - Screened` = Raw - Screened) %>% 
  tidyr::gather(Dataset, "Count", -Study, -Stat) %>% 
  tidyr::spread(Stat, Count) %>% 
  mutate(
    Dataset = Dataset %>% 
      factor(c("Raw", "Screened", "Raw - Screened"))) %>% 
  select(Dataset, Study, `N Children`, `N Blocks`, `N Trials`) %>% 
  arrange(Dataset, Study) %>% 
  mutate(
    Dataset = Dataset %>% as.character() %>% replace_if_same_as_last(""),
    Study = convert_study_to_age(Study))

knitr::kable(
  screening_results2,
  caption = glue::glue(
    "Eyetracking data before and after data screening. For convenience, the \\
     number of exclusions is included as Raw − Screened."),
  linesep = c("", "", "\\addlinespace"),
  caption.short = "Eyetracking data before and after data screening.", 
  booktabs = TRUE)
```





Model preparation
-----------------------------------------------------------------------

```{r, include = FALSE}
opts_model <- list(
  bin_width = 3,
  start_time = 250,
  end_time = 1500
)
opts_model$bin_length <- round(opts_model$bin_width * 16.67, -1)
opts_model
```

To prepare the data for modeling, I downsampled the data into
`r opts_model$bin_length`-ms (`r opts_model$bin_width`-frame) bins,
reducing the eyetracker's effective sampling rate to
`r 60 / opts_model$bin_width` Hz. Eye movements have durations on the
order of 100 or 200 ms, so capturing data every 16.67 ms oversamples eye
movements and can introduce high-frequency noise into the signal.
Binning together data from neighboring frames can smooth out this noise.
I modeled the looks from `r opts_model$start_time`
to `r opts_model$end_time` ms. Lastly, I aggregated looks by child,
study and time, and created orthogonal polynomials to use as time
features for the model.

```{r, echo = FALSE, message = FALSE, warnings = FALSE}
m_data <- data %>% 
  select(Study, ResearchID, TrialID:GazeByImageAOI) %>% 
  assign_bins(bin_width = opts_model$bin_width, Time, TrialID)

# Compute time at center of each bin
bin_times <- m_data %>% 
  distinct(Time, .bin) %>% 
  group_by(.bin) %>% 
  mutate(BinTime = round(median(Time), -1)) %>% 
  ungroup()

# Attach bin times
binned <- m_data %>% 
  left_join(bin_times, by = c("Time", ".bin")) %>% 
  ungroup() %>% 
  select(-Time) %>% 
  rename(Time = BinTime) 

resp_def <- create_response_def(
  primary = "Target",
  others = c("PhonologicalFoil", "SemanticFoil", "Unrelated"),
  elsewhere = "tracked",
  missing = NA)
  
d <- binned %>% 
  aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI)

d_m <- d %>% 
  filter(opts_model$start_time <= Time, Time <= opts_model$end_time) %>% 
  polypoly::poly_add_columns(
    Time, degree = 3, scale_width = 1, prefix = "ot")

readr::write_csv(d_m, "./data/aim1-model-ready.csv.gz")
```

Figure \@ref(fig:aim1-spaghetti) depicts each child's proportion of looks
to the target image following the data screening and model preparation
steps. These are the observed or empirical growth curves; these are the
probabilities that will be modeled with growth curve analysis. The
lines start around .25 which is chance performance on four-alternative
forced choice task. The lines rise as the word unfolds, and they peak
and plateau around 1400 ms.

(ref:aim1-spaghetti) Empirical word recognition growth curves from each year of the study. Each line represents an individual child's proportion of looks to the target image over time. The heavy lines are the averages of the lines for each year.

(ref:aim1-spaghetti-scap) Empirical word recognition growth curves from each year of the study.

```{r aim1-spaghetti, fig.cap = "(ref:aim1-spaghetti)", echo = FALSE, message = FALSE, out.width = "100%", fig.height = 3, fig.width = 6, fig.scap="(ref:aim1-spaghetti-scap)"}
d_m %>% 
  mutate(Study = convert_study_to_age(Study)) %>% 
  ggplot() + 
    aes(x = Time, y = Prop) + 
    geom_hline(yintercept = .25, color = "white", size = 2) + 
    geom_line(aes(group = ResearchID), alpha = .16, color = constants$col_off_black) + 
    stat_summary(fun.y = mean, geom = "line", size = 1.25, color = constants$col_blue_highlight) +
    # stat_smooth() +
    facet_grid(. ~ Study) + 
    labs(x = constants$x_time,
         y = constants$y_prop_target,
         caption = "One line per child. Heavy line: Average of lines.")
```
