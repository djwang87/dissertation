

Look at raw data
===========================================================================

First, let's load in all the data and plot the data from each year.

```{r, message = FALSE, warnings = FALSE}
library(dplyr)
looks1 <- readr::read_csv("./data-raw/rwl_timepoint1_looks.csv.gz")
looks2 <- readr::read_csv("./data-raw/rwl_timepoint2_looks.csv.gz")
looks3 <- readr::read_csv("./data-raw/rwl_timepoint3_looks.csv.gz")
looks <- bind_rows(looks1, looks2, looks3) %>% 
  filter(Version == "Standard")

library(littlelisteners)
resp_def <- create_response_def(
  primary = "Target",
  others = c("PhonologicalFoil", "SemanticFoil", "Unrelated"),
  elsewhere = "tracked",
  missing = NA
)

raw_data <- looks %>% 
  filter(-505 <= Time, Time <= 2020) %>% 
  aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI)
```

Make some plots of overall averages.

```{r raw-aim1, out.width = "50%", fig.show='hold', fig.height=3, fig.width=3}
library(ggplot2)

ggplot(raw_data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_summary() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "Mean +/- SE") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))

ggplot(raw_data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_smooth() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "GAM Smooth") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))
```

The raw data plainly confirm hypothesis 1:

> Childrenâ€™s accuracy and efficiency of recognizing words will improve each year.

Look at a spaghetti plot...

```{r spaghetti-aim1, out.width = "100%", fig.height=2.5, fig.width=6}
ggplot(raw_data) + 
  aes(x = Time, y = Prop, group = ResearchID) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  geom_line(alpha = .15) +
  facet_grid(~ Study) +
  theme_grey(base_size = 9) +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "Lines: Individual participants") +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))
```



## Data cleaning

We use the following options for data screening.

```{r}
rules <- list(
  screening_window = c(0, 2000),
  missing_data_limit = .5,
  min_trials = 12
)
```

That is:

* Filter out bad trials. These have at least 
  `r rules$missing_data_limit * 100`% missing data between 
  `r rules$screening_window[1]` to `r rules$screening_window[2]`ms. 
* Filter out bad blocks. These have fewer than `r rules$min_trials` trials.

These are the default conventions four lab on these eyetracking experiments.

```{r}
# offset to catch frame before and after window
screen_from <- rules$screening_window[1] - 20
screen_to <- rules$screening_window[2] + 20

missing_data_by_trial <- looks %>% 
  filter(screen_from <= Time, Time <= screen_to) %>% 
  aggregate_looks(
    resp_def, 
    Study + Version + ResearchID + Basename + TrialNo ~ GazeByImageAOI) %>% 
  mutate(BadTrial = rules$missing_data_limit <= PropNA)

bad_trial_counts <- missing_data_by_trial %>% 
  count(Study, ResearchID, Basename, BadTrial) %>% 
  tidyr::spread(BadTrial, n) %>% 
  rename(n_bad = `TRUE`, n_good = `FALSE`) %>% 
  # Replace NAs with 0, in case there were 0 good trials in a block or 
  # 0 bad trials in a block
  mutate(n_bad = coalesce(n_bad, 0L),
         n_good = coalesce(n_good, 0L),
         trials = n_good + n_bad,
         prop_bad = round(n_bad / trials, 2)) 

blocks_to_drop <- bad_trial_counts %>% 
  filter(.5 <= prop_bad)
blocks_to_drop

leftover_bad_trials <- missing_data_by_trial %>% 
  anti_join(blocks_to_drop, by = c("Study", "ResearchID", "Basename")) %>% 
  filter(.5 <= PropNA)

clean_looks <- looks %>% 
  anti_join(blocks_to_drop, by = c("Study", "ResearchID", "Basename")) %>% 
  anti_join(leftover_bad_trials)
```

Do some head counts.

```{r}
screening_results <- list(Screened = clean_looks, Raw = looks) %>% 
  bind_rows(.id = "Dataset") %>% 
  distinct(Dataset, Study, ResearchID, TrialID) %>% 
  group_by(Dataset, Study) %>% 
  summarise(
    `Num Children` = n_distinct(ResearchID),
    `Num Trials` = n_distinct(TrialID))

screening_results %>% 
  knitr::kable(caption = "Eyetracking data before and after data screening")
```

Plot the data after partial data screening.

```{r}
data <- clean_looks %>% 
  filter(-505 <= Time, Time <= 2020, Version == "Standard") %>% 
  aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI)
```

We include the curves from the earlier plots in gray. The data-cleaning process 
slightly increases the average accuracy during the plateau-ed portion of the
growth curve.

```{r clean-aim1, out.width = "50%", fig.show='hold', fig.height=3, fig.width=3}
ggplot(data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_summary(aes(group = Study), data = raw_data, 
               color = "gray70") +
  stat_summary() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "Mean +/- SE") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))

ggplot(data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_smooth(aes(group = Study), data = raw_data, 
              color = "gray70") +
  stat_smooth() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "GAM smooth on partially screened data") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))
```

### Special case data screening

(_Skip for now._ This is where I review the participant notes and will remove 
children who have to be excluded for other reasons, like being diagnosed with a 
language disorder at TimePoint 3.)


### Add a note about the bad version of the experiment

(_Skip for now._)

### Aggregate looks to each image type

```{r by-aoi-prop, out.width = "100%", fig.height=2.5, fig.width=6}
# Create a rule for each AOI type
fast_resp_def <- function(i) {
  aois <- c("Target", "PhonologicalFoil", "SemanticFoil", "Unrelated")
  create_response_def(
    primary = aois[i], others = aois[-i],
    elsewhere = "tracked", missing = NA)
}
defs <- purrr::map(1:4, fast_resp_def)
names(defs) <- defs %>% purrr::map("primary")

# Aggregate looks using each rule
data <- clean_looks %>% 
  filter(-505 <= Time, Time <= 2020) 

looks_by_aoi <- defs %>% 
  purrr::map_df(
    ~ aggregate_looks(data, .x, 
                      Study + ResearchID + Time ~ GazeByImageAOI), 
    .id = "AOI") %>% 
  select(AOI:Time, Prop, Primary, Unrelated) %>% 
  mutate(AOI = factor(AOI, c("Target", "PhonologicalFoil", 
                             "SemanticFoil", "Unrelated")))
  
ggplot(looks_by_aoi) + 
  aes(x = Time, y = Prop, color = Study) + 
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_smooth() + 
  facet_grid( ~ AOI) + 
    labs(x = "Time after target onset [ms]",
       y = "Proportion looks",
       caption = "GAM smooth on partially screened data") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.95, 0.95), 
        legend.justification = c(1, 1))
```

Normalize by using ratio of looks to each AOI versus the unrelated image. 

```{r by-aoi-logit, out.width = "100%", fig.height=2.5, fig.width=6}
ggplot(looks_by_aoi %>% filter(AOI != "Unrelated")) + 
  aes(x = Time, y = log(Primary / Unrelated), color = Study) + 
  geom_hline(size = 2, color = "white", yintercept = 0) +
  stat_smooth() + 
  facet_grid( ~ AOI) + 
    labs(x = "Time after target onset [ms]",
       y = "Log odds looking to word vs. unrelated word",
       caption = "GAM smooth on partially screened data") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.95, 0.95), 
        legend.justification = c(1, 1))
```

Each curve is the log odds of looking to the target, phonological foil, and
semantic foil versus the unrelated word. Positive values mean more looks to an
image type than the unrelated. If you think of the _y_ axis as the image's
_relatedness_ to the target, you can see a time course of relatedness in each
panel: Here early phonological effects meaning early relatedness and later,
flatter semantic effects meaning late relate relatedness. (Which makes extra
sense if phonological representations come into play before semantic ones.)

_To do: Filter items for each comparison._

This plot suggests an important finding: Children becoming more sensitive to the
phonological and semantic foils as they grow older. (I use the verb _suggest_
because this is still a preliminary finding). Jan and I had made opposite 
predictions about whether this would happen. Her argument, I think, was that 
children become better at word recognition by becoming better able to inhibit 
interference from competing words. This plot would suggest that they show 
increased sensitive to the target and foils words by looking less to the 
unrelated word as they age and reapportioning those looks to the other three 
lexically relevant words.

