Look at raw data
===========================================================================

First, let's load in all the data and plot the data from each year.


```{r include = FALSE}
knitr::read_chunk("./helpers.R")
```

```{r helpers, message = FALSE, warnings = FALSE}
```

```{r, message = FALSE, warnings = FALSE}
looks1 <- readr::read_csv("./data-raw/rwl_timepoint1_looks.csv.gz")
looks2 <- readr::read_csv("./data-raw/rwl_timepoint2_looks.csv.gz")
looks3 <- readr::read_csv("./data-raw/rwl_timepoint3_looks.csv.gz")
looks <- bind_rows(looks1, looks2, looks3) %>% 
  filter(Version == "Standard")

resp_def <- create_response_def(
  primary = "Target",
  others = c("PhonologicalFoil", "SemanticFoil", "Unrelated"),
  elsewhere = "tracked",
  missing = NA
)

# Keep only frames from -500 to 2000 plus or minus any frames to make the 
# number of frames divisible by 3 (for binning)
times_to_keep <- looks %>% 
  distinct(Time) %>% 
  trim_to_bin_width(3, time_var = Time, key_time = 0, key_position = 2, 
                    min_time = -500, max_time = 2000) %>% 
  pull(Time) %>% 
  range()

raw_data <- looks %>% 
  filter(between(Time, times_to_keep[1], times_to_keep[2])) %>% 
  aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI)
```

Make some plots of overall averages.

```{r raw-aim1, out.width = "50%", fig.show='hold', fig.height=3, fig.width=3}
ggplot(raw_data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_summary() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "Mean +/- SE") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))

ggplot(raw_data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_smooth() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "GAM Smooth") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))
```

The raw data plainly confirm hypothesis 1:

> Childrenâ€™s accuracy and efficiency of recognizing words will improve each year.

Look at a spaghetti plot...

```{r spaghetti-aim1, out.width = "100%", fig.height=2.5, fig.width=6}
ggplot(raw_data) + 
  aes(x = Time, y = Prop, group = ResearchID) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  geom_line(alpha = .15) +
  facet_grid(~ Study) +
  theme_grey(base_size = 9) +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "Lines: Individual participants") +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))
```



## Data cleaning

We use the following options for data screening.

```{r}
rules <- list(
  screening_window = c(0, 2020),
  missing_data_limit = .5,
  min_trials = 12
)
```

That is:

* Filter out bad trials. These have at least 
  `r rules$missing_data_limit * 100`% missing data between 
  `r rules$screening_window[1]` to `r rules$screening_window[2]`ms. 
* Filter out bad blocks. These have fewer than `r rules$min_trials` trials.

These are the default conventions four lab on these eyetracking experiments.

```{r}
# offset to catch frame before and after window 
# (to reflect binning boundaries)
screening_times <- looks %>% 
  distinct(Time) %>% 
  trim_to_bin_width(3, 0, 2, Time, min_time = rules$screening_window[1], 
                    max_time = rules$screening_window[2]) %>% 
  pull(Time) %>% 
  range()
screening_times

missing_data_by_trial <- looks %>% 
  filter(screening_times[1] <= Time, Time <= screening_times[2]) %>% 
  aggregate_looks(
    resp_def, 
    Study + Version + ResearchID + Basename + TrialNo ~ GazeByImageAOI) %>% 
  mutate(BadTrial = rules$missing_data_limit <= PropNA)

bad_trial_counts <- missing_data_by_trial %>% 
  count(Study, ResearchID, Basename, BadTrial) %>% 
  tidyr::spread(BadTrial, n) %>% 
  rename(n_bad = `TRUE`, n_good = `FALSE`) %>% 
  # Replace NAs with 0, in case there were 0 good trials in a block or 
  # 0 bad trials in a block
  mutate(n_bad = coalesce(n_bad, 0L),
         n_good = coalesce(n_good, 0L),
         trials = n_good + n_bad,
         prop_bad = round(n_bad / trials, 2)) 

blocks_to_drop <- bad_trial_counts %>% 
  filter(.5 <= prop_bad)
blocks_to_drop

leftover_bad_trials <- missing_data_by_trial %>% 
  anti_join(blocks_to_drop, by = c("Study", "ResearchID", "Basename")) %>% 
  filter(.5 <= PropNA)

clean_looks <- looks %>% 
  anti_join(blocks_to_drop, by = c("Study", "ResearchID", "Basename")) %>% 
  anti_join(leftover_bad_trials)
```

Do some head counts.

```{r}
screening_results <- list(Screened = clean_looks, Raw = looks) %>% 
  bind_rows(.id = "Dataset") %>% 
  distinct(Dataset, Study, ResearchID, TrialID) %>% 
  group_by(Dataset, Study) %>% 
  summarise(
    `Num Children` = n_distinct(ResearchID),
    `Num Trials` = n_distinct(TrialID))

screening_results %>% 
  knitr::kable(caption = "Eyetracking data before and after data screening")
```

Plot the data after partial data screening.

```{r}
data <- clean_looks %>% 
  filter(between(Time, times_to_keep[1], times_to_keep[2])) %>% 
  readr::write_csv("./data/aim1-screened.csv.gz") 

agg_data <- data %>% 
  aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI)
```

We include the curves from the earlier plots in gray. The data-cleaning process 
slightly increases the average accuracy during the plateau-ed portion of the
growth curve.

```{r clean-aim1, out.width = "50%", fig.show='hold', fig.height=3, fig.width=3}
ggplot(agg_data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_summary(aes(group = Study), data = raw_data, 
               color = "gray70") +
  stat_summary() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "Mean +/- SE") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))

ggplot(agg_data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_smooth(aes(group = Study), data = raw_data, 
              color = "gray70") +
  stat_smooth() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "GAM smooth on partially screened data") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))
```

### Special case data screening

(_Skip for now._ This is where I review the participant notes and will remove 
children who have to be excluded for other reasons, like being diagnosed with a 
language disorder at TimePoint 3.)


### Add a note about the bad version of the experiment

(_Skip for now._)


### Interim summary

* Visual evidence that group averages get faster and more reliable at looking 
  to target each year 
  


## Next steps

- Model year over year changes.
- Download test scores and individual differences. 
- Model individual differences

```{r, eval = FALSE}
# Downsample to 50 ms bins
d <- data %>% 
  select(Study, ResearchID, TrialID:GazeByImageAOI) %>% 
  filter(-467 <= Time) %>% 
  assign_bins(bin_width = 3, Time, TrialID)

bin_counts <- d %>% 
  distinct(Time, .bin) %>% 
  count(.bin)
stopifnot(bin_counts$n == 3)
  
time_counts <- d %>% count(.bin)
stopifnot(bin_counts$n == bin_counts$n[1])

d <- d %>% 
  group_by(.bin) %>% 
  mutate(BinTime = round(median(.data$Time), -1)) %>% 
  ungroup() %>% 
  select(-Time) %>% 
  rename(Time = BinTime) %>% 
  aggregate_looks(defs[[1]], Study + ResearchID + Time ~ GazeByImageAOI)


d_m <- d %>% 
  filter(250 <= Time, Time <= 1500) %>% 
  polypoly::poly_add_columns(Time, degree = 3, 
                             scale_width = 1, prefix = "ot")

ggplot(d_m) + 
  aes(x = Time, y = empirical_logit(Primary, Others)) + 
  geom_line(aes(group = ResearchID), alpha = .2) + 
  stat_smooth() +
  facet_grid(. ~ Study)
```


```{r, eval = FALSE}
library(lme4)
fx <- . %>% 
  glmer(
    cbind(Primary, Others) ~
      (ot1 + ot2) +
      (ot1 + ot2 | ResearchID),
    family = binomial,
    data = .)

m <- glmer(
    cbind(Primary, Others) ~
      (ot1 + ot2) * Study +
      (ot1 + ot2 | ResearchID/Study),
    family = binomial,
    data = d_m)

d_m$quad_fit <- fitted(m)
ggplot(d_m) + 
  aes(x = Time, y = quad_fit) + 
  geom_line(aes(group = ResearchID), alpha = .2) + 
  stat_smooth() +
  facet_grid(. ~ Study)

top_10 <- m %>% ranef() %>% getElement("ResearchID") %>% tibble::rownames_to_column("ResearchID") %>% as_tibble() %>% 
  top_n(10, ot1) %>% 
  select(ResearchID, slope = ot1)

bot_10 <- m %>% ranef() %>% getElement("ResearchID") %>% tibble::rownames_to_column("ResearchID") %>% as_tibble() %>% 
  top_n(10, -ot1) %>% 
  select(ResearchID, slope = ot1)



ggplot(d_m) + 
  aes(x = Time, y = quad_fit) + 
  geom_line(aes(group = ResearchID), alpha = .2) +
  geom_line(aes(group = ResearchID), data = semi_join(d_m, top_10), size = 1) + 
  geom_line(aes(group = ResearchID), data = semi_join(d_m, bot_10), size = 1, color = "red") + 
  facet_grid(. ~ Study)
```



