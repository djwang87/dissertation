

Look at raw data
===========================================================================

First, let's load in all the data and plot the data from each year.

```{r, message = FALSE, warnings = FALSE}
library(dplyr)
looks1 <- readr::read_csv("./data-raw/rwl_timepoint1_looks.csv.gz")
looks2 <- readr::read_csv("./data-raw/rwl_timepoint2_looks.csv.gz")
looks3 <- readr::read_csv("./data-raw/rwl_timepoint3_looks.csv.gz")
looks <- bind_rows(looks1, looks2, looks3) %>% 
  filter(Version == "Standard")

library(littlelisteners)
resp_def <- create_response_def(
  primary = "Target",
  others = c("PhonologicalFoil", "SemanticFoil", "Unrelated"),
  elsewhere = "tracked",
  missing = NA
)

raw_data <- looks %>% 
  filter(-505 <= Time, Time <= 2020) %>% 
  aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI)
```

Make some plots of overall averages.

```{r raw-aim1, out.width = "50%", fig.show='hold', fig.height=3, fig.width=3}
library(ggplot2)

ggplot(raw_data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_summary() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "Mean +/- SE") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))

ggplot(raw_data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_smooth() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "GAM Smooth") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))
```

The raw data plainly confirm hypothesis 1:

> Childrenâ€™s accuracy and efficiency of recognizing words will improve each year.

Look at a spaghetti plot...

```{r spaghetti-aim1, out.width = "100%", fig.height=2.5, fig.width=6}
ggplot(raw_data) + 
  aes(x = Time, y = Prop, group = ResearchID) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  geom_line(alpha = .15) +
  facet_grid(~ Study) +
  theme_grey(base_size = 9) +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "Lines: Individual participants") +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))
```



## Data cleaning

We use the following options for data screening.

```{r}
rules <- list(
  screening_window = c(0, 2000),
  missing_data_limit = .5,
  min_trials = 12
)
```

That is:

* Filter out bad trials. These have at least 
  `r rules$missing_data_limit * 100`% missing data between 
  `r rules$screening_window[1]` to `r rules$screening_window[2]`ms. 
* Filter out bad blocks. These have fewer than `r rules$min_trials` trials.

These are the default conventions four lab on these eyetracking experiments.

```{r}
# offset to catch frame before and after window
screen_from <- rules$screening_window[1] - 20
screen_to <- rules$screening_window[2] + 20

missing_data_by_trial <- looks %>% 
  filter(screen_from <= Time, Time <= screen_to) %>% 
  aggregate_looks(
    resp_def, 
    Study + Version + ResearchID + Basename + TrialNo ~ GazeByImageAOI) %>% 
  mutate(BadTrial = rules$missing_data_limit <= PropNA)

bad_trial_counts <- missing_data_by_trial %>% 
  count(Study, ResearchID, Basename, BadTrial) %>% 
  tidyr::spread(BadTrial, n) %>% 
  rename(n_bad = `TRUE`, n_good = `FALSE`) %>% 
  # Replace NAs with 0, in case there were 0 good trials in a block or 
  # 0 bad trials in a block
  mutate(n_bad = coalesce(n_bad, 0L),
         n_good = coalesce(n_good, 0L),
         trials = n_good + n_bad,
         prop_bad = round(n_bad / trials, 2)) 

blocks_to_drop <- bad_trial_counts %>% 
  filter(.5 <= prop_bad)
blocks_to_drop

leftover_bad_trials <- missing_data_by_trial %>% 
  anti_join(blocks_to_drop, by = c("Study", "ResearchID", "Basename")) %>% 
  filter(.5 <= PropNA)

clean_looks <- looks %>% 
  anti_join(blocks_to_drop, by = c("Study", "ResearchID", "Basename")) %>% 
  anti_join(leftover_bad_trials)
```

Do some head counts.

```{r}
screening_results <- list(Screened = clean_looks, Raw = looks) %>% 
  bind_rows(.id = "Dataset") %>% 
  distinct(Dataset, Study, ResearchID, TrialID) %>% 
  group_by(Dataset, Study) %>% 
  summarise(
    `Num Children` = n_distinct(ResearchID),
    `Num Trials` = n_distinct(TrialID))

screening_results %>% 
  knitr::kable(caption = "Eyetracking data before and after data screening")
```

Plot the data after partial data screening.

```{r}
data <- clean_looks %>% 
  filter(-505 <= Time, Time <= 2020, Version == "Standard") %>% 
  aggregate_looks(resp_def, Study + ResearchID + Time ~ GazeByImageAOI)
```

We include the curves from the earlier plots in gray. The data-cleaning process 
slightly increases the average accuracy during the plateau-ed portion of the
growth curve.

```{r clean-aim1, out.width = "50%", fig.show='hold', fig.height=3, fig.width=3}
ggplot(data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_summary(aes(group = Study), data = raw_data, 
               color = "gray70") +
  stat_summary() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "Mean +/- SE") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))

ggplot(data) + 
  aes(x = Time, y = Prop, color = Study) +
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_smooth(aes(group = Study), data = raw_data, 
              color = "gray70") +
  stat_smooth() +
  labs(x = "Time after target onset [ms]",
       y = "Proportion looks to target",
       caption = "GAM smooth on partially screened data") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.05, 0.95), 
        legend.justification = c(0, 1))
```

### Special case data screening

(_Skip for now._ This is where I review the participant notes and will remove 
children who have to be excluded for other reasons, like being diagnosed with a 
language disorder at TimePoint 3.)


### Add a note about the bad version of the experiment

(_Skip for now._)



## Visualize looks to each image type

_To do:_ Update little listeners R package to support multiple simultaneous 
aggregations.

```{r by-aoi-prop, out.width = "100%", fig.height=2.5, fig.width=6}
# Create a rule for each AOI type
fast_resp_def <- function(i) {
  aois <- c("Target", "PhonologicalFoil", "SemanticFoil", "Unrelated")
  create_response_def(
    primary = aois[i], others = aois[-i],
    elsewhere = "tracked", missing = NA)
}
defs <- purrr::map(1:4, fast_resp_def)
names(defs) <- defs %>% purrr::map("primary")

# Aggregate looks using each rule
data <- clean_looks %>% 
  filter(-505 <= Time, Time <= 2020) 

looks_by_aoi <- defs %>% 
  purrr::map_df(
    ~ aggregate_looks(data, .x, 
                      Study + ResearchID + Time ~ GazeByImageAOI), 
    .id = "AOI") %>% 
  select(AOI:Time, Prop, Primary, Unrelated) %>% 
  mutate(AOI = factor(AOI, c("Target", "PhonologicalFoil", 
                             "SemanticFoil", "Unrelated")))
  
ggplot(looks_by_aoi) + 
  aes(x = Time, y = Prop, color = Study) + 
  geom_hline(size = 2, color = "white", yintercept = .25) +
  stat_smooth() + 
  facet_grid( ~ AOI) + 
    labs(x = "Time after target onset [ms]",
       y = "Proportion looks",
       caption = "GAM smooth on partially screened data") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.95, 0.95), 
        legend.justification = c(1, 1))
```

Normalize by using ratio of looks to each AOI versus the unrelated image. 

```{r by-aoi-logit, out.width = "100%", fig.height=2.5, fig.width=6}
ggplot(looks_by_aoi %>% filter(AOI != "Unrelated")) + 
  aes(x = Time, y = log(Primary / Unrelated), color = Study) + 
  geom_hline(size = 2, color = "white", yintercept = 0) +
  stat_smooth() + 
  facet_grid( ~ AOI) + 
    labs(x = "Time after target onset [ms]",
       y = "Log odds looking to word vs. unrelated word",
       caption = "GAM smooth on partially screened data") +
  theme_grey(base_size = 9) +
  theme(legend.position = c(0.95, 0.95), 
        legend.justification = c(1, 1))
```

Each curve is the log odds of looking to the target, phonological foil, and
semantic foil versus the unrelated word. Positive values mean more looks to an
image type than the unrelated. If you think of the _y_ axis as the image's
_relatedness_ to the target, you can see a time course of relatedness in each
panel: Here early phonological effects meaning early relatedness and later,
flatter semantic effects meaning late relate relatedness. (Which makes extra
sense if phonological representations come into play before semantic ones.)

This plot suggests an important finding: Children becoming more sensitive to the
phonological and semantic foils as they grow older. (I use the verb _suggest_
because this is still a preliminary finding). Jan and I had made opposite 
predictions about whether this would happen. Her argument, I think, was that 
children become better at word recognition by becoming better able to inhibit 
interference from competing words. This plot would suggest that they show 
increased sensitive to the target and foils words by looking less to the 
unrelated word as they age and reapportioning those looks to the other three 
lexically relevant words.

### Comparing strong versus weak foils

In @RWLPaper, we ignored trials for certain items where we didn't think the
phonological or semantic similarity was strong enough.

```{r, message = FALSE,}
trial_info <- bind_rows(
  readr::read_csv("data-raw/rwl_timepoint1_trials.csv.gz"),
  readr::read_csv("data-raw/rwl_timepoint2_trials.csv.gz"),
  readr::read_csv("data-raw/rwl_timepoint3_trials.csv.gz")) %>% 
  select(TrialID, Target = WordTarget, 
         PhonologicalFoil = WordPhonologicalFoil,
         SemanticFoil = WordSemanticFoil, 
         Unrelated = WordUnrelated)

good_phono <- c("bear", "bee", "bell", "dress", "drum", "flag", "fly", 
                "heart", "horse", "pan", "pear", "pen", "vase")

good_semy <- c("bear", "bee", "bell", "bread", "cheese", "dress", 
               "drum", "fly", "horse", "pan", "pear", "shirt", "spoon")

words <- trial_info %>% 
  distinct(Target, PhonologicalFoil, SemanticFoil, Unrelated)

phono_foils <- split(words, words$Target %in% good_phono) %>% 
  lapply(arrange, Target) %>% 
  setNames(c("weak_foil", "strong_foil"))

semy_foils <- split(words, words$Target %in% good_semy) %>% 
  lapply(arrange, Target) %>% 
  setNames(c("weak_foil", "strong_foil"))

phono_foils$strong_foil %>% 
  knitr::kable(caption = "Trials with strong phonological foils.")

phono_foils$weak_foil %>% 
  knitr::kable(caption = "Trials with weak phonological foils.")

semy_foils$strong_foil %>% 
  knitr::kable(caption = "Trials with strong semantic foils.")

semy_foils$weak_foil %>% 
  knitr::kable(caption = "Trials with weak semantic foils.")
```

We should verify that the two sets of words behave differently.

```{r by-aoi-logit-foils, out.width = "100%", fig.height=2.5, fig.width=6}
weak_phon_looks <- trial_info %>% 
  semi_join(phono_foils$weak_foil) %>% 
  left_join(data) %>% 
  mutate(PhonFoil = "Weak")

strong_phon_looks <- trial_info %>% 
  semi_join(phono_foils$strong_foil) %>% 
  left_join(data) %>% 
  mutate(PhonFoil = "Strong")

phon_data <- bind_rows(strong_phon_looks, weak_phon_looks)

weak_semy_looks <- trial_info %>% 
  semi_join(semy_foils$weak_foil) %>% 
  left_join(data) %>% 
  mutate(SemyFoil = "Weak")

strong_semy_looks <- trial_info %>% 
  semi_join(semy_foils$strong_foil) %>% 
  left_join(data) %>% 
  mutate(SemyFoil = "Strong")

phon_data <- bind_rows(strong_phon_looks, weak_phon_looks)
semy_data <- bind_rows(weak_semy_looks, strong_semy_looks)

looks_by_aoi2 <- defs[1:2] %>% 
  purrr::map_df(
    ~ aggregate_looks(phon_data, .x, 
                      Study + ResearchID + PhonFoil + Time ~ GazeByImageAOI), 
    .id = "AOI") %>% 
  select(AOI:Time, PhonFoil, Prop, Primary, PhonologicalFoil, Unrelated) %>% 
  mutate(AOI = factor(AOI, c("Target", "PhonologicalFoil", 
                             "SemanticFoil", "Unrelated")))

looks_by_aoi3 <- defs[c(1, 3)] %>% 
  purrr::map_df(
    ~ aggregate_looks(semy_data, .x, 
                      Study + ResearchID + SemyFoil + Time ~ GazeByImageAOI), 
    .id = "AOI") %>% 
  select(AOI:Time, SemyFoil, Prop, Primary, SemanticFoil, Unrelated) %>% 
  mutate(AOI = factor(AOI, c("Target", "PhonologicalFoil", 
                             "SemanticFoil", "Unrelated")))

ggplot(looks_by_aoi2 %>% filter(AOI != "Unrelated")) + 
  aes(x = Time, y = log(Primary / Unrelated), 
      color = Study, linetype = PhonFoil) + 
  geom_hline(size = 2, color = "white", yintercept = 0) +
  stat_smooth() + 
  facet_grid( ~ AOI) + 
    labs(x = "Time after target onset [ms]",
       y = "Log odds looking to word vs. unrelated word",
       caption = "GAM smooth on partially screened data") +
  theme_grey(base_size = 9) 

ggplot(looks_by_aoi3 %>% filter(AOI != "Unrelated")) + 
  aes(x = Time, y = log(Primary / Unrelated), 
      color = Study, linetype = SemyFoil) + 
  geom_hline(size = 2, color = "white", yintercept = 0) +
  stat_smooth() + 
  facet_grid( ~ AOI) + 
    labs(x = "Time after target onset [ms]",
       y = "Log odds looking to word vs. unrelated word",
       caption = "GAM smooth on partially screened data") +
  theme_grey(base_size = 9) 
```

What's going on here:

* The weak phonological foils are indeed weaker than the strong foils.
* The strong semantic foils appear stronger than the weak ones. The strong foils 
  show a growth curve pattern of increasing looks away from baseline and there 
  a developmental difference among the growth curves for each time point.
* Children have a lower advantage for the target (vs unrelated) in weak 
  foil trials because... why?

Now let's look at the target versus each foil and the unrelated.

```{r by-aoi-logit-target-phon, out.width = "100%", fig.height=2.5, fig.width=6}
semantic_target_curves <- looks_by_aoi3 %>% 
  filter(AOI == "Target") %>% 
  mutate(`Target vs Semantic` = log(Primary / SemanticFoil), 
         `Target vs Unrelated` = log(Primary / Unrelated)) %>% 
  tidyr::gather("Comparison", "LogOdds", 
                `Target vs Semantic`, `Target vs Unrelated`) 
  
phonological_target_curves <- looks_by_aoi2 %>% 
  filter(AOI == "Target") %>% 
  mutate(`Target vs Phonological` = log(Primary / PhonologicalFoil), 
         `Target vs Unrelated` = log(Primary / Unrelated)) %>% 
  tidyr::gather("Comparison", "LogOdds", 
                `Target vs Phonological`, `Target vs Unrelated`) 
  
ggplot(phonological_target_curves) + 
  aes(x = Time, y = LogOdds, 
      color = Study, linetype = PhonFoil) + 
  geom_hline(size = 2, color = "white", yintercept = 0) +
  stat_smooth() + 
  facet_grid( ~ Comparison) + 
    labs(x = "Time after target onset [ms]",
       y = "Log odds looking",
       caption = "GAM smooth on partially screened data") +
  theme_grey(base_size = 9) 
```

Both comparisons attain the same height, so phonological and unrelated foils 
affect processing equally later in the trial. The strong phonological foils 
curves in the Target vs Phonological comparison rise later than the weak foils, 
reflecting early looks to the phonological foils.

```{r by-aoi-logit-target-semi, out.width = "100%", fig.height=2.5, fig.width=6}
ggplot(semantic_target_curves) + 
  aes(x = Time, y = LogOdds, 
      color = Study, linetype = SemyFoil) + 
  geom_hline(size = 2, color = "white", yintercept = 0) +
  stat_smooth() + 
  facet_grid( ~ Comparison) + 
    labs(x = "Time after target onset [ms]",
       y = "Log odds looking",
       caption = "GAM smooth on partially screened data") +
  theme_grey(base_size = 9) 
```

The two comparisons do not attain the same height, so the semantic foil reduces 
odds of fixating to the target later on in a trial. There appears to be no 
difference in strong and weak foils in Year 2 and Year 3, so I might be able 
to collapse to remove this distinction and include more items in the analysis.

