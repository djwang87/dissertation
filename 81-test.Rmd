```{r include = FALSE}
knitr::read_chunk("./helpers.R")
if (interactive()) source("./helpers.R")
```

```{r helpers, include = FALSE}
```



# Quick testing sandbox

_This is a chapter for quickly previewing and testing how content appears_

We fit a generalized additive model with fast restricted maximum likelihood
estimation [@Wood2017; @Soskuthy2017 for a tutorial for linguists; see Box 1].
We included main effects of study year. These *parametric* terms work like
conventional regression effects and determined the growth curve's average
values. We used age-4 as the reference year, so the model's intercept
represented the average looking probability at age 4. The model's year effects
therefore represented differences between age 4 vs. age 3 and age 4 vs. age 5.

We included a *smooth* term for time. We included a smooth term for trial time
to represent a general effect of time following noun onset across all studies,
and we also included smooth terms for time for each study. These study-specific
smooths estimate how the shape of the data differs in each individual study. As
an equation, our model estimated: [@Barr2008;]
[vers. `r packageVersion("itsadug")`; @itsadug]





\Begin{infobox}
<div class = "infobox">
**Box 1: The Intuition Behind Generalized Additive Models**.

In these analyses, the outcome of interest is a value that changes over time in
a nonlinear way. We model these time series by building a set of features to
represent time values. In the growth curve analyses of familiar word
recognition, we used a set of polynomial features which expressed time as the
weighted sum of a linear trend, a quadratic trend and cubic trend. That is:

$$
\text{log-odds}(\mathit{looking}) = 
  \alpha + \beta_1 * \textit{Time}^1 +
           \beta_2 * \textit{Time}^2 +
           \beta_3 * \textit{Time}^3
$$

But another way to think about the polynomial terms is as *basis functions*: A
set of features that combine to approximate some nonlinear function of
time. Under this framework, the model can be expressed as:

$$
\text{log-odds}(\mathit{looking}) = 
  \alpha + f(\textit{Time})
$$
  
This is the idea behind generalized additive models and their *smooth terms*.
These smooths fit nonlinear functions of data by weighting and adding 
simple functions together. The figures below show 9 basis functions from a
"thin-plate spline" and how they can be weighted and summed to fit a growth
curve.

```{r infobox-1-figs, message = FALSE, echo = FALSE, out.width = "66%", fig.width = 6, fig.height = 3, fig.align='center'}
library(mgcv)
library(ggplot2)
library(dplyr)

t1_fam <- structure(
  list(
    Time = c(
      250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 
      950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500), 
    Prop = c(
      0.252, 0.255, 0.257, 0.261, 0.265, 0.275, 0.286, 0.298, 0.311, 0.315, 
      0.329, 0.342, 0.365, 0.392, 0.407, 0.422, 0.446, 0.464, 0.479, 0.497, 
      0.514, 0.524, 0.532, 0.545, 0.549, 0.555)), 
  .Names = c("Time", "Prop"), 
  class = c("tbl_df", "tbl", "data.frame"), 
  row.names = c(NA, -26L))

times <- modelr::seq_range(t1_fam$Time, 80)
newdata <- data.frame(Time = times)
t1_gam <- gam(Prop ~ s(Time), data = t1_fam)

t1_gam <- gam(Prop ~ s(Time, bs = "tp", k = 10), data = t1_fam)
basis_matrix <- predict(t1_gam, newdata, type = "lpmatrix")[, 2:10]

basis <- polypoly::poly_melt(basis_matrix) %>% 
  mutate(observation = as.numeric(observation)) %>% 
  left_join(
    data.frame(Time = times, observation = seq_along(times)), 
    by = "observation")

t1_poly <- lm(Prop ~ poly(Time, 3), data = t1_fam)

poly_basis <- polypoly::poly_melt(poly(t1_fam$Time, 3)) %>% 
  mutate(observation = as.numeric(observation)) %>% 
  left_join(
    data.frame(Time = times, observation = seq_along(times)), 
    by = "observation")

p1 <- ggplot(basis) + 
  aes(x = Time, y = value) + 
  geom_line(aes(color = degree)) + 
  ylim(c(-2, 2)) + 
  guides(color = FALSE) + 
  ggtitle("Basis functions (time features)") + 
  labs(y = NULL) +
  theme_grey(base_size = 9) + 
  theme(
    plot.background = element_rect(fill = "#eef7fa", colour = "#eef7fa"),
    panel.background = element_rect(fill = "#E5EAEF"))

p1_poly <- ggplot(poly_basis) + 
  aes(x = Time, y = value) + 
  geom_line(aes(color = degree)) + 
  # ylim(c(-2, 2)) + 
  guides(color = FALSE) + 
  ggtitle("Basis functions (time features)") + 
  labs(y = NULL) +
  theme_grey(base_size = 9)

weighted <- (basis_matrix %*% diag(coef(t1_gam)[-1])) %>% 
  polypoly::poly_melt() %>% 
  mutate(observation = as.numeric(observation)) %>% 
  left_join(
    data.frame(Time = times, observation = seq_along(times)), 
    by = "observation")

poly_weighted <- (poly(t1_fam$Time, 3) %*% diag(coef(t1_poly)[-1])) %>% 
  polypoly::poly_melt() %>% 
  mutate(observation = as.numeric(observation)) %>% 
  left_join(
    data.frame(Time = times, observation = seq_along(times)), 
    by = "observation")

p2 <- ggplot(weighted) + 
  aes(x = Time, y = value) + 
  geom_line(aes(color = factor(degree))) + 
  stat_summary(fun.y = sum, color = "#0074D9", geom = "line", size = 1.25) + 
  ylim(c(-.2, .2)) + 
  guides(color = FALSE) + 
  ggtitle("Weighted basis functions") + 
  labs(y = NULL) +
  theme_grey(base_size = 9)  + 
  theme(
    plot.background = element_rect(fill = "#eef7fa", colour = "#eef7fa"),
    panel.background = element_rect(fill = "#E5EAEF"))

p2_poly <- ggplot(poly_weighted) + 
  aes(x = Time, y = value) + 
  geom_line(aes(color = factor(degree))) + 
  stat_summary(fun.y = sum, color = "#0074D9", geom = "line", size = 1.25) + 
  ylim(c(-.2, .2)) + 
  guides(color = FALSE) + 
  ggtitle("Weighted basis functions") + 
  labs(y = NULL) +
  theme_grey(base_size = 9) 

p3 <- cowplot::plot_grid(p1, p2)
p3_poly <- cowplot::plot_grid(p1_poly, p2_poly)
print(p3)
# ggsave("./misc/basis-raw.png", p1,  width = 3, height = 3)
# ggsave("./misc/basis-weighted.png", p2, width = 3, height = 3)
# cowplot::ggsave("./misc/basis-both.png", p3, width = 6, height = 3)
# cowplot::ggsave("./misc/basis-both.png", p3, width = 6, height = 3)
```

Each of these basis functions is weighted by a model coefficient, but the
individual basis functions are not a priori meaningful. Rather, it is the whole
set of functions that approximate the curvature of the data---i.e.,
*f*(Time))---so we statistically evaluate the whole batch of coefficients
simultaneously. This joint testing is similar to how one might test a batch of
effects in an ANOVA. If the batch of effects jointly improve model fit, we infer
that there is a significant smooth or shape effect. (Not quite sure this is 100%
accurate yet.)

Smooth terms come with an estimated degrees of freedom (EDF). These values
provide a sense of how many degrees of freedom the smooth consumed. An EDF of 1
is a perfectly straight line, indicating no smoothing. Higher EDF values
indicate that the smooth term captured more curvature from the data.

<!-- The other important thing to know about generalized additive models is that -->
<!-- wigglyness is penalized. With so many functions, one might worry about -->
<!-- overfitting the data and including incidental wiggliness into *f*(Time). These -->
<!-- models, however, include a smoothing parameter that -->
</div>
\End{infobox}
